This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
app/
  use_cases/
    __init__.py
    pipeline_support.py
  __init__.py
chess_clients/
  __init__.py
  base_chess_client.py
  chess_fetch_request.py
  chess_fetch_result.py
  chess_game_row.py
  fetch_helpers.py
  fixture_helpers.py
  GameRowInputs.py
  mock_chess_client.py
db/
  __init__.py
  _append_date_range_filters.py
  _append_optional_filter.py
  _append_time_control_filter.py
  _build_legacy_raw_pgn_inserts.py
  _build_raw_pgn_upsert_plan.py
  _drop_table_if_exists.py
  _fetch_next_raw_pgn_id.py
  _insert_raw_pgn_plan.py
  _migration_add_columns.py
  _migration_add_pipeline_views.py
  _migration_add_position_legality.py
  _migration_add_tactic_explanations.py
  _migration_add_training_attempt_latency.py
  _migration_base_tables.py
  _migration_raw_pgns_versioning.py
  _normalize_filter.py
  _rating_bucket_clause.py
  _rows_to_dicts.py
  dashboard_repository_provider.py
  delete_game_rows.py
  duckdb_dashboard_reader.py
  duckdb_dashboard_repository.py
  duckdb_metrics_repository.py
  duckdb_position_repository.py
  duckdb_raw_pgn_repository.py
  duckdb_store.py
  duckdb_tactic_repository.py
  fetch_latest_raw_pgns.py
  fetch_unanalyzed_positions.py
  metrics_repository_provider.py
  position_repository_provider.py
  postgres_repository.py
  query_helpers.py
  raw_pgn_repository_provider.py
  raw_pgn_summary.py
  raw_pgns_queries.py
  RawPgnInsertInputs.py
  record_training_attempt.py
  tactic_repository_provider.py
domain/
  __init__.py
errors/
  __init__.py
infra/
  clients/
    __init__.py
    chesscom_client.py
    lichess_client.py
  db/
    __init__.py
  __init__.py
models/
  __init__.py
  chess_position.py
  practice_attempt_request.py
ports/
  __init__.py
  game_source_client.py
tactics/
  __init__.py
utils/
  __init__.py
  generate_id.py
  hasher.py
  logger.py
  normalize_string.py
  now.py
  to_int.py
__init__.py
_apply_engine_options.py
_apply_fork_severity_floor.py
_apply_mate_overrides.py
_apply_outcome__failed_attempt_discovered_attack.py
_apply_outcome__failed_attempt_discovered_check.py
_apply_outcome__failed_attempt_hanging_piece.py
_apply_outcome__failed_attempt_line_tactics.py
_apply_outcome__failed_attempt_pin.py
_apply_outcome__failed_attempt_skewer.py
_apply_outcome__unclear_discovered_attack.py
_apply_outcome__unclear_discovered_check.py
_apply_outcome__unclear_fork.py
_apply_outcome__unclear_hanging_piece.py
_apply_outcome__unclear_mate_in_one.py
_apply_outcome__unclear_mate_in_two.py
_apply_outcome__unclear_mate.py
_apply_outcome__unclear_pin.py
_apply_outcome__unclear_skewer.py
_apply_outcome__unclear_variant.py
_apply_outcome__unclear.py
_apply_outcome_overrides.py
_best_san_from_fen.py
_build_pgn_context.py
_build_pgn_upsert_plan.py
_build_raw_pgn_summary.py
_build_schema_label.py
_build_tactic_rows.py
_clock_from_comment.py
_clock_to_seconds.py
_clock_token.py
_coerce_fixture_rows.py
_collect_tables.py
_compare_move__best_line.py
_compute_eval__discovered_attack_unclear_threshold.py
_compute_eval__discovered_check_unclear_threshold.py
_compute_eval__failed_attempt_threshold.py
_compute_eval__fork_unclear_threshold.py
_compute_eval__hanging_piece_unclear_threshold.py
_compute_eval__pin_unclear_threshold.py
_compute_eval__skewer_unclear_threshold.py
_compute_severity__tactic.py
_configure_engine_options.py
_connection_kwargs.py
_core.pyi
_delete_existing_analysis.py
_disabled_raw_pgn_summary.py
_empty_pgn_metadata.py
_engine_command_available.py
_ensure_chesscom_site_url.py
_evaluate_engine_position.py
_extract_metadata_from_headers.py
_extract_positions_fallback.py
_extract_positions_python.py
_extract_site_id.py
_fallback_kwargs.py
_FallbackKwargs.py
_fetch_latest_pgn_metadata.py
_fetch_raw_pgn_summary.py
_filter_fixture_games.py
_filter_supported_options.py
_fixture_payload.py
_fork_floor_for_settings.py
_forks_meet_threshold.py
_get_game_result_for_user_from_pgn_headers.py
_get_user_color_from_pgn_headers.py
_has_discovered_attack.py
_has_discovered_check.py
_has_discovered_line.py
_has_game_source.py
_has_new_target.py
_has_pin_in_steps.py
_has_skewer_in_steps.py
_ignore_progress.py
_infer_hanging_or_detected_motif.py
_initialize_engine.py
_insert_analysis_outcome.py
_insert_analysis_tactic.py
_insert_raw_pgn_row.py
_is_discovered_check_slider.py
_is_fork_piece.py
_is_line_tactic.py
_is_missed_mate.py
_is_new_hanging_piece.py
_is_profile_in.py
_is_skewer_in_step.py
_is_swing_at_least.py
_is_unclear_discovered_attack_candidate.py
_is_unclear_discovered_check_candidate.py
_is_unclear_fork_candidate.py
_is_unclear_hanging_piece_candidate.py
_is_unclear_pin_candidate.py
_is_unclear_skewer_candidate.py
_is_unclear_two_move_mate.py
_iter_position_contexts.py
_latest_pgn_metadata.py
_list_tables.py
_match_site_id.py
_maybe_upsert_raw_pgn_row.py
_normalize_clock_parts.py
_normalize_header_value.py
_normalize_profile__settings.py
_normalize_side_filter.py
_opponent_king_square.py
_override_mate_motif.py
_override_motif_for_missed.py
_parse_elo.py
_parse_game_source.py
_parse_optional_int.py
_parse_user_move.py
_parse_utc_start_ms.py
_position_context_helpers.py
_position_from_node.py
_prepare_position_inputs.py
_push_and_none.py
_reclassify_failed_attempt.py
_reclassify_motif.py
_record_upsert_result.py
_resolve_chesscom_profile_value.py
_resolve_dashboard_filters.py
_resolve_fixture_message.py
_resolve_game_context.py
_resolve_mate_in.py
_resolve_profile_value__settings.py
_resolve_user_rating.py
_schema_tables.py
_score_after_move.py
_score_best_line__after_move.py
_select_motif__discovered_attack_target.py
_select_motif__discovered_check_target.py
_select_motif__hanging_piece_target.py
_select_motif__pin_target.py
_select_motif__skewer_target.py
_severity_for_found_tactic.py
_severity_for_nonfound_tactic.py
_severity_for_result.py
_should_include_fixture.py
_should_mark_unclear_candidate.py
_should_mark_unclear_discovered_attack.py
_should_mark_unclear_discovered_check.py
_should_mark_unclear_fork.py
_should_mark_unclear_hanging_piece.py
_should_mark_unclear_mate_in_one.py
_should_mark_unclear_mate_in_two.py
_should_mark_unclear_pin.py
_should_mark_unclear_skewer.py
_should_override__discovered_attack_failed_attempt.py
_should_override__discovered_check_failed_attempt.py
_should_override__hanging_piece_failed_attempt.py
_should_override__pin_failed_attempt.py
_should_override__skewer_failed_attempt.py
_should_override_motif.py
_should_reclassify.py
_should_upgrade_mate_result.py
_skewer_sources.py
_upsert_postgres_raw_pgn_rows.py
airflow_daily_sync_context.py
airflow_settings.py
analyse_with_retries__pipeline.py
analysis_context.py
analysis_progress_interval__pipeline.py
analysis_signature__pipeline.py
AnalysisLoopContext.py
AnalysisPrepResult.py
analyze_position.py
analyze_positions__pipeline.py
analyze_positions_with_progress__pipeline.py
analyze_positions.py
analyze_tactics__positions.py
analyzer.py
api.py
apply_airflow_optional_conf__airflow_jobs.py
apply_backfill_filter__pipeline.py
apply_env_user_overrides__config.py
apply_settings_aliases__config.py
attach_position_ids__pipeline.py
BaseTacticDetector.py
build_airflow_conf__airflow_jobs.py
build_chess_client__pipeline.py
build_chunk_row__pipeline.py
build_daily_sync_payload__pipeline.py
build_dashboard_cache_key__api_cache.py
build_dashboard_stats_payload__api.py
build_extractor_request.py
build_pipeline_settings__pipeline.py
CaptureDetector.py
check_airflow_enabled__airflow_settings.py
chess_fen_char.py
chess_game_result.py
chess_game.py
chess_piece_type.py
chess_piece.py
chess_player_color.py
chess_time_control.py
chesscom_raw_games__pipeline.py
clear_dashboard_cache__api_cache.py
CLK_PATTERN.py
coerce_backfill_end_ms__airflow_jobs.py
coerce_cache_value__api_cache.py
coerce_date_cache_value__api_cache.py
coerce_date_to_datetime__datetime.py
collect_game_ids__pipeline.py
collect_positions_for_monitor__pipeline.py
compute_pgn_hashes__pipeline.py
config.py
conversion_payload__pipeline.py
convert_raw_pgns_to_positions__pipeline.py
count_hash_matches__pipeline.py
DailyAnalysisResult.py
dashboard_cache_state__api_cache.py
dashboard_query_filters.py
dashboard_query.py
dedupe_games__pipeline.py
define_base_db_store__db_store.py
define_base_db_store_context__db_store.py
define_chess_game__chess_game.py
define_chesscom_settings__config.py
define_config_defaults__config.py
define_db_schemas__const.py
define_lichess_settings__config.py
define_mock_store__db.py
define_outcome_insert_plan__db_store.py
define_pipeline_state__pipeline.py
define_settings__config.py
define_stockfish_settings__config.py
define_tactic_insert_plan__db_store.py
define_time_controls__const.py
detect_tactics__motifs.py
DiscoveredAttackContext.py
DiscoveredAttackDetector.py
DiscoveredCheckContext.py
DiscoveredCheckDetector.py
empty_conversion_payload__pipeline.py
engine_result.py
ensure_airflow_success__airflow_jobs.py
expand_pgn_rows__pipeline.py
expand_single_pgn_row__pipeline.py
extract_api_token__request_auth.py
extract_game_id.py
extract_last_timestamp_ms.py
extract_pgn_metadata.py
extract_positions__pgn.py
extract_positions_for_new_games__pipeline.py
extract_positions_for_rows__pipeline.py
extract_positions_from_games__pipeline.py
extract_positions_with_fallback__pgn.py
extract_positions.py
extractor_context.py
fetch_analysis_tactics.py
fetch_chesscom_games__pipeline.py
fetch_dag_run__airflow_api.py
fetch_incremental_games__pipeline.py
fetch_json__airflow_api.py
fetch_lichess_games__pipeline.py
fetch_ops_events.py
fetch_postgres_raw_pgns_summary.py
FetchContext.py
filter_backfill_games__pipeline.py
filter_games_by_window__pipeline.py
filter_games_for_window__pipeline.py
filter_positions_to_process__pipeline.py
filter_unprocessed_games__pipeline.py
FIXTURE_SPLIT_RE.py
ForkDetector.py
format_sse__api_streaming.py
format_tactics__explanation.py
GameRow.py
gather_auth__airflow_credentials.py
gather_url__airflow_base.py
get_airflow_run_id__airflow_response.py
get_airflow_state__airflow_jobs.py
get_auth_token__api.py
get_cached_dashboard_payload__api_cache.py
get_dashboard__api.py
get_dashboard_payload__pipeline.py
get_dashboard_summary__api.py
get_game_detail__api.py
get_health__api.py
get_job_status__api_jobs.py
get_material_value.py
get_postgres_analysis__api.py
get_postgres_raw_pgns__api.py
get_postgres_status__api.py
get_postgres_status.py
get_practice_next__api.py
get_practice_queue__api.py
get_raw_pgns_summary__api.py
get_settings__config.py
get_tactics_search__api.py
HangingPieceDetector.py
init_analysis_schema_if_needed__pipeline.py
init_analysis_schema.py
init_pgn_schema.py
init_postgres_schema.py
is_backfill_mode__pipeline.py
is_latest_hash__db_store.py
job_stream.py
latest_timestamp.py
legacy_args.py
line_tactic_helpers.py
LineTacticContext.py
list_sources_for_cache_refresh__api_cache.py
load_fixture_games.py
load_resume_positions__pipeline.py
log_raw_pgns_persisted__pipeline.py
manage_lifespan__fastapi.py
mate_outcome.py
maybe_sync_analysis_results__pipeline.py
maybe_upsert_postgres_analysis__pipeline.py
motif_stats.py
no_games_payload_contexts.py
normalize_and_expand_games__pipeline.py
normalize_game_row__pipeline.py
normalize_pgn_text__db_store.py
normalize_pgn.py
normalize_source__source.py
ops_event.py
orchestrate_dag_run__airflow_trigger.py
outcome_context.py
OutcomeDetails.py
OutcomeRow.py
persist_and_extract_positions__pipeline.py
persist_raw_pgns__pipeline.py
pgn_context_kwargs.py
pgn_headers.py
pgn_utils.py
PgnContext.py
PgnContextKwargs.py
PgnUpsertInputs.py
PgnUpsertPlan.py
PinDetector.py
pipeline_run_filters.py
pipeline.py
PositionContext.py
post_practice_attempt__api.py
postgres_analysis_enabled.py
postgres_connection.py
postgres_enabled.py
postgres_pgns_enabled.py
postgres_status.py
PostgresSettings.py
prepare_analysis_inputs__pipeline.py
prepare_dag_helpers__airflow.py
prepare_error__http_status.py
prepare_games_for_sync__pipeline.py
prepare_raw_pgn_context__pipeline.py
prime_dashboard_cache__api_cache.py
process_analysis_position__pipeline.py
ProgressCallback.py
raise_for_hash_mismatch__pipeline.py
raise_on_unexpected_kwargs__config.py
raise_unsupported_job__api_jobs.py
read_fork_severity_floor__config.py
read_optional_text__filesystem.py
record_daily_sync_complete__pipeline.py
record_ops_event.py
refresh_dashboard_cache_async__api_cache.py
refresh_metrics_result.py
refresh_raw_pgns_for_existing_positions__pipeline.py
request_chesscom_games__pipeline.py
require_api_token__request_auth.py
resolve_backfill_end_ms__airflow_jobs.py
resolve_field_value__config.py
resolve_pgn_hash__db_store.py
resolve_timestamp__db_store.py
resolve_unclear_outcome_context.py
run_analysis_and_metrics__pipeline.py
run_analysis_loop__pipeline.py
run_daily_game_sync__pipeline.py
run_daily_game_sync_internal__pipeline.py
run_migrations__pipeline.py
run_monitor_new_positions__pipeline.py
run_pipeline__api.py
run_refresh_metrics__pipeline.py
serialize_status.py
set_dashboard_cache__api_cache.py
should_override_failed_attempt__tactics.py
should_skip_backfill__pipeline.py
SkewerDetector.py
split_pgn_chunks.py
sql_tactics.py
stockfish_runner.py
StockfishEngine.py
sync_contexts.py
sync_postgres_analysis_results__pipeline.py
TacticContext.py
TacticDetails.py
TacticRow.py
TacticRowInput.py
tactics_search_filters.py
trend_stats.py
trigger_airflow_daily_sync__airflow_jobs.py
trigger_daily_sync__api_jobs.py
trigger_job__api_jobs.py
trigger_migrations__api_jobs.py
trigger_refresh_metrics__api_jobs.py
unclear_outcome_params.py
update_metrics_and_version__pipeline.py
upsert_analysis_tactic_with_outcome.py
upsert_postgres_raw_pgns_if_enabled__pipeline.py
upsert_postgres_raw_pgns.py
validate_backfill_window__airflow_jobs.py
validate_raw_pgn_hashes__pipeline.py
verify_stockfish_checksum.py
within_window__pipeline.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/use_cases/__init__.py">
"""Application use-case entrypoints."""

from tactix.app.use_cases.pipeline_support import (
    ANALYSIS_CHECKPOINTS,
    CHECKPOINT_UPDATES,
    COERCIONS,
    EMITTERS,
    NO_GAMES,
    PROFILES,
    PipelineAnalysisCheckpoint,
    PipelineCheckpointUpdates,
    PipelineCoercions,
    PipelineEmissions,
    PipelineNoGames,
    PipelineProfileFilters,
)

__all__ = [
    "ANALYSIS_CHECKPOINTS",
    "CHECKPOINT_UPDATES",
    "COERCIONS",
    "EMITTERS",
    "NO_GAMES",
    "PROFILES",
    "PipelineAnalysisCheckpoint",
    "PipelineCheckpointUpdates",
    "PipelineCoercions",
    "PipelineEmissions",
    "PipelineNoGames",
    "PipelineProfileFilters",
]
</file>

<file path="app/use_cases/pipeline_support.py">
"""Object-backed pipeline helpers consolidated from small pipeline modules."""

from __future__ import annotations

import json
import time
from dataclasses import dataclass

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import (
    CHESSCOM_BLACK_PROFILES,
    INDEX_OFFSET,
    LICHESS_BLACK_PROFILES,
    ZERO_COUNT,
    ProgressCallback,
    logger,
)
from tactix.FetchContext import FetchContext
from tactix.GameRow import GameRow
from tactix.infra.clients.chesscom_client import write_cursor as write_chesscom_cursor
from tactix.infra.clients.lichess_client import write_checkpoint
from tactix.latest_timestamp import latest_timestamp
from tactix.no_games_payload_contexts import (
    build_no_games_after_dedupe_payload_context,
    build_no_games_payload_context,
)
from tactix.ops_event import OpsEvent
from tactix.record_ops_event import record_ops_event
from tactix.sync_contexts import (
    DailySyncStartContext,
    FetchProgressContext,
    NoGamesAfterDedupeContext,
    NoGamesAfterDedupePayloadContext,
    NoGamesContext,
    NoGamesPayloadContext,
    WindowFilterContext,
)
from tactix.update_metrics_and_version__pipeline import _update_metrics_and_version
from tactix.utils.normalize_string import normalize_string


class PipelineCoercions:
    """Coerce pipeline input values into canonical types."""

    def coerce_int(self, value: object) -> int:
        """Return an integer representation of the value."""
        if isinstance(value, bool):
            return int(value)
        if isinstance(value, (int, float)):
            return int(value)
        if isinstance(value, str):
            try:
                return int(value)
            except ValueError:
                return 0
        return 0

    def coerce_str(self, value: object) -> str:
        """Return a string representation of the value."""
        if isinstance(value, str):
            return value
        if value is None:
            return ""
        return str(value)

    def coerce_pgn(self, value: object) -> str:
        """Return a string PGN representation."""
        if isinstance(value, (bytes, bytearray)):
            return value.decode("utf-8", errors="replace")
        return self.coerce_str(value)


@dataclass(frozen=True)
class PipelineProfileFilters:
    """Profile and side-to-move helpers."""

    def black_profiles_for_source(self, source: str) -> set[str] | None:
        """Return blacklisted profiles for the given source."""
        if source == "lichess":
            return LICHESS_BLACK_PROFILES
        if source == "chesscom":
            return CHESSCOM_BLACK_PROFILES
        return None

    def normalized_profile_for_source(self, settings: Settings, source: str) -> str:
        """Return a normalized profile string for the source."""
        profiles = {
            "lichess": settings.lichess_profile or settings.rapid_perf,
            "chesscom": settings.chesscom.profile or settings.chesscom.time_class,
        }
        raw_profile = profiles.get(source)
        return (raw_profile or "").strip().lower()

    def side_filter_for_profile(self, profile: str, black_profiles: set[str]) -> str | None:
        """Return a side filter for the provided profile."""
        return "black" if profile in black_profiles else None

    def resolve_side_to_move_filter(self, settings: Settings) -> str | None:
        """Resolve the side-to-move filter from settings."""
        source = normalize_string(settings.source)
        profile = self.normalized_profile_for_source(settings, source)
        black_profiles = self.black_profiles_for_source(source)
        if not profile or black_profiles is None:
            return None
        return self.side_filter_for_profile(profile, black_profiles)


@dataclass(frozen=True)
class PipelineEmissions:  # pylint: disable=too-many-arguments,too-many-positional-arguments
    """Emit progress and operational events for pipeline steps."""

    def emit_progress(self, progress: ProgressCallback | None, step: str, **fields: object) -> None:
        """Invoke progress callback with standard payload."""
        if progress is None:
            return
        payload: dict[str, object] = {"step": step, "timestamp": time.time()}
        payload.update(fields)
        progress(payload)

    def emit_fetch_progress(self, ctx: FetchProgressContext) -> None:
        """Emit progress for fetch steps."""
        self.emit_progress(
            ctx.progress,
            "fetch_games",
            source=ctx.settings.source,
            fetched_games=ctx.fetched_games,
            since_ms=ctx.fetch_context.since_ms,
            cursor=ctx.fetch_context.next_cursor or ctx.fetch_context.cursor_value,
            backfill=ctx.backfill_mode,
            backfill_start_ms=ctx.window_start_ms,
            backfill_end_ms=ctx.window_end_ms,
        )

    def emit_positions_ready(
        self,
        settings: Settings,
        progress: ProgressCallback | None,
        positions: list[dict[str, object]],
    ) -> None:
        """Emit progress when positions are ready."""
        self.emit_progress(
            progress,
            "positions_ready",
            source=settings.source,
            positions=len(positions),
        )

    def emit_daily_sync_start(self, ctx: DailySyncStartContext) -> None:
        """Emit progress and ops events for a sync start."""
        self.emit_progress(
            ctx.progress,
            "start",
            source=ctx.settings.source,
            message="Starting pipeline run",
        )
        record_ops_event(
            OpsEvent(
                settings=ctx.settings,
                component=ctx.settings.run_context,
                event_type="daily_game_sync_start",
                source=ctx.settings.source,
                profile=ctx.profile,
                metadata={
                    "backfill": ctx.backfill_mode,
                    "window_start_ms": ctx.window_start_ms,
                    "window_end_ms": ctx.window_end_ms,
                },
            )
        )

    def emit_backfill_window_filtered(
        self,
        settings: Settings,
        progress: ProgressCallback | None,
        filtered: int,
        window_start_ms: int | None,
        window_end_ms: int | None,
    ) -> None:  # pylint: disable=too-many-arguments,too-many-positional-arguments
        """Emit progress when backfill windows are filtered."""
        if not filtered:
            return
        logger.info(
            "Filtered %s games outside backfill window for source=%s",
            filtered,
            settings.source,
        )
        self.emit_progress(
            progress,
            "backfill_window_filtered",
            source=settings.source,
            filtered=filtered,
            backfill_start_ms=window_start_ms,
            backfill_end_ms=window_end_ms,
        )

    def maybe_emit_analysis_progress(
        self,
        progress: ProgressCallback | None,
        settings: Settings,
        idx: int,
        total_positions: int,
        progress_every: int,
    ) -> None:  # pylint: disable=too-many-arguments,too-many-positional-arguments
        """Emit analysis progress updates when due."""
        if not progress:
            return
        if idx == total_positions - INDEX_OFFSET or (
            (idx + INDEX_OFFSET) % progress_every == ZERO_COUNT
        ):
            self.emit_progress(
                progress,
                "analyze_positions",
                source=settings.source,
                analyzed=idx + INDEX_OFFSET,
                total=total_positions,
            )

    def maybe_emit_window_filtered(self, context: WindowFilterContext) -> None:
        """Emit backfill window metrics when needed."""
        if not context.backfill_mode or not context.window_filtered:
            return
        self.emit_backfill_window_filtered(
            context.settings,
            context.progress,
            context.window_filtered,
            context.window_start_ms,
            context.window_end_ms,
        )

    def log_skipped_backfill(self, settings: Settings, skipped_games: list[GameRow]) -> None:
        """Log skipped historical games in backfill mode."""
        if skipped_games:
            logger.info(
                "Skipping %s historical games already processed for source=%s",
                len(skipped_games),
                settings.source,
            )


@dataclass(frozen=True)
class PipelineNoGames:
    """Handle no-games branches in the pipeline."""

    def no_games_checkpoint(
        self, settings: Settings, backfill_mode: bool, fetch_context: FetchContext
    ) -> int | None:
        """Return the checkpoint value for no-games scenarios."""
        if backfill_mode or settings.source == "chesscom":
            return None
        return fetch_context.since_ms

    def no_games_cursor(self, backfill_mode: bool, fetch_context: FetchContext) -> str | None:
        """Resolve cursors when no games are returned."""
        if backfill_mode:
            return fetch_context.cursor_before
        return fetch_context.next_cursor or fetch_context.cursor_value

    def apply_no_games_dedupe_checkpoint(
        self,
        settings: Settings,
        backfill_mode: bool,
        fetch_context: FetchContext,
        last_timestamp_value: int,
    ) -> tuple[int | None, int]:
        """Apply checkpoint updates when no games were deduped."""
        if backfill_mode:
            return None, last_timestamp_value
        if settings.source == "chesscom":
            write_chesscom_cursor(settings.checkpoint_path, fetch_context.next_cursor)
            return None, last_timestamp_value
        checkpoint_value = max(fetch_context.since_ms, last_timestamp_value)
        write_checkpoint(settings.checkpoint_path, checkpoint_value)
        return checkpoint_value, checkpoint_value

    def build_no_games_payload(self, ctx: NoGamesPayloadContext) -> dict[str, object]:
        """Build payload when no games are fetched."""
        metrics_version = _update_metrics_and_version(ctx.settings, ctx.conn)
        checkpoint_ms = self.no_games_checkpoint(
            ctx.settings,
            ctx.backfill_mode,
            ctx.fetch_context,
        )
        cursor = self.no_games_cursor(ctx.backfill_mode, ctx.fetch_context)
        return {
            "source": ctx.settings.source,
            "user": ctx.settings.user,
            "fetched_games": 0,
            "raw_pgns_inserted": 0,
            "raw_pgns_hashed": 0,
            "raw_pgns_matched": 0,
            "positions": 0,
            "tactics": 0,
            "metrics_version": metrics_version,
            "checkpoint_ms": checkpoint_ms,
            "cursor": cursor,
            "last_timestamp_ms": ctx.last_timestamp_value or ctx.fetch_context.since_ms,
            "since_ms": ctx.fetch_context.since_ms,
            "window_filtered": ctx.window_filtered,
        }

    def build_no_games_after_dedupe_payload(
        self, ctx: NoGamesAfterDedupePayloadContext
    ) -> dict[str, object]:
        """Build payload when no games remain after dedupe."""
        metrics_version = _update_metrics_and_version(ctx.settings, ctx.conn)
        checkpoint_ms, last_timestamp_value = self.apply_no_games_dedupe_checkpoint(
            ctx.settings,
            ctx.backfill_mode,
            ctx.fetch_context,
            ctx.last_timestamp_value,
        )
        return {
            "source": ctx.settings.source,
            "user": ctx.settings.user,
            "fetched_games": len(ctx.games),
            "raw_pgns_inserted": 0,
            "raw_pgns_hashed": 0,
            "raw_pgns_matched": 0,
            "postgres_raw_pgns_inserted": 0,
            "positions": 0,
            "tactics": 0,
            "metrics_version": metrics_version,
            "checkpoint_ms": checkpoint_ms,
            "cursor": self.no_games_cursor(ctx.backfill_mode, ctx.fetch_context),
            "last_timestamp_ms": last_timestamp_value,
            "since_ms": ctx.fetch_context.since_ms,
            "window_filtered": ctx.window_filtered,
        }

    def handle_no_games(self, context: NoGamesContext) -> dict[str, object]:
        """Return the no-games payload and emit progress updates."""
        logger.info(
            "No new games for source=%s at checkpoint=%s",
            context.settings.source,
            context.fetch_context.since_ms,
        )
        EMITTERS.emit_progress(
            context.progress,
            "no_games",
            source=context.settings.source,
            message="No new games to process",
        )
        return self.build_no_games_payload(build_no_games_payload_context(context))

    def handle_no_games_after_dedupe(self, context: NoGamesAfterDedupeContext) -> dict[str, object]:
        """Return the no-games payload after dedupe."""
        logger.info(
            "No new games to process after backfill dedupe for source=%s",
            context.settings.source,
        )
        return self.build_no_games_after_dedupe_payload(
            build_no_games_after_dedupe_payload_context(context)
        )


@dataclass(frozen=True)
class PipelineAnalysisCheckpoint:
    """Read and write analysis checkpoint files."""

    def read_analysis_checkpoint(self, checkpoint_path, signature: str) -> int:
        """Return the checkpoint index for the signature or -1."""
        if not checkpoint_path.exists():
            return -1
        try:
            data = json.loads(checkpoint_path.read_text())
        except json.JSONDecodeError:
            return -1
        if data.get("signature") != signature:
            return -1
        return int(data.get("index", -1))

    def write_analysis_checkpoint(self, checkpoint_path, signature: str, index: int) -> None:
        """Persist analysis checkpoint metadata."""
        payload = {"signature": signature, "index": index}
        checkpoint_path.write_text(json.dumps(payload))

    def clear_analysis_checkpoint(self, checkpoint_path) -> None:
        """Remove analysis checkpoint data."""
        if checkpoint_path.exists():
            checkpoint_path.unlink()

    def maybe_clear_analysis_checkpoint(self, analysis_checkpoint_path) -> None:
        """Clear the analysis checkpoint if a path is provided."""
        if analysis_checkpoint_path is not None:
            self.clear_analysis_checkpoint(analysis_checkpoint_path)

    def maybe_write_analysis_checkpoint(
        self,
        analysis_checkpoint_path,
        analysis_signature: str,
        index: int,
    ) -> None:
        """Write the analysis checkpoint if enabled."""
        if analysis_checkpoint_path is None:
            return
        self.write_analysis_checkpoint(analysis_checkpoint_path, analysis_signature, index)


@dataclass(frozen=True)
class PipelineCheckpointUpdates:  # pylint: disable=too-many-arguments,too-many-positional-arguments
    """Resolve and persist checkpoint metadata."""

    def resolve_last_timestamp_value(self, games: list[GameRow], fallback: int) -> int:
        """Return the last timestamp from games or a fallback value."""
        if not games:
            return fallback
        return latest_timestamp(games) or fallback

    def resolve_chesscom_last_timestamp(
        self,
        fetch_context: FetchContext,
        games: list[GameRow],
        last_timestamp_value: int,
    ) -> int:
        """Resolve last chess.com timestamp values."""
        if fetch_context.chesscom_result:
            return fetch_context.chesscom_result.last_timestamp_ms
        if games:
            return latest_timestamp(games) or last_timestamp_value
        return last_timestamp_value

    def cursor_last_timestamp(self, cursor_value: str | None) -> int:
        """Parse last timestamps from cursor values."""
        if not cursor_value:
            return 0
        try:
            return int(cursor_value.split(":", 1)[0])
        except ValueError:
            return 0

    def update_chesscom_checkpoint(
        self,
        settings: Settings,
        fetch_context: FetchContext,
        games: list[GameRow],
        last_timestamp_value: int,
    ) -> tuple[int | None, int]:
        """Persist cursor and compute updated last timestamp."""
        write_chesscom_cursor(settings.checkpoint_path, fetch_context.next_cursor)
        last_timestamp_value = self.resolve_chesscom_last_timestamp(
            fetch_context,
            games,
            last_timestamp_value,
        )
        return None, last_timestamp_value

    def update_lichess_checkpoint(
        self,
        settings: Settings,
        fetch_context: FetchContext,
        games: list[GameRow],
    ) -> tuple[int | None, int]:
        """Persist and return the updated checkpoint value."""
        checkpoint_value = max(fetch_context.since_ms, latest_timestamp(games))
        write_checkpoint(settings.checkpoint_path, checkpoint_value)
        return checkpoint_value, checkpoint_value

    def update_daily_checkpoint(
        self,
        settings: Settings,
        backfill_mode: bool,
        fetch_context: FetchContext,
        games: list[GameRow],
        last_timestamp_value: int,
    ) -> tuple[int | None, int]:  # pylint: disable=too-many-arguments,too-many-positional-arguments
        """Update daily checkpoint values after sync."""
        if backfill_mode:
            return None, last_timestamp_value
        if settings.source == "chesscom":
            return self.update_chesscom_checkpoint(
                settings,
                fetch_context,
                games,
                last_timestamp_value,
            )
        return self.update_lichess_checkpoint(settings, fetch_context, games)


COERCIONS = PipelineCoercions()
PROFILES = PipelineProfileFilters()
EMITTERS = PipelineEmissions()
NO_GAMES = PipelineNoGames()
ANALYSIS_CHECKPOINTS = PipelineAnalysisCheckpoint()
CHECKPOINT_UPDATES = PipelineCheckpointUpdates()


def _coerce_int(value: object) -> int:
    return COERCIONS.coerce_int(value)


def _coerce_str(value: object) -> str:
    return COERCIONS.coerce_str(value)


def _coerce_pgn(value: object) -> str:
    return COERCIONS.coerce_pgn(value)


def _black_profiles_for_source(source: str) -> set[str] | None:
    return PROFILES.black_profiles_for_source(source)


def _normalized_profile_for_source(settings: Settings, source: str) -> str:
    return PROFILES.normalized_profile_for_source(settings, source)


def _side_filter_for_profile(profile: str, black_profiles: set[str]) -> str | None:
    return PROFILES.side_filter_for_profile(profile, black_profiles)


def _resolve_side_to_move_filter(settings: Settings) -> str | None:
    return PROFILES.resolve_side_to_move_filter(settings)


def _emit_progress(progress: ProgressCallback | None, step: str, **fields: object) -> None:
    EMITTERS.emit_progress(progress, step, **fields)


def _emit_fetch_progress(ctx: FetchProgressContext) -> None:
    EMITTERS.emit_fetch_progress(ctx)


def _emit_positions_ready(
    settings: Settings,
    progress: ProgressCallback | None,
    positions: list[dict[str, object]],
) -> None:
    EMITTERS.emit_positions_ready(settings, progress, positions)


def _emit_daily_sync_start(ctx: DailySyncStartContext) -> None:
    EMITTERS.emit_daily_sync_start(ctx)


def _emit_backfill_window_filtered(
    settings: Settings,
    progress: ProgressCallback | None,
    filtered: int,
    window_start_ms: int | None,
    window_end_ms: int | None,
) -> None:
    EMITTERS.emit_backfill_window_filtered(
        settings,
        progress,
        filtered,
        window_start_ms,
        window_end_ms,
    )


def _maybe_emit_analysis_progress(
    progress: ProgressCallback | None,
    settings: Settings,
    idx: int,
    total_positions: int,
    progress_every: int,
) -> None:
    EMITTERS.maybe_emit_analysis_progress(
        progress,
        settings,
        idx,
        total_positions,
        progress_every,
    )


def _maybe_emit_window_filtered(context: WindowFilterContext) -> None:
    EMITTERS.maybe_emit_window_filtered(context)


def _log_skipped_backfill(settings: Settings, skipped_games: list[GameRow]) -> None:
    EMITTERS.log_skipped_backfill(settings, skipped_games)


def _no_games_checkpoint(
    settings: Settings, backfill_mode: bool, fetch_context: FetchContext
) -> int | None:
    return NO_GAMES.no_games_checkpoint(settings, backfill_mode, fetch_context)


def _no_games_cursor(backfill_mode: bool, fetch_context: FetchContext) -> str | None:
    return NO_GAMES.no_games_cursor(backfill_mode, fetch_context)


def _apply_no_games_dedupe_checkpoint(
    settings: Settings,
    backfill_mode: bool,
    fetch_context: FetchContext,
    last_timestamp_value: int,
) -> tuple[int | None, int]:
    return NO_GAMES.apply_no_games_dedupe_checkpoint(
        settings,
        backfill_mode,
        fetch_context,
        last_timestamp_value,
    )


def _build_no_games_payload(ctx: NoGamesPayloadContext) -> dict[str, object]:
    return NO_GAMES.build_no_games_payload(ctx)


def _build_no_games_after_dedupe_payload(
    ctx: NoGamesAfterDedupePayloadContext,
) -> dict[str, object]:
    return NO_GAMES.build_no_games_after_dedupe_payload(ctx)


def _handle_no_games(context: NoGamesContext) -> dict[str, object]:
    return NO_GAMES.handle_no_games(context)


def _handle_no_games_after_dedupe(context: NoGamesAfterDedupeContext) -> dict[str, object]:
    return NO_GAMES.handle_no_games_after_dedupe(context)


def _read_analysis_checkpoint(checkpoint_path, signature: str) -> int:
    return ANALYSIS_CHECKPOINTS.read_analysis_checkpoint(checkpoint_path, signature)


def _write_analysis_checkpoint(checkpoint_path, signature: str, index: int) -> None:
    ANALYSIS_CHECKPOINTS.write_analysis_checkpoint(checkpoint_path, signature, index)


def _clear_analysis_checkpoint(checkpoint_path) -> None:
    ANALYSIS_CHECKPOINTS.clear_analysis_checkpoint(checkpoint_path)


def _maybe_clear_analysis_checkpoint(analysis_checkpoint_path) -> None:
    ANALYSIS_CHECKPOINTS.maybe_clear_analysis_checkpoint(analysis_checkpoint_path)


def _maybe_write_analysis_checkpoint(
    analysis_checkpoint_path,
    analysis_signature: str,
    index: int,
) -> None:
    ANALYSIS_CHECKPOINTS.maybe_write_analysis_checkpoint(
        analysis_checkpoint_path,
        analysis_signature,
        index,
    )


def _resolve_last_timestamp_value(games: list[GameRow], fallback: int) -> int:
    return CHECKPOINT_UPDATES.resolve_last_timestamp_value(games, fallback)


def _resolve_chesscom_last_timestamp(
    fetch_context: FetchContext,
    games: list[GameRow],
    last_timestamp_value: int,
) -> int:
    return CHECKPOINT_UPDATES.resolve_chesscom_last_timestamp(
        fetch_context,
        games,
        last_timestamp_value,
    )


def _cursor_last_timestamp(cursor_value: str | None) -> int:
    return CHECKPOINT_UPDATES.cursor_last_timestamp(cursor_value)


def _update_chesscom_checkpoint(
    settings: Settings,
    fetch_context: FetchContext,
    games: list[GameRow],
    last_timestamp_value: int,
) -> tuple[int | None, int]:
    return CHECKPOINT_UPDATES.update_chesscom_checkpoint(
        settings,
        fetch_context,
        games,
        last_timestamp_value,
    )


def _update_lichess_checkpoint(
    settings: Settings,
    fetch_context: FetchContext,
    games: list[GameRow],
) -> tuple[int | None, int]:
    return CHECKPOINT_UPDATES.update_lichess_checkpoint(settings, fetch_context, games)


def _update_daily_checkpoint(
    settings: Settings,
    backfill_mode: bool,
    fetch_context: FetchContext,
    games: list[GameRow],
    last_timestamp_value: int,
) -> tuple[int | None, int]:
    return CHECKPOINT_UPDATES.update_daily_checkpoint(
        settings,
        backfill_mode,
        fetch_context,
        games,
        last_timestamp_value,
    )


__all__ = [
    "ANALYSIS_CHECKPOINTS",
    "CHECKPOINT_UPDATES",
    "COERCIONS",
    "EMITTERS",
    "NO_GAMES",
    "PROFILES",
    "PipelineAnalysisCheckpoint",
    "PipelineCheckpointUpdates",
    "PipelineCoercions",
    "PipelineEmissions",
    "PipelineNoGames",
    "PipelineProfileFilters",
    "_apply_no_games_dedupe_checkpoint",
    "_black_profiles_for_source",
    "_build_no_games_after_dedupe_payload",
    "_build_no_games_payload",
    "_clear_analysis_checkpoint",
    "_coerce_int",
    "_coerce_pgn",
    "_coerce_str",
    "_cursor_last_timestamp",
    "_emit_backfill_window_filtered",
    "_emit_daily_sync_start",
    "_emit_fetch_progress",
    "_emit_positions_ready",
    "_emit_progress",
    "_handle_no_games",
    "_handle_no_games_after_dedupe",
    "_log_skipped_backfill",
    "_maybe_clear_analysis_checkpoint",
    "_maybe_emit_analysis_progress",
    "_maybe_emit_window_filtered",
    "_maybe_write_analysis_checkpoint",
    "_no_games_checkpoint",
    "_no_games_cursor",
    "_normalized_profile_for_source",
    "_read_analysis_checkpoint",
    "_resolve_chesscom_last_timestamp",
    "_resolve_last_timestamp_value",
    "_resolve_side_to_move_filter",
    "_side_filter_for_profile",
    "_update_chesscom_checkpoint",
    "_update_daily_checkpoint",
    "_update_lichess_checkpoint",
    "_write_analysis_checkpoint",
]
</file>

<file path="app/__init__.py">

</file>

<file path="chess_clients/__init__.py">
"""Public exports for chess client abstractions."""

from __future__ import annotations

from importlib import import_module
from typing import TYPE_CHECKING

from tactix.chess_clients.base_chess_client import BaseChessClient, BaseChessClientContext
from tactix.chess_clients.chess_fetch_request import ChessFetchRequest
from tactix.chess_clients.chess_fetch_result import ChessFetchResult
from tactix.chess_clients.chess_game_row import ChessGameRow

__all__ = [
    "BaseChessClient",
    "BaseChessClientContext",
    "ChessFetchRequest",
    "ChessFetchResult",
    "ChessGameRow",
    "ChesscomClient",
    "LichessClient",
]


def __getattr__(name: str):
    if name == "ChesscomClient":
        return import_module("tactix.infra.clients.chesscom_client").ChesscomClient
    if name == "LichessClient":
        return import_module("tactix.infra.clients.lichess_client").LichessClient
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")


def __dir__() -> list[str]:
    return sorted(__all__)


if TYPE_CHECKING:
    from tactix.infra.clients.chesscom_client import ChesscomClient
    from tactix.infra.clients.lichess_client import LichessClient

    __getattr__("ChesscomClient")
    __getattr__("LichessClient")
    __dir__()
</file>

<file path="chess_clients/base_chess_client.py">
"""Base chess client classes and helpers."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from datetime import UTC, datetime

from ..config import Settings
from .chess_fetch_request import ChessFetchRequest
from .chess_fetch_result import ChessFetchResult
from .chess_game_row import ChessGameRow


@dataclass(slots=True)
class BaseChessClientContext:
    """Shared context for chess API clients.

    Attributes:
        settings: Application settings used for API calls.
        logger: Logger for client-specific messages.
    """

    settings: Settings
    logger: logging.Logger


class BaseChessClient:
    """Base class for chess API clients.

    Subclasses are expected to implement `fetch_incremental_games`.
    """

    def __init__(self, context: BaseChessClientContext) -> None:
        """Initialize the client with shared context.

        Args:
            context: Base context containing settings and logger.
        """

        self._context = context

    @property
    def settings(self) -> Settings:
        """Expose the settings from the context.

        Returns:
            The active `Settings` instance.
        """

        return self._context.settings

    @property
    def logger(self) -> logging.Logger:
        """Expose the logger from the context.

        Returns:
            Logger used by the client.
        """

        return self._context.logger

    def fetch_incremental_games(self, request: ChessFetchRequest) -> ChessFetchResult:
        """Fetch games incrementally.

        Args:
            request: Request parameters for the incremental fetch.

        Returns:
            A `ChessFetchResult` containing the games and cursor metadata.

        Raises:
            NotImplementedError: When the subclass does not implement this method.

        Example:
            >>> client.fetch_incremental_games(ChessFetchRequest(since_ms=0))
        """

        raise NotImplementedError("Subclasses must implement fetch_incremental_games")

    def _now_utc(self) -> datetime:
        """Return the current UTC time.

        Returns:
            Current UTC datetime.
        """

        return datetime.now(UTC)

    def _build_game_row(self, game_id: str, pgn: str, last_timestamp_ms: int) -> ChessGameRow:
        """Create a normalized game row for the active settings.

        Args:
            game_id: Stable identifier for the game.
            pgn: Raw PGN text.
            last_timestamp_ms: Last move timestamp in milliseconds.

        Returns:
            Normalized `ChessGameRow` instance.
        """

        return ChessGameRow(
            game_id=game_id,
            user=self.settings.user,
            source=self.settings.source,
            fetched_at=self._now_utc(),
            pgn=pgn,
            last_timestamp_ms=last_timestamp_ms,
        )

    def _build_fetch_result(
        self,
        games: list[dict],
        next_cursor: str | None,
        last_timestamp_ms: int,
    ) -> ChessFetchResult:
        """Build a fetch result model from raw values.

        Args:
            games: List of normalized game rows.
            next_cursor: Cursor token for the next fetch.
            last_timestamp_ms: Latest timestamp across all games.

        Returns:
            A `ChessFetchResult` instance.
        """

        return ChessFetchResult(
            games=games,
            next_cursor=next_cursor,
            last_timestamp_ms=last_timestamp_ms,
        )
</file>

<file path="chess_clients/chess_fetch_request.py">
"""Request model for chess fetches."""

from pydantic import BaseModel


class ChessFetchRequest(BaseModel):
    """Request model for incremental fetches.

    Attributes:
        since_ms: Lower bound timestamp (inclusive) in milliseconds.
        until_ms: Optional upper bound timestamp (exclusive) in milliseconds.
        cursor: Optional cursor token for cursor-based APIs.
        full_history: Whether to fetch full history (bypass window limits).

    Example:
        >>> ChessFetchRequest(since_ms=0, cursor=None, full_history=False)
    """

    since_ms: int = 0
    until_ms: int | None = None
    cursor: str | None = None
    full_history: bool = False
</file>

<file path="chess_clients/chess_fetch_result.py">
"""Response model for chess fetch results."""

from pydantic import BaseModel, Field


class ChessFetchResult(BaseModel):
    """Response model for incremental fetches.

    Attributes:
        games: Parsed game rows as dictionaries.
        next_cursor: Cursor to resume from on the next request.
        last_timestamp_ms: Latest timestamp seen across all games.

    Example:
        >>> ChessFetchResult(games=[], next_cursor=None, last_timestamp_ms=0)
    """

    games: list[dict] = Field(default_factory=list)
    next_cursor: str | None = None
    last_timestamp_ms: int = 0
</file>

<file path="chess_clients/chess_game_row.py">
"""Models and helpers for chess game rows."""

from collections.abc import Callable, Iterable
from datetime import UTC, datetime
from typing import cast

from pydantic import BaseModel

from tactix.chess_clients.GameRowInputs import GameRowInputs


class ChessGameRow(BaseModel):
    """Represents a normalized chess game row.

    Attributes:
        game_id: Stable identifier for the game.
        user: Username associated with the game.
        source: Data source (e.g., "lichess" or "chesscom").
        fetched_at: Timestamp when the game was fetched.
        pgn: Raw PGN text.
        last_timestamp_ms: Last move timestamp in milliseconds.
    """

    game_id: str
    user: str
    source: str
    fetched_at: datetime
    pgn: str
    last_timestamp_ms: int


def build_game_row_dict[T: "ChessGameRow"](
    inputs: "GameRowInputs[T]",
) -> dict[str, object]:
    """Build a normalized game row dictionary from inputs."""
    fetched_at = datetime.now(UTC) if inputs.fetched_at is None else inputs.fetched_at
    model_cls = inputs.model_cls or cast(type[T], ChessGameRow)
    row = ChessGameRow(
        game_id=inputs.game_id,
        user=inputs.user,
        source=inputs.source,
        fetched_at=fetched_at,
        pgn=inputs.pgn,
        last_timestamp_ms=inputs.last_timestamp_ms,
    )
    return coerce_game_row_dict(row, model_cls=model_cls)


def coerce_game_row_dict[T: "ChessGameRow"](
    row: ChessGameRow,
    *,
    model_cls: type[T] | None = None,
) -> dict[str, object]:
    """Coerce a game row to the specified model and return a dict."""
    if model_cls is None:
        model_cls = cast(type[T], ChessGameRow)
    return model_cls.model_validate(row.model_dump()).model_dump()


def coerce_game_rows[T: "ChessGameRow"](
    rows: Iterable[dict],
    model_cls: type[T],
) -> list[dict]:
    """Coerce multiple row dicts to a model and return dicts."""
    return [model_cls.model_validate(row).model_dump() for row in rows]


def coerce_rows_for_model[T: "ChessGameRow"](
    model_cls: type[T],
) -> Callable[[Iterable[dict]], list[dict]]:
    """Return a helper that coerces rows to the model class."""

    def _coerce(rows: Iterable[dict]) -> list[dict]:
        return coerce_game_rows(rows, model_cls)

    return _coerce
</file>

<file path="chess_clients/fetch_helpers.py">
"""Helpers for chess client fetch flows."""

from __future__ import annotations

from collections.abc import Callable
from typing import Any

from tactix.chess_clients.base_chess_client import ChessFetchRequest, ChessFetchResult
from tactix.chess_clients.fixture_helpers import should_use_fixture_games
from tactix.load_fixture_games import load_fixture_games


def should_use_fixture_data(token: str | None, use_fixture_when_no_token: bool) -> bool:
    """Return whether fixture data should be used for a fetch."""
    return should_use_fixture_games(token, use_fixture_when_no_token)


def use_fixture_games(token: str | None, use_fixture_when_no_token: bool) -> bool:
    """Alias for fixture usage decision."""
    return should_use_fixture_data(token, use_fixture_when_no_token)


__all__ = [
    "load_fixture_games",
    "run_incremental_fetch",
    "should_use_fixture_data",
    "use_fixture_games",
]


def run_incremental_fetch(
    *,
    build_client: Callable[[], Any],
    request: ChessFetchRequest,
) -> ChessFetchResult:
    """Run an incremental fetch using a constructed client."""
    return build_client().fetch_incremental_games(request)
</file>

<file path="chess_clients/fixture_helpers.py">
"""Fixture helpers for chess client tests."""

from __future__ import annotations


def should_use_fixture_games(token: str | None, use_fixture_when_no_token: bool) -> bool:
    """Return True when fixtures should be used in place of API calls."""
    return bool(not token and use_fixture_when_no_token)
</file>

<file path="chess_clients/GameRowInputs.py">
"""Dataclass inputs for building chess game rows."""

# pylint: disable=invalid-name

from dataclasses import dataclass
from datetime import datetime
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from tactix.chess_clients.chess_game_row import ChessGameRow


@dataclass(frozen=True)
class GameRowInputs[T: "ChessGameRow"]:
    """Input fields used to build a chess game row."""

    game_id: str
    pgn: str
    last_timestamp_ms: int
    user: str
    source: str
    fetched_at: datetime | None = None
    model_cls: type[T] | None = None
</file>

<file path="chess_clients/mock_chess_client.py">
"""Mock chess client implementation."""

from __future__ import annotations

from collections.abc import Mapping
from typing import cast

from tactix.chess_clients.base_chess_client import (
    BaseChessClient,
    BaseChessClientContext,
    ChessFetchRequest,
    ChessFetchResult,
)
from tactix.chess_clients.chess_game_row import ChessGameRow


class MockChessClient(BaseChessClient):
    """Mock chess client that returns in-memory game rows."""

    def __init__(
        self,
        context: BaseChessClientContext,
        games: list[ChessGameRow | Mapping[str, object]] | None = None,
        next_cursor: str | None = None,
        page_size: int | None = None,
    ) -> None:
        """Initialize the mock client with optional games and paging."""
        super().__init__(context)
        self._games = list(games or [])
        self._next_cursor = next_cursor
        self._page_size = page_size

    def _normalize_games(self) -> list[dict[str, object]]:
        """Normalize stored games into dictionaries."""
        games: list[dict[str, object]] = []
        for game in self._games:
            if isinstance(game, ChessGameRow):
                games.append(game.model_dump())
            else:
                games.append(dict(game))
        return games

    @staticmethod
    def _apply_window(
        games: list[dict[str, object]],
        request: ChessFetchRequest,
    ) -> list[dict[str, object]]:
        """Apply time window filtering to games."""
        if request.full_history:
            return games
        since_ms = request.since_ms or 0
        until_ms = request.until_ms
        return [
            row
            for row in games
            if _row_in_window(cast(int, row.get("last_timestamp_ms", 0)), since_ms, until_ms)
        ]

    def _apply_cursor(
        self,
        games: list[dict[str, object]],
        request: ChessFetchRequest,
    ) -> tuple[list[dict[str, object]], str | None]:
        """Apply cursor pagination to games."""
        offset = 0
        if request.cursor:
            try:
                offset = int(request.cursor)
            except ValueError:
                offset = 0
        remaining = games[offset:]
        next_cursor = self._next_cursor
        if self._page_size is None:
            return remaining, next_cursor
        page = remaining[: self._page_size]
        next_cursor = str(offset + len(page)) if offset + len(page) < len(games) else None
        return page, next_cursor

    def fetch_incremental_games(self, request: ChessFetchRequest) -> ChessFetchResult:
        """Return paged games based on the request."""
        games = self._normalize_games()
        games = self._apply_window(games, request)
        games, next_cursor = self._apply_cursor(games, request)
        last_ts = max((cast(int, row.get("last_timestamp_ms", 0)) for row in games), default=0)
        return self._build_fetch_result(games, next_cursor, last_ts)


def _row_in_window(last_ts: int, since_ms: int, until_ms: int | None) -> bool:
    return last_ts >= since_ms and (until_ms is None or last_ts < until_ms)
</file>

<file path="db/__init__.py">

</file>

<file path="db/_append_date_range_filters.py">
"""Append date range filters for SQL queries."""

from __future__ import annotations

from datetime import date, datetime


def _append_date_range_filters(
    conditions: list[str],
    params: list[object],
    start_date: date | datetime | None,
    end_date: date | datetime | None,
    column: str,
) -> None:
    """Append date range conditions when start/end dates are provided."""
    if start_date is not None:
        conditions.append(f"CAST({column} AS DATE) >= ?")
        params.append(start_date.date() if isinstance(start_date, datetime) else start_date)
    if end_date is not None:
        conditions.append(f"CAST({column} AS DATE) <= ?")
        params.append(end_date.date() if isinstance(end_date, datetime) else end_date)
</file>

<file path="db/_append_optional_filter.py">
"""Append optional SQL filters."""

from __future__ import annotations


def _append_optional_filter(
    conditions: list[str],
    params: list[object],
    clause: str,
    value: object | None,
) -> None:
    """Append clause and parameter when value is present."""
    if value is None:
        return
    conditions.append(clause)
    params.append(value)
</file>

<file path="db/_append_time_control_filter.py">
"""Append time control filters for SQL queries."""

from __future__ import annotations


def _append_time_control_filter(
    conditions: list[str],
    params: list[object],
    time_control: str | None,
    column: str,
) -> None:
    """Append a time control filter when provided."""
    if not time_control:
        return
    normalized = str(time_control).lower()
    base_expr = f"try_cast(split_part({column}, '+', 1) as INTEGER)"
    ranges: dict[str, tuple[int | None, int | None]] = {
        "bullet": (None, 180),
        "blitz": (180, 600),
        "rapid": (600, 1800),
        "classical": (1800, 7200),
        "correspondence": (7200, None),
    }

    if normalized in ranges:
        lower, upper = ranges[normalized]
        clauses: list[str] = []
        if lower is not None:
            clauses.append(f"{base_expr} > {lower}")
        if upper is not None:
            clauses.append(f"{base_expr} <= {upper}")
        range_expr = " AND ".join(clauses)
        conditions.append(f"({column} = ? OR ({range_expr}))")
        params.append(normalized)
    else:
        conditions.append(f"{column} = ?")
        params.append(time_control)
</file>

<file path="db/_build_legacy_raw_pgn_inserts.py">
"""Build inserts for legacy raw PGN rows."""

from __future__ import annotations

from collections.abc import Iterable
from datetime import UTC, datetime

from tactix.define_base_db_store__db_store import BaseDbStore
from tactix.extract_pgn_metadata import extract_pgn_metadata


def _build_legacy_raw_pgn_inserts(
    rows: Iterable[tuple[object, ...]],
) -> list[tuple[object, ...]]:
    """Return insert tuples for legacy raw PGN rows."""
    inserts: list[tuple[object, ...]] = []
    for idx, row in enumerate(rows, start=1):
        game_id, user, source, fetched_at, pgn, last_timestamp_ms, cursor = row
        pgn_text = str(pgn or "")
        metadata = extract_pgn_metadata(pgn_text, str(user or ""))
        inserts.append(
            (
                idx,
                game_id,
                user,
                source,
                fetched_at,
                pgn_text,
                BaseDbStore.hash_pgn(pgn_text),
                1,
                metadata.get("user_rating"),
                metadata.get("time_control"),
                datetime.now(UTC),
                last_timestamp_ms,
                cursor,
            )
        )
    return inserts
</file>

<file path="db/_build_raw_pgn_upsert_plan.py">
"""Build DuckDB raw PGN upsert plans."""

from __future__ import annotations

from collections.abc import Mapping

import duckdb

from tactix._build_pgn_upsert_plan import _build_pgn_upsert_plan


def _build_raw_pgn_upsert_plan(
    conn: duckdb.DuckDBPyConnection,
    row: Mapping[str, object],
    game_id: str,
    source: str,
    latest_cache: dict[tuple[str, str], tuple[str | None, int]],
):
    """Return an upsert plan for a raw PGN row."""
    cached = latest_cache.get((game_id, source))
    if cached:
        latest_hash, latest_version = cached
    else:
        record = conn.execute(
            """
            SELECT pgn_hash, pgn_version
            FROM raw_pgns
            WHERE game_id = ? AND source = ?
            ORDER BY pgn_version DESC
            LIMIT 1
            """,
            [game_id, source],
        ).fetchone()
        if record:
            latest_hash = record[0]
            latest_version = int(record[1] or 0)
        else:
            latest_hash = None
            latest_version = 0
    return _build_pgn_upsert_plan(row, latest_hash, latest_version)
</file>

<file path="db/_drop_table_if_exists.py">
"""Drop a DuckDB table when present."""

from __future__ import annotations

import duckdb


def _drop_table_if_exists(conn: duckdb.DuckDBPyConnection, table: str) -> None:
    """Drop a table if it exists."""
    conn.execute(f"DROP TABLE IF EXISTS {table}")
</file>

<file path="db/_fetch_next_raw_pgn_id.py">
"""Fetch the next raw PGN identifier."""

from __future__ import annotations

import duckdb


def _fetch_next_raw_pgn_id(conn: duckdb.DuckDBPyConnection) -> int:
    """Return the next raw PGN id for inserts."""
    row = conn.execute("SELECT MAX(raw_pgn_id) FROM raw_pgns").fetchone()
    return int(row[0] or 0) if row else 0
</file>

<file path="db/_insert_raw_pgn_plan.py">
"""Insert a raw PGN row from a plan."""

from __future__ import annotations

import duckdb

from tactix.db.RawPgnInsertInputs import RawPgnInsertInputs


def _insert_raw_pgn_plan(
    conn: duckdb.DuckDBPyConnection,
    inputs: RawPgnInsertInputs,
) -> None:
    """Insert a raw PGN row for a prepared plan."""
    metadata = inputs.plan.metadata
    conn.execute(
        """
        INSERT INTO raw_pgns (
            raw_pgn_id,
            game_id,
            user,
            source,
            fetched_at,
            pgn,
            pgn_hash,
            pgn_version,
            user_rating,
            time_control,
            ingested_at,
            last_timestamp_ms,
            cursor
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
        [
            inputs.raw_pgn_id,
            inputs.game_id,
            inputs.row.get("user"),
            inputs.source,
            inputs.plan.fetched_at,
            inputs.plan.pgn_text,
            inputs.plan.pgn_hash,
            inputs.plan.pgn_version,
            metadata.get("user_rating"),
            metadata.get("time_control"),
            inputs.plan.ingested_at,
            inputs.plan.last_timestamp_ms,
            inputs.plan.cursor,
        ],
    )
</file>

<file path="db/_migration_add_columns.py">
"""Add missing columns to raw PGN tables."""

from __future__ import annotations

import duckdb


def _migration_add_columns(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure recent columns exist on raw PGN rows."""
    columns = {row[1] for row in conn.execute("PRAGMA table_info('raw_pgns')").fetchall()}
    additions = {
        "user_rating": "ALTER TABLE raw_pgns ADD COLUMN user_rating INTEGER",
        "time_control": "ALTER TABLE raw_pgns ADD COLUMN time_control TEXT",
        "ingested_at": "ALTER TABLE raw_pgns ADD COLUMN ingested_at TIMESTAMP",
        "cursor": "ALTER TABLE raw_pgns ADD COLUMN cursor TEXT",
        "last_timestamp_ms": "ALTER TABLE raw_pgns ADD COLUMN last_timestamp_ms BIGINT",
    }
    for column, statement in additions.items():
        if column not in columns:
            conn.execute(statement)
</file>

<file path="db/_migration_add_pipeline_views.py">
"""Add pipeline compatibility views and columns."""

from __future__ import annotations

import duckdb

from tactix.db.raw_pgns_queries import latest_raw_pgns_query


def _migration_add_pipeline_views(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure pipeline compatibility views and columns exist."""
    columns = {row[1] for row in conn.execute("PRAGMA table_info('positions')").fetchall()}
    if "side_to_move" not in columns:
        conn.execute("ALTER TABLE positions ADD COLUMN side_to_move TEXT")
    if "user_to_move" not in columns:
        conn.execute("ALTER TABLE positions ADD COLUMN user_to_move BOOLEAN DEFAULT TRUE")
        conn.execute("UPDATE positions SET user_to_move = TRUE WHERE user_to_move IS NULL")

    conn.execute(
        f"""
        CREATE OR REPLACE VIEW games AS
        WITH latest_pgns AS (
            {latest_raw_pgns_query()}
        )
        SELECT
            game_id,
            user,
            source,
            pgn,
            pgn_hash,
            pgn_version,
            time_control,
            user_rating,
            fetched_at,
            ingested_at,
            last_timestamp_ms,
            cursor,
            to_timestamp(last_timestamp_ms / 1000) AS played_at
        FROM latest_pgns
        """
    )

    conn.execute(
        """
        CREATE OR REPLACE VIEW user_moves AS
        SELECT
            position_id AS user_move_id,
            position_id,
            game_id,
            user,
            source,
            uci AS played_uci,
            san AS played_san,
            created_at
        FROM positions
        """
    )

    conn.execute(
        """
        CREATE OR REPLACE VIEW opportunities AS
        SELECT
            t.tactic_id AS opportunity_id,
            t.position_id,
            p.game_id,
            p.user,
            p.source,
            t.motif,
            t.severity,
            t.best_uci,
            t.best_san,
            t.explanation,
            t.eval_cp,
            t.created_at
        FROM tactics t
        INNER JOIN positions p ON p.position_id = t.position_id
        """
    )

    conn.execute(
        """
        CREATE OR REPLACE VIEW conversions AS
        SELECT
            o.outcome_id AS conversion_id,
            o.tactic_id AS opportunity_id,
            opp.position_id,
            opp.game_id,
            opp.user,
            opp.source,
            o.result,
            o.user_uci,
            o.eval_delta,
            o.created_at
        FROM tactic_outcomes o
        INNER JOIN opportunities opp ON opp.opportunity_id = o.tactic_id
        """
    )

    conn.execute(
        """
        CREATE OR REPLACE VIEW practice_queue AS
        SELECT
            opp.opportunity_id,
            conv.conversion_id,
            conv.result,
            opp.position_id,
            opp.game_id,
            opp.user,
            opp.source,
            opp.motif,
            opp.severity,
            opp.best_uci,
            opp.best_san,
            opp.explanation,
            opp.eval_cp,
            p.fen,
            p.uci AS position_uci,
            p.san,
            p.ply,
            p.move_number,
            p.side_to_move,
            p.clock_seconds,
            opp.created_at AS opportunity_created_at,
            conv.created_at AS conversion_created_at
        FROM conversions conv
        INNER JOIN opportunities opp ON opp.opportunity_id = conv.opportunity_id
        INNER JOIN positions p ON p.position_id = opp.position_id
        WHERE conv.result = 'missed'
        """
    )


__all__ = ["_migration_add_pipeline_views"]
</file>

<file path="db/_migration_add_position_legality.py">
"""Add legality column to positions."""

from __future__ import annotations

import duckdb


def _migration_add_position_legality(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure positions include legality metadata."""
    columns = {row[1] for row in conn.execute("PRAGMA table_info('positions')").fetchall()}
    if "is_legal" not in columns:
        conn.execute("ALTER TABLE positions ADD COLUMN is_legal BOOLEAN")
</file>

<file path="db/_migration_add_tactic_explanations.py">
"""Add explanation column to tactics."""

from __future__ import annotations

import duckdb


def _migration_add_tactic_explanations(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure tactics include explanation text."""
    columns = {row[1] for row in conn.execute("PRAGMA table_info('tactics')").fetchall()}
    if "explanation" not in columns:
        conn.execute("ALTER TABLE tactics ADD COLUMN explanation TEXT")
</file>

<file path="db/_migration_add_training_attempt_latency.py">
"""Add latency column to training attempts."""

from __future__ import annotations

import duckdb


def _migration_add_training_attempt_latency(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure training attempts include latency metadata."""
    columns = {row[1] for row in conn.execute("PRAGMA table_info('training_attempts')").fetchall()}
    if "latency_ms" not in columns:
        conn.execute("ALTER TABLE training_attempts ADD COLUMN latency_ms BIGINT")
</file>

<file path="db/_migration_base_tables.py">
"""Create base tables for DuckDB storage."""

from __future__ import annotations

import duckdb

RAW_PGNS_SCHEMA = """
CREATE TABLE IF NOT EXISTS raw_pgns (
    raw_pgn_id BIGINT PRIMARY KEY,
    game_id TEXT,
    user TEXT,
    source TEXT,
    fetched_at TIMESTAMP,
    pgn TEXT,
    pgn_hash TEXT,
    pgn_version INTEGER,
    user_rating INTEGER,
    time_control TEXT,
    ingested_at TIMESTAMP,
    last_timestamp_ms BIGINT,
    cursor TEXT
);
"""

POSITIONS_SCHEMA = """
CREATE TABLE IF NOT EXISTS positions (
    position_id BIGINT PRIMARY KEY,
    game_id TEXT,
    user TEXT,
    source TEXT,
    fen TEXT,
    ply INTEGER,
    move_number INTEGER,
    side_to_move TEXT,
    user_to_move BOOLEAN DEFAULT TRUE,
    uci TEXT,
    san TEXT,
    clock_seconds DOUBLE,
    is_legal BOOLEAN,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

TACTICS_SCHEMA = """
CREATE TABLE IF NOT EXISTS tactics (
    tactic_id BIGINT PRIMARY KEY,
    game_id TEXT,
    position_id BIGINT,
    motif TEXT,
    severity DOUBLE,
    best_uci TEXT,
    best_san TEXT,
    explanation TEXT,
    eval_cp INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

TACTIC_OUTCOMES_SCHEMA = """
CREATE TABLE IF NOT EXISTS tactic_outcomes (
    outcome_id BIGINT PRIMARY KEY,
    tactic_id BIGINT,
    result TEXT,
    user_uci TEXT,
    eval_delta INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

METRICS_VERSION_SCHEMA = """
CREATE TABLE IF NOT EXISTS metrics_version (
    version BIGINT,
    updated_at TIMESTAMP
);
"""

METRICS_SUMMARY_SCHEMA = """
CREATE TABLE IF NOT EXISTS metrics_summary (
    source TEXT,
    metric_type TEXT,
    motif TEXT,
    window_days INTEGER,
    trend_date DATE,
    rating_bucket TEXT,
    time_control TEXT,
    total BIGINT,
    found BIGINT,
    missed BIGINT,
    failed_attempt BIGINT,
    unclear BIGINT,
    found_rate DOUBLE,
    miss_rate DOUBLE,
    updated_at TIMESTAMP
);
"""

TRAINING_ATTEMPTS_SCHEMA = """
CREATE TABLE IF NOT EXISTS training_attempts (
    attempt_id BIGINT PRIMARY KEY,
    tactic_id BIGINT,
    position_id BIGINT,
    source TEXT,
    attempted_uci TEXT,
    correct BOOLEAN,
    success BOOLEAN,
    best_uci TEXT,
    motif TEXT,
    severity DOUBLE,
    eval_delta INTEGER,
    latency_ms BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""


def _migration_base_tables(conn: duckdb.DuckDBPyConnection) -> None:
    """Create base tables required by the DuckDB store."""
    conn.execute(RAW_PGNS_SCHEMA)
    conn.execute(POSITIONS_SCHEMA)
    conn.execute(TACTICS_SCHEMA)
    conn.execute(TACTIC_OUTCOMES_SCHEMA)
    conn.execute(METRICS_VERSION_SCHEMA)
    conn.execute(METRICS_SUMMARY_SCHEMA)
    conn.execute(TRAINING_ATTEMPTS_SCHEMA)
</file>

<file path="db/_migration_raw_pgns_versioning.py">
"""Ensure raw PGN tables include versioning columns."""

from __future__ import annotations

import duckdb


def _migration_raw_pgns_versioning(conn: duckdb.DuckDBPyConnection) -> None:
    """Add versioning columns to raw PGN rows when missing."""
    columns = {row[1] for row in conn.execute("PRAGMA table_info('raw_pgns')").fetchall()}
    if "pgn_hash" not in columns:
        conn.execute("ALTER TABLE raw_pgns ADD COLUMN pgn_hash TEXT")
    if "pgn_version" not in columns:
        conn.execute("ALTER TABLE raw_pgns ADD COLUMN pgn_version INTEGER")
        conn.execute("UPDATE raw_pgns SET pgn_version = 1 WHERE pgn_version IS NULL")
</file>

<file path="db/_normalize_filter.py">
"""Normalize dashboard filter inputs."""

from __future__ import annotations


def _normalize_filter(value: str | None) -> str | None:
    """Normalize a filter string or return None when empty."""
    if value is None:
        return None
    normalized = str(value).strip().lower()
    if normalized in {"", "all", "any"}:
        return None
    return normalized
</file>

<file path="db/_rating_bucket_clause.py">
"""Build a rating bucket SQL clause."""

from __future__ import annotations


def _rating_bucket_clause(bucket: str) -> str:
    """Return a SQL clause for filtering by rating bucket."""
    normalized = bucket.strip().lower()
    if normalized == "unknown":
        return "r.user_rating IS NULL"
    for resolver in (_resolve_lower_bucket, _resolve_range_bucket, _resolve_upper_bucket):
        clause = resolver(normalized)
        if clause is not None:
            return clause
    return "r.user_rating IS NULL"


def _resolve_lower_bucket(normalized: str) -> str | None:
    if not normalized.startswith("<"):
        return None
    value = normalized.lstrip("<").strip()
    if not value.isdigit():
        return None
    return f"r.user_rating IS NOT NULL AND r.user_rating < {int(value)}"


def _resolve_range_bucket(normalized: str) -> str | None:
    if "-" not in normalized:
        return None
    start, _, end = normalized.partition("-")
    start = start.strip()
    end = end.strip()
    if not (start.isdigit() and end.isdigit()):
        return None
    return f"r.user_rating >= {int(start)} AND r.user_rating <= {int(end)}"


def _resolve_upper_bucket(normalized: str) -> str | None:
    if not normalized.endswith("+"):
        return None
    lower = normalized.rstrip("+").strip()
    if not lower.isdigit():
        return None
    return f"r.user_rating >= {int(lower)}"
</file>

<file path="db/_rows_to_dicts.py">
"""Convert DuckDB query results to dictionaries."""

from __future__ import annotations

import duckdb


def _rows_to_dicts(
    result: duckdb.DuckDBPyConnection | duckdb.DuckDBPyRelation,
) -> list[dict[str, object]]:
    """Return rows as a list of dictionaries."""
    columns = [desc[0] for desc in result.description]
    return [dict(zip(columns, row, strict=True)) for row in result.fetchall()]
</file>

<file path="db/dashboard_repository_provider.py">
"""Factory helpers for DuckDB dashboard repository access."""

from __future__ import annotations

import duckdb

from tactix.dashboard_query import DashboardQuery
from tactix.db.duckdb_dashboard_repository import (
    DuckDbDashboardRepository,
    default_dashboard_repository_dependencies,
)


def dashboard_repository(conn: duckdb.DuckDBPyConnection) -> DuckDbDashboardRepository:
    """Return a DuckDbDashboardRepository bound to the provided connection."""
    return DuckDbDashboardRepository(
        conn,
        dependencies=default_dashboard_repository_dependencies(),
    )


def fetch_pipeline_table_counts(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    filters: DashboardQuery | None = None,
    **legacy: object,
) -> dict[str, int]:
    """Return per-table counts for pipeline verification."""
    return dashboard_repository(conn).fetch_pipeline_table_counts(
        query,
        filters=filters,
        **legacy,
    )


def fetch_opportunity_motif_counts(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    filters: DashboardQuery | None = None,
    **legacy: object,
) -> dict[str, int]:
    """Return motif counts for opportunities in the dashboard range."""
    return dashboard_repository(conn).fetch_opportunity_motif_counts(
        query,
        filters=filters,
        **legacy,
    )


def fetch_metrics(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    filters: DashboardQuery | None = None,
    **legacy: object,
) -> list[dict[str, object]]:
    """Fetch metrics summary rows with optional filters."""
    return dashboard_repository(conn).fetch_metrics(
        query,
        filters=filters,
        **legacy,
    )


def fetch_motif_stats(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    filters: DashboardQuery | None = None,
    **legacy: object,
) -> list[dict[str, object]]:
    """Return motif breakdown metrics."""
    rows = fetch_metrics(conn, query, filters=filters, **legacy)
    return [row for row in rows if row.get("metric_type") == "motif_breakdown"]


def fetch_trend_stats(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    filters: DashboardQuery | None = None,
    **legacy: object,
) -> list[dict[str, object]]:
    """Return trend metrics rows."""
    rows = fetch_metrics(conn, query, filters=filters, **legacy)
    return [row for row in rows if row.get("metric_type") == "trend"]


def fetch_recent_games(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    limit: int = 20,
    user: str | None = None,
    **legacy: object,
) -> list[dict[str, object]]:
    """Return recent games for dashboard display."""
    return dashboard_repository(conn).fetch_recent_games(
        query,
        limit=limit,
        user=user,
        **legacy,
    )


def fetch_recent_positions(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    limit: int = 20,
    **legacy: object,
) -> list[dict[str, object]]:
    """Return recent positions for dashboard display."""
    return dashboard_repository(conn).fetch_recent_positions(
        query,
        limit=limit,
        **legacy,
    )


def fetch_recent_tactics(
    conn: duckdb.DuckDBPyConnection,
    query: DashboardQuery | str | None = None,
    *,
    limit: int = 20,
    **legacy: object,
) -> list[dict[str, object]]:
    """Return recent tactics for dashboard display."""
    return dashboard_repository(conn).fetch_recent_tactics(
        query,
        limit=limit,
        **legacy,
    )
</file>

<file path="db/delete_game_rows.py">
"""Delete position, tactic, outcome, and raw PGN rows for games."""

from __future__ import annotations

import duckdb


def delete_game_rows(conn: duckdb.DuckDBPyConnection, game_ids: list[str]) -> None:
    """Delete position, tactic, outcome, and raw PGN rows for the provided games."""
    if not game_ids:
        return
    placeholders = ", ".join(["?"] * len(game_ids))
    tactic_ids = conn.execute(
        f"SELECT tactic_id FROM tactics WHERE game_id IN ({placeholders})",
        game_ids,
    ).fetchall()
    tactic_id_values = [row[0] for row in tactic_ids]
    if tactic_id_values:
        outcome_placeholders = ", ".join(["?"] * len(tactic_id_values))
        conn.execute(
            f"DELETE FROM tactic_outcomes WHERE tactic_id IN ({outcome_placeholders})",
            tactic_id_values,
        )
    conn.execute(
        f"DELETE FROM tactics WHERE game_id IN ({placeholders})",
        game_ids,
    )
    conn.execute(
        f"DELETE FROM positions WHERE game_id IN ({placeholders})",
        game_ids,
    )
    conn.execute(
        f"DELETE FROM raw_pgns WHERE game_id IN ({placeholders})",
        game_ids,
    )
</file>

<file path="db/duckdb_dashboard_reader.py">
"""Dashboard payload reader for DuckDB-backed data."""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass

import duckdb

from tactix.dashboard_query import DashboardQuery


@dataclass(frozen=True)
class DuckDbDashboardFetchers:
    metrics: Callable[..., list[dict[str, object]]]
    recent_games: Callable[..., list[dict[str, object]]]
    recent_positions: Callable[..., list[dict[str, object]]]
    recent_tactics: Callable[..., list[dict[str, object]]]


@dataclass(frozen=True)
class DuckDbDashboardDependencies:
    resolve_query: Callable[..., DashboardQuery]
    clone_query: Callable[..., DashboardQuery]
    fetchers: DuckDbDashboardFetchers
    fetch_version: Callable[[duckdb.DuckDBPyConnection], int]
    init_schema: Callable[[duckdb.DuckDBPyConnection], None]


class DuckDbDashboardReader:
    """Build dashboard payloads from a DuckDB connection."""

    def __init__(
        self,
        conn: duckdb.DuckDBPyConnection,
        *,
        user: str,
        dependencies: DuckDbDashboardDependencies,
    ) -> None:
        self._conn = conn
        self._user = user
        self._dependencies = dependencies

    def get_dashboard_payload(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, object]:
        deps = self._dependencies
        resolved = deps.resolve_query(query, filters=filters, **legacy)
        deps.init_schema(self._conn)
        active_source = None if resolved.source in (None, "all") else resolved.source
        response_source = "all" if active_source is None else active_source
        metrics_query = deps.clone_query(resolved, source=active_source, motif=resolved.motif)
        non_motif_query = deps.clone_query(resolved, source=active_source, motif=None)
        tactics_query = deps.clone_query(resolved, source=active_source, motif=resolved.motif)
        fetchers = deps.fetchers
        return {
            "source": response_source,
            "user": self._user,
            "metrics": fetchers.metrics(self._conn, metrics_query),
            "recent_games": fetchers.recent_games(
                self._conn,
                non_motif_query,
                user=self._user,
            ),
            "positions": fetchers.recent_positions(
                self._conn,
                non_motif_query,
            ),
            "tactics": fetchers.recent_tactics(
                self._conn,
                tactics_query,
            ),
            "metrics_version": deps.fetch_version(self._conn),
        }

    def __call__(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, object]:
        return self.get_dashboard_payload(query, filters=filters, **legacy)
</file>

<file path="db/duckdb_dashboard_repository.py">
"""Dashboard query repository for DuckDB-backed storage."""

from __future__ import annotations

from collections.abc import Callable, Mapping
from dataclasses import dataclass
from datetime import UTC, datetime
from io import StringIO

import chess.pgn
import duckdb

from tactix._get_game_result_for_user_from_pgn_headers import (
    _get_game_result_for_user_from_pgn_headers,
)
from tactix.dashboard_query import DashboardQuery, resolve_dashboard_query
from tactix.db._append_date_range_filters import _append_date_range_filters
from tactix.db._append_optional_filter import _append_optional_filter
from tactix.db._append_time_control_filter import _append_time_control_filter
from tactix.db._normalize_filter import _normalize_filter
from tactix.db._rating_bucket_clause import _rating_bucket_clause
from tactix.db._rows_to_dicts import _rows_to_dicts
from tactix.db.raw_pgns_queries import latest_raw_pgns_query
from tactix.extract_pgn_metadata import extract_pgn_metadata


@dataclass(frozen=True)
class DashboardFilterHelpers:
    """Filtering helpers for dashboard queries."""

    append_optional_filter: Callable[[list[str], list[object], str, object | None], None]
    append_date_range_filters: Callable[
        [list[str], list[object], object | None, object | None, str], None
    ]
    append_time_control_filter: Callable[[list[str], list[object], object | None, str], None]
    normalize_filter: Callable[[str | None], str | None]


@dataclass(frozen=True)
class DashboardQueryBuilders:
    """Query builders for dashboard reads."""

    build_games_filtered_query: Callable[[DashboardQuery], tuple[str, list[object]]]
    build_recent_games_query: Callable[[int, DashboardQuery], tuple[str, list[object]]]
    build_recent_tactics_query: Callable[[int, DashboardQuery], tuple[str, list[object]]]
    latest_raw_pgns_query: Callable[[], str]
    rating_bucket_clause: Callable[[str], str]


@dataclass(frozen=True)
class DuckDbDashboardRepositoryDependencies:
    """Dependencies used by the dashboard repository."""

    resolve_query: Callable[..., DashboardQuery]
    rows_to_dicts: Callable[[duckdb.DuckDBPyRelation], list[dict[str, object]]]
    filters: DashboardFilterHelpers
    builders: DashboardQueryBuilders
    format_recent_game_row: Callable[[Mapping[str, object], str | None], dict[str, object]]


class DuckDbDashboardRepository:
    """Encapsulates dashboard queries for DuckDB."""

    def __init__(
        self,
        conn: duckdb.DuckDBPyConnection,
        *,
        dependencies: DuckDbDashboardRepositoryDependencies,
    ) -> None:
        self._conn = conn
        self._dependencies = dependencies

    def _build_games_filtered_sql(
        self,
        query: DashboardQuery | str | None,
        *,
        filters: DashboardQuery | None,
        legacy: dict[str, object],
    ) -> tuple[str, list[object]]:
        deps = self._dependencies
        resolved = deps.resolve_query(query, filters=filters, **legacy)
        return deps.builders.build_games_filtered_query(resolved)

    def fetch_pipeline_table_counts(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, int]:
        sql, params = self._build_games_filtered_sql(
            query,
            filters=filters,
            legacy=legacy,
        )
        return _fetch_pipeline_table_counts(self._conn, sql, params)

    def fetch_opportunity_motif_counts(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, int]:
        sql, params = self._build_games_filtered_sql(
            query,
            filters=filters,
            legacy=legacy,
        )
        return _fetch_opportunity_motif_counts(self._conn, self._dependencies, sql, params)

    def fetch_metrics(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> list[dict[str, object]]:
        deps = self._dependencies
        resolved = deps.resolve_query(query, filters=filters, **legacy)
        sql = "SELECT * FROM metrics_summary"
        params: list[object] = []
        conditions: list[str] = []
        if resolved.source:
            conditions.append("source = ?")
            params.append(resolved.source)
        deps.filters.append_optional_filter(conditions, params, "motif = ?", resolved.motif)
        deps.filters.append_optional_filter(
            conditions, params, "rating_bucket = ?", resolved.rating_bucket
        )
        deps.filters.append_optional_filter(
            conditions,
            params,
            "time_control = ?",
            resolved.time_control,
        )
        deps.filters.append_date_range_filters(
            conditions,
            params,
            resolved.start_date,
            resolved.end_date,
            "trend_date",
        )
        if conditions:
            sql += " WHERE " + " AND ".join(conditions)
        result = self._conn.execute(sql, params)
        return deps.rows_to_dicts(result)

    def fetch_recent_games(
        self,
        query: DashboardQuery | str | None = None,
        *,
        limit: int = 20,
        user: str | None = None,
        **legacy: object,
    ) -> list[dict[str, object]]:
        deps = self._dependencies
        resolved = deps.resolve_query(query, **legacy)
        final_query, params = deps.builders.build_recent_games_query(limit, resolved)
        rows = deps.rows_to_dicts(self._conn.execute(final_query, params))
        return [deps.format_recent_game_row(row, user) for row in rows]

    def fetch_recent_positions(
        self,
        query: DashboardQuery | str | None = None,
        *,
        limit: int = 20,
        **legacy: object,
    ) -> list[dict[str, object]]:
        deps = self._dependencies
        resolved = deps.resolve_query(query, **legacy)
        normalized_rating = deps.filters.normalize_filter(resolved.rating_bucket)
        sql = f"""
            WITH latest_pgns AS (
                {deps.builders.latest_raw_pgns_query()}
            )
            SELECT p.*
            FROM positions p
            LEFT JOIN latest_pgns r ON r.game_id = p.game_id AND r.source = p.source
        """
        params: list[object] = []
        conditions: list[str] = []
        if resolved.source:
            conditions.append("p.source = ?")
            params.append(resolved.source)
        deps.filters.append_time_control_filter(
            conditions,
            params,
            resolved.time_control,
            "r.time_control",
        )
        if normalized_rating:
            conditions.append(deps.builders.rating_bucket_clause(normalized_rating))
        deps.filters.append_date_range_filters(
            conditions,
            params,
            resolved.start_date,
            resolved.end_date,
            "p.created_at",
        )
        if conditions:
            sql += " WHERE " + " AND ".join(conditions)
        sql += " ORDER BY p.created_at DESC LIMIT ?"
        params.append(limit)
        result = self._conn.execute(sql, params)
        return deps.rows_to_dicts(result)

    def fetch_recent_tactics(
        self,
        query: DashboardQuery | str | None = None,
        *,
        limit: int = 20,
        **legacy: object,
    ) -> list[dict[str, object]]:
        deps = self._dependencies
        resolved = deps.resolve_query(query, **legacy)
        sql, params = deps.builders.build_recent_tactics_query(limit, resolved)
        return deps.rows_to_dicts(self._conn.execute(sql, params))


def _fetch_pipeline_table_counts(
    conn: duckdb.DuckDBPyConnection,
    sql: str,
    params: list[object],
) -> dict[str, int]:
    sql += f"""
        SELECT
            (SELECT COUNT(*) FROM games_filtered) AS games,
            (
                SELECT COUNT(*)
                FROM positions p
                {_games_filtered_join_clause("p")}
            ) AS positions,
            (
                SELECT COUNT(*)
                FROM user_moves u
                {_games_filtered_join_clause("u")}
            ) AS user_moves,
            (
                SELECT COUNT(*)
                {_opportunities_from_games_filtered()}
            ) AS opportunities,
            (
                SELECT COUNT(*)
                FROM conversions c
                {_games_filtered_join_clause("c")}
            ) AS conversions,
            (
                SELECT COUNT(*)
                FROM practice_queue q
                {_games_filtered_join_clause("q")}
            ) AS practice_queue
    """
    result = conn.execute(sql, params)
    row = result.fetchone()
    if not row:
        return {
            "games": 0,
            "positions": 0,
            "user_moves": 0,
            "opportunities": 0,
            "conversions": 0,
            "practice_queue": 0,
        }
    columns = [desc[0] for desc in result.description]
    return {column: int(value or 0) for column, value in zip(columns, row, strict=False)}


def _fetch_opportunity_motif_counts(
    conn: duckdb.DuckDBPyConnection,
    deps: DuckDbDashboardRepositoryDependencies,
    sql: str,
    params: list[object],
) -> dict[str, int]:
    sql += f"""
        SELECT o.motif, COUNT(*) AS total
        {_opportunities_from_games_filtered()}
        GROUP BY o.motif
        ORDER BY o.motif
    """
    rows = deps.rows_to_dicts(conn.execute(sql, params))
    return {str(row.get("motif")): int(row.get("total") or 0) for row in rows}


def _build_games_filtered_query(resolved: DashboardQuery) -> tuple[str, list[object]]:
    sql = """
        WITH games_filtered AS (
            SELECT *
            FROM games r
    """
    params: list[object] = []
    conditions: list[str] = []
    if resolved.source:
        conditions.append("r.source = ?")
        params.append(resolved.source)
    _append_game_query_filters(
        conditions,
        params,
        resolved,
        timestamp_expr="to_timestamp(r.last_timestamp_ms / 1000)",
    )
    if conditions:
        sql += " WHERE " + " AND ".join(conditions)
    sql += """
        )
    """
    return sql, params


def _games_filtered_join_clause(alias: str) -> str:
    return (
        "INNER JOIN games_filtered r\n"
        f"                ON {alias}.game_id = r.game_id"
        f" AND {alias}.source = r.source"
    )


def _opportunities_from_games_filtered() -> str:
    return "FROM opportunities o\n" + _games_filtered_join_clause("o")


def _append_game_query_filters(
    conditions: list[str],
    params: list[object],
    query: DashboardQuery,
    *,
    timestamp_expr: str,
) -> None:
    _append_time_control_filter(conditions, params, query.time_control, "r.time_control")
    normalized_rating = _normalize_filter(query.rating_bucket)
    if normalized_rating:
        conditions.append(_rating_bucket_clause(normalized_rating))
    _append_date_range_filters(
        conditions,
        params,
        query.start_date,
        query.end_date,
        timestamp_expr,
    )


def _build_recent_games_query(limit: int, query: DashboardQuery) -> tuple[str, list[object]]:
    sql = f"""
        WITH latest_pgns AS (
            {latest_raw_pgns_query()}
        ),
        filtered AS (
            SELECT
                r.game_id,
                r.source,
                r.user,
                r.pgn,
                r.time_control,
                r.user_rating,
                r.last_timestamp_ms
            FROM latest_pgns r
    """
    params: list[object] = []
    conditions: list[str] = []
    _append_game_query_filters(
        conditions,
        params,
        query,
        timestamp_expr="to_timestamp(r.last_timestamp_ms / 1000)",
    )
    if conditions:
        sql += " WHERE " + " AND ".join(conditions)
    sql += "\n        )\n    "
    if query.source:
        params.append(query.source)
        final_query = (
            sql
            + "SELECT * FROM filtered WHERE source = ? "
            + "ORDER BY last_timestamp_ms DESC, game_id LIMIT ?"
        )
        params.append(limit)
        return final_query, params
    final_query = (
        sql
        + """
        , ranked AS (
            SELECT
                *,
                ROW_NUMBER() OVER (
                    PARTITION BY source
                    ORDER BY last_timestamp_ms DESC
                ) AS source_rank
            FROM filtered
        )
        SELECT * EXCLUDE (source_rank)
        FROM ranked
        WHERE source_rank <= ?
        ORDER BY last_timestamp_ms DESC, game_id
        """
    )
    params.append(limit)
    return final_query, params


def _build_recent_tactics_query(limit: int, query: DashboardQuery) -> tuple[str, list[object]]:
    normalized_motif = _normalize_filter(query.motif)
    normalized_rating = _normalize_filter(query.rating_bucket)
    sql = f"""
        WITH latest_pgns AS (
            {latest_raw_pgns_query()}
        )
        SELECT t.*, o.result, o.eval_delta, o.user_uci, p.source
        FROM tactics t
        LEFT JOIN tactic_outcomes o ON o.tactic_id = t.tactic_id
        LEFT JOIN positions p ON p.position_id = t.position_id
        LEFT JOIN latest_pgns r ON r.game_id = p.game_id AND r.source = p.source
    """
    params: list[object] = []
    conditions: list[str] = []
    _append_recent_tactics_filters(
        conditions,
        params,
        query,
        normalized_motif,
        normalized_rating,
    )
    if conditions:
        sql += " WHERE " + " AND ".join(conditions)
    sql += " ORDER BY t.created_at DESC LIMIT ?"
    params.append(limit)
    return sql, params


def _append_recent_tactics_filters(
    conditions: list[str],
    params: list[object],
    query: DashboardQuery,
    normalized_motif: str | None,
    normalized_rating: str | None,
) -> None:
    _append_optional_filter(conditions, params, "p.source = ?", query.source)
    _append_optional_filter(conditions, params, "t.motif = ?", normalized_motif)
    _append_time_control_filter(conditions, params, query.time_control, "r.time_control")
    if normalized_rating:
        conditions.append(_rating_bucket_clause(normalized_rating))
    _append_date_range_filters(
        conditions,
        params,
        query.start_date,
        query.end_date,
        "t.created_at",
    )


def _resolve_recent_game_user(row: Mapping[str, object], user: str | None) -> str:
    return user or str(row.get("user") or "")


def _normalize_player_name(value: object | None) -> str:
    return str(value or "").lower()


def _is_user_player(player: str, user_lower: str) -> bool:
    return bool(player) and player == user_lower


def _fallback_opponent(white: object | None, black: object | None) -> object | None:
    return black or white


def _resolve_opponent_and_color(
    user_lower: str,
    white: object | None,
    black: object | None,
) -> tuple[object | None, str | None]:
    white_lower = _normalize_player_name(white)
    black_lower = _normalize_player_name(black)
    if _is_user_player(white_lower, user_lower):
        return black, "white"
    if _is_user_player(black_lower, user_lower):
        return white, "black"
    return _fallback_opponent(white, black), None


def _timestamp_ms_to_iso(value: object) -> str | None:
    if isinstance(value, (int, float)) and int(value) > 0:
        return datetime.fromtimestamp(int(value) / 1000, tz=UTC).isoformat()
    return None


def _resolve_played_at(
    metadata: Mapping[str, object],
    row: Mapping[str, object],
) -> str | None:
    played_at = _timestamp_ms_to_iso(metadata.get("start_timestamp_ms"))
    if played_at:
        return played_at
    return _timestamp_ms_to_iso(row.get("last_timestamp_ms"))


def _resolve_recent_game_result(
    pgn_text: str,
    user: str,
    fallback: object,
) -> object:
    if not pgn_text.strip().startswith("["):
        return fallback
    try:
        headers = chess.pgn.read_headers(StringIO(pgn_text))
        if headers is None:
            return fallback
        return str(_get_game_result_for_user_from_pgn_headers(headers, user))
    except (ValueError, TypeError):
        return fallback


def _recent_game_payload(
    row: Mapping[str, object],
    metadata: Mapping[str, object],
    opponent: object | None,
    user_color: str | None,
    played_at: str | None,
) -> dict[str, object]:
    return {
        "game_id": str(row.get("game_id") or ""),
        "source": row.get("source"),
        "opponent": opponent,
        "result": metadata.get("result"),
        "played_at": played_at,
        "time_control": metadata.get("time_control") or row.get("time_control") or None,
        "user_color": user_color,
    }


def _format_recent_game_row(row: Mapping[str, object], user: str | None) -> dict[str, object]:
    raw_user = _resolve_recent_game_user(row, user)
    pgn_text = str(row.get("pgn") or "")
    metadata = extract_pgn_metadata(pgn_text, raw_user)
    opponent, user_color = _resolve_opponent_and_color(
        raw_user.lower(),
        metadata.get("white_player"),
        metadata.get("black_player"),
    )
    played_at = _resolve_played_at(metadata, row)
    result = _resolve_recent_game_result(pgn_text, raw_user, metadata.get("result"))
    return _recent_game_payload(
        row,
        {**metadata, "result": result},
        opponent,
        user_color,
        played_at,
    )


def default_dashboard_repository_dependencies() -> DuckDbDashboardRepositoryDependencies:
    filters = DashboardFilterHelpers(
        append_optional_filter=_append_optional_filter,
        append_date_range_filters=_append_date_range_filters,
        append_time_control_filter=_append_time_control_filter,
        normalize_filter=_normalize_filter,
    )
    builders = DashboardQueryBuilders(
        build_games_filtered_query=_build_games_filtered_query,
        build_recent_games_query=_build_recent_games_query,
        build_recent_tactics_query=_build_recent_tactics_query,
        latest_raw_pgns_query=latest_raw_pgns_query,
        rating_bucket_clause=_rating_bucket_clause,
    )
    return DuckDbDashboardRepositoryDependencies(
        resolve_query=resolve_dashboard_query,
        rows_to_dicts=_rows_to_dicts,
        filters=filters,
        builders=builders,
        format_recent_game_row=_format_recent_game_row,
    )
</file>

<file path="db/duckdb_metrics_repository.py">
"""Metrics repository for DuckDB-backed storage."""

from __future__ import annotations

from collections import defaultdict
from collections.abc import Callable, Mapping
from dataclasses import dataclass
from datetime import UTC, datetime

import duckdb

from tactix.db._rows_to_dicts import _rows_to_dicts
from tactix.db.duckdb_store import init_schema
from tactix.db.raw_pgns_queries import latest_raw_pgns_query
from tactix.utils.to_int import to_int


@dataclass(frozen=True)
class DuckDbMetricsDependencies:
    """Dependencies used by the metrics repository."""

    init_schema: Callable[[duckdb.DuckDBPyConnection], None]
    latest_raw_pgns_query: Callable[[], str]
    rows_to_dicts: Callable[[duckdb.DuckDBPyRelation], list[dict[str, object]]]


class DuckDbMetricsRepository:
    """Encapsulates metrics summary updates for DuckDB."""

    def __init__(
        self,
        conn: duckdb.DuckDBPyConnection,
        *,
        dependencies: DuckDbMetricsDependencies,
    ) -> None:
        self._conn = conn
        self._dependencies = dependencies

    def update_metrics_summary(self) -> None:
        """Recompute metrics summary rows."""
        deps = self._dependencies
        deps.init_schema(self._conn)
        self._conn.execute("DELETE FROM metrics_summary")
        metric_rows = self.build_metrics_summary_rows()
        if not metric_rows:
            return
        self._conn.executemany(
            """
            INSERT INTO metrics_summary (
                source,
                metric_type,
                motif,
                window_days,
                trend_date,
                rating_bucket,
                time_control,
                total,
                found,
                missed,
                failed_attempt,
                unclear,
                found_rate,
                miss_rate,
                updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """,
            metric_rows,
        )

    def build_metrics_summary_rows(self) -> list[tuple[object, ...]]:
        """Return metrics summary rows computed from current inputs."""
        return _build_metrics_summary_rows(self._conn, self._dependencies)


def default_metrics_dependencies() -> DuckDbMetricsDependencies:
    """Return default dependency wiring for DuckDB metrics operations."""
    return DuckDbMetricsDependencies(
        init_schema=init_schema,
        latest_raw_pgns_query=latest_raw_pgns_query,
        rows_to_dicts=_rows_to_dicts,
    )


def _coerce_metric_count(value: object) -> int:
    """Coerce metric counts into integers."""
    if value is None:
        return 0
    if isinstance(value, bool):
        return int(value)
    if isinstance(value, float):
        return int(value)
    parsed = to_int(value)
    return parsed if parsed is not None else 0


def _coerce_metric_rate(value: object, numerator: int, denominator: int) -> float | None:
    """Coerce metric rates into floats."""
    if isinstance(value, (int, float)):
        return float(value)
    if denominator <= 0:
        return 0.0
    return numerator / denominator


def _rating_bucket_for_rating(rating: int | None) -> str | None:
    """Return the rating bucket label for a rating."""
    if rating is None:
        return "unknown"
    bucket_size = 200
    start = (rating // bucket_size) * bucket_size
    end = start + bucket_size - 1
    return f"{start}-{end}"


def _build_metrics_summary_rows(
    conn: duckdb.DuckDBPyConnection,
    deps: DuckDbMetricsDependencies,
) -> list[tuple[object, ...]]:
    rows = _fetch_metric_inputs(conn, deps)
    if not rows:
        return []
    metrics: list[tuple[object, ...]] = []
    metrics.extend(_build_motif_breakdowns(rows))
    metrics.extend(_build_trend_rows(rows, window_days=7))
    metrics.extend(_build_trend_rows(rows, window_days=30))
    metrics.extend(_build_time_trouble_rows(rows))
    return metrics


def _fetch_metric_inputs(
    conn: duckdb.DuckDBPyConnection,
    deps: DuckDbMetricsDependencies,
) -> list[dict[str, object]]:
    latest_query = deps.latest_raw_pgns_query()
    result = conn.execute(
        f"""
        WITH latest_pgns AS (
            {latest_query}
        )
        SELECT
            t.game_id,
            t.motif,
            COALESCE(o.result, 'unclear') AS result,
            p.source,
            p.clock_seconds,
            p.created_at,
            r.user_rating,
            r.time_control,
            r.last_timestamp_ms
        FROM tactics t
        LEFT JOIN tactic_outcomes o ON o.tactic_id = t.tactic_id
        LEFT JOIN positions p ON p.position_id = t.position_id
        LEFT JOIN latest_pgns r ON r.game_id = p.game_id AND r.source = p.source
        """
    )
    raw_rows = deps.rows_to_dicts(result)
    for row in raw_rows:
        rating = row.get("user_rating")
        row["rating_bucket"] = _rating_bucket_for_rating(
            int(rating) if rating is not None else None
        )
        row["trend_date"] = _trend_date_from_row(row)
    return raw_rows


def _trend_date_from_row(row: Mapping[str, object]) -> datetime.date | None:
    timestamp_ms = row.get("last_timestamp_ms")
    if isinstance(timestamp_ms, (int, float)) and timestamp_ms > 0:
        return datetime.fromtimestamp(int(timestamp_ms) / 1000, tz=UTC).date()
    created_at = row.get("created_at")
    if isinstance(created_at, datetime):
        return created_at.date()
    return None


def _count_result_types(items: list[dict[str, object]]) -> dict[str, int]:
    counts = {
        "found": 0,
        "missed": 0,
        "failed_attempt": 0,
        "unclear": 0,
    }
    for item in items:
        result = item.get("result")
        if isinstance(result, str) and result in counts:
            counts[result] += 1
    return counts


def _window_rate(values: list[int], idx: int, window_days: int) -> float:
    start = max(0, idx - window_days + 1)
    window = values[start : idx + 1]
    return sum(window) / len(window) if window else 0.0


def _sorted_trend_items(items: list[dict[str, object]]) -> list[dict[str, object]]:
    return sorted(
        items,
        key=lambda item: (
            item.get("last_timestamp_ms") or 0,
            item.get("created_at") or datetime.min.replace(tzinfo=UTC),
        ),
    )


def _result_flag(item: dict[str, object], expected: str) -> int:
    return 1 if item.get("result") == expected else 0


def _build_trend_row(
    group: tuple[object, object, object, object],
    item: dict[str, object],
    window_days: int,
    found_rate: float,
    miss_rate: float,
) -> tuple[object, ...]:
    source, motif, rating_bucket, time_control = group
    result = item.get("result")
    return (
        source,
        "trend",
        motif,
        window_days,
        item.get("trend_date"),
        rating_bucket,
        time_control,
        1,
        1 if result == "found" else 0,
        1 if result == "missed" else 0,
        1 if result == "failed_attempt" else 0,
        1 if result == "unclear" else 0,
        found_rate,
        miss_rate,
    )


def _build_trend_rows_for_group(
    group: tuple[object, object, object, object],
    items: list[dict[str, object]],
    window_days: int,
) -> list[tuple[object, ...]]:
    sorted_items = _sorted_trend_items(items)
    results = [_result_flag(item, "found") for item in sorted_items]
    misses = [_result_flag(item, "missed") for item in sorted_items]
    metric_rows: list[tuple[object, ...]] = []
    for idx, item in enumerate(sorted_items):
        found_rate = _window_rate(results, idx, window_days)
        miss_rate = _window_rate(misses, idx, window_days)
        metric_rows.append(_build_trend_row(group, item, window_days, found_rate, miss_rate))
    return metric_rows


def _is_time_trouble_item(item: dict[str, object], threshold: int) -> bool:
    clock_seconds = item.get("clock_seconds")
    if clock_seconds is None:
        return False
    if not isinstance(clock_seconds, (int, float)):
        return False
    return clock_seconds <= threshold


def _split_time_trouble_items(
    items: list[dict[str, object]],
    threshold: int,
) -> tuple[list[dict[str, object]], list[dict[str, object]]]:
    trouble_items = [item for item in items if _is_time_trouble_item(item, threshold)]
    safe_items = [item for item in items if item not in trouble_items]
    return trouble_items, safe_items


def _count_found(items: list[dict[str, object]]) -> int:
    return sum(1 for item in items if item.get("result") == "found")


def _group_metric_rows(
    rows: list[dict[str, object]],
) -> dict[tuple[object, object, object, object], list[dict[str, object]]]:
    grouped: dict[
        tuple[object, object, object, object],
        list[dict[str, object]],
    ] = defaultdict(list)
    for row in rows:
        grouped[
            (
                row.get("source"),
                row.get("motif"),
                row.get("rating_bucket"),
                row.get("time_control"),
            )
        ].append(row)
    return grouped


def _build_motif_breakdowns(rows: list[dict[str, object]]) -> list[tuple[object, ...]]:
    grouped = _group_metric_rows(rows)
    metric_rows: list[tuple[object, ...]] = []
    for (source, motif, rating_bucket, time_control), items in grouped.items():
        total = len(items)
        counts = _count_result_types(items)
        found_rate = _coerce_metric_rate(None, counts["found"], total)
        miss_rate = _coerce_metric_rate(None, counts["missed"], total)
        metric_rows.append(
            (
                source,
                "motif_breakdown",
                motif,
                0,
                None,
                rating_bucket,
                time_control,
                total,
                counts["found"],
                counts["missed"],
                counts["failed_attempt"],
                counts["unclear"],
                found_rate,
                miss_rate,
            )
        )
    return metric_rows


def _build_trend_rows(
    rows: list[dict[str, object]],
    *,
    window_days: int,
) -> list[tuple[object, ...]]:
    grouped = _group_metric_rows(rows)
    metric_rows: list[tuple[object, ...]] = []
    for (source, motif, rating_bucket, time_control), items in grouped.items():
        metric_rows.extend(
            _build_trend_rows_for_group(
                (source, motif, rating_bucket, time_control),
                items=items,
                window_days=window_days,
            )
        )
    return metric_rows


def _build_time_trouble_rows(rows: list[dict[str, object]]) -> list[tuple[object, ...]]:
    grouped: dict[tuple[object, ...], list[dict[str, object]]] = defaultdict(list)
    for row in rows:
        key = (row.get("source"), row.get("time_control"))
        grouped[key].append(row)
    metric_rows: list[tuple[object, ...]] = []
    for (source, time_control), items in grouped.items():
        metric_rows.append(_build_time_trouble_row(source, time_control, items))
    return metric_rows


def _build_time_trouble_row(
    source: object,
    time_control: object,
    items: list[dict[str, object]],
) -> tuple[object, ...]:
    total = len(items)
    counts = _count_result_types(items)
    miss_rate = _coerce_metric_rate(None, counts["missed"], total)
    trouble_threshold = 30
    trouble_items, safe_items = _split_time_trouble_items(items, trouble_threshold)
    trouble_found = _count_found(trouble_items)
    safe_found = _count_found(safe_items)
    trouble_rate = trouble_found / len(trouble_items) if trouble_items else 0.0
    safe_rate = safe_found / len(safe_items) if safe_items else 0.0
    found_rate = safe_rate - trouble_rate
    return (
        source,
        "time_trouble_correlation",
        None,
        0,
        None,
        None,
        time_control,
        total,
        counts["found"],
        counts["missed"],
        counts["failed_attempt"],
        counts["unclear"],
        found_rate,
        miss_rate,
    )


_VULTURE_USED = (_coerce_metric_count,)
</file>

<file path="db/duckdb_position_repository.py">
"""Position repository for DuckDB-backed storage."""

from __future__ import annotations

from collections.abc import Callable, Mapping
from dataclasses import dataclass

import duckdb

from tactix.db._rows_to_dicts import _rows_to_dicts


@dataclass(frozen=True)
class DuckDbPositionDependencies:
    """Dependencies used by the position repository."""

    rows_to_dicts: Callable[[duckdb.DuckDBPyRelation], list[dict[str, object]]]


class DuckDbPositionRepository:
    """Encapsulates position persistence and reads for DuckDB."""

    def __init__(
        self,
        conn: duckdb.DuckDBPyConnection,
        *,
        dependencies: DuckDbPositionDependencies,
    ) -> None:
        self._conn = conn
        self._dependencies = dependencies

    def fetch_position_counts(
        self,
        game_ids: list[str],
        source: str | None,
    ) -> dict[str, int]:
        """Return position counts keyed by game id."""
        if not game_ids:
            return {}
        placeholders = ", ".join(["?"] * len(game_ids))
        params: list[object] = list(game_ids)
        sql = f"SELECT game_id, COUNT(*) FROM positions WHERE game_id IN ({placeholders})"
        if source:
            sql += " AND source = ?"
            params.append(source)
        sql += " GROUP BY game_id"
        rows = self._conn.execute(sql, params).fetchall()
        return {str(game_id): int(count) for game_id, count in rows}

    def fetch_positions_for_games(self, game_ids: list[str]) -> list[dict[str, object]]:
        """Return stored positions for the provided games."""
        if not game_ids:
            return []
        placeholders = ", ".join(["?"] * len(game_ids))
        result = self._conn.execute(
            f"SELECT * FROM positions WHERE game_id IN ({placeholders}) ORDER BY position_id",
            game_ids,
        )
        return self._dependencies.rows_to_dicts(result)

    def insert_positions(self, positions: list[Mapping[str, object]]) -> list[int]:
        """Insert position rows and return new ids."""
        if not positions:
            return []
        row = self._conn.execute("SELECT MAX(position_id) FROM positions").fetchone()
        next_id = int(row[0] or 0) if row else 0
        ids: list[int] = []
        for pos in positions:
            next_id += 1
            self._conn.execute(
                """
                INSERT INTO positions (
                    position_id,
                    game_id,
                    user,
                    source,
                    fen,
                    ply,
                    move_number,
                    side_to_move,
                    uci,
                    san,
                    clock_seconds,
                    is_legal
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                [
                    next_id,
                    pos.get("game_id"),
                    pos.get("user"),
                    pos.get("source"),
                    pos.get("fen"),
                    pos.get("ply"),
                    pos.get("move_number"),
                    pos.get("side_to_move"),
                    pos.get("uci"),
                    pos.get("san"),
                    pos.get("clock_seconds"),
                    pos.get("is_legal", True),
                ],
            )
            ids.append(next_id)
        return ids


def default_position_dependencies() -> DuckDbPositionDependencies:
    """Return default dependency wiring for DuckDB position operations."""
    return DuckDbPositionDependencies(rows_to_dicts=_rows_to_dicts)
</file>

<file path="db/duckdb_raw_pgn_repository.py">
"""Raw PGN repository for DuckDB-backed storage."""

from __future__ import annotations

from collections.abc import Callable, Iterable, Mapping
from dataclasses import dataclass

import duckdb

from tactix.db._build_raw_pgn_upsert_plan import _build_raw_pgn_upsert_plan
from tactix.db._fetch_next_raw_pgn_id import _fetch_next_raw_pgn_id
from tactix.db._insert_raw_pgn_plan import _insert_raw_pgn_plan
from tactix.db._rows_to_dicts import _rows_to_dicts
from tactix.db.fetch_latest_raw_pgns import fetch_latest_raw_pgns as _fetch_latest_raw_pgns
from tactix.db.raw_pgn_summary import (
    build_raw_pgn_summary_payload,
    coerce_raw_pgn_summary_rows,
)
from tactix.db.raw_pgns_queries import latest_raw_pgns_query
from tactix.db.RawPgnInsertInputs import RawPgnInsertInputs


# pylint: disable=too-many-instance-attributes
@dataclass(frozen=True)
class DuckDbRawPgnDependencies:
    """Dependencies used by the raw PGN repository."""

    fetch_next_raw_pgn_id: Callable[[duckdb.DuckDBPyConnection], int]
    build_raw_pgn_upsert_plan: Callable[
        [
            duckdb.DuckDBPyConnection,
            Mapping[str, object],
            str,
            str,
            dict[tuple[str, str], tuple[str | None, int]],
        ],
        object,
    ]
    insert_raw_pgn_plan: Callable[[duckdb.DuckDBPyConnection, RawPgnInsertInputs], None]
    fetch_latest_raw_pgns: Callable[
        [duckdb.DuckDBPyConnection, str | None, int | None], list[dict[str, object]]
    ]
    latest_raw_pgns_query: Callable[[str], str]
    rows_to_dicts: Callable[[duckdb.DuckDBPyRelation], list[dict[str, object]]]
    build_raw_pgn_summary_payload: Callable[..., dict[str, object]]
    coerce_raw_pgn_summary_rows: Callable[[Iterable[Mapping[str, object]]], list[dict[str, object]]]


class DuckDbRawPgnRepository:
    """Encapsulates raw PGN persistence and reads for DuckDB."""

    def __init__(
        self,
        conn: duckdb.DuckDBPyConnection,
        *,
        dependencies: DuckDbRawPgnDependencies,
    ) -> None:
        self._conn = conn
        self._dependencies = dependencies

    def upsert_raw_pgns(self, rows: Iterable[Mapping[str, object]]) -> int:
        """Insert raw PGN rows with version tracking."""
        rows_list = list(rows)
        if not rows_list:
            return 0
        self._conn.execute("BEGIN TRANSACTION")
        try:
            inserted = self._upsert_raw_pgn_rows(rows_list)
            self._conn.execute("COMMIT")
        except duckdb.Error:
            self._conn.execute("ROLLBACK")
            raise
        return inserted

    def _upsert_raw_pgn_rows(
        self,
        rows_list: list[Mapping[str, object]],
    ) -> int:
        deps = self._dependencies
        next_id = deps.fetch_next_raw_pgn_id(self._conn)
        latest_cache: dict[tuple[str, str], tuple[str | None, int]] = {}
        inserted = 0
        for row in rows_list:
            game_id = str(row["game_id"])
            source = str(row["source"])
            plan = deps.build_raw_pgn_upsert_plan(
                self._conn,
                row,
                game_id,
                source,
                latest_cache,
            )
            if plan is None:
                continue
            next_id += 1
            deps.insert_raw_pgn_plan(
                self._conn,
                RawPgnInsertInputs(
                    raw_pgn_id=next_id,
                    game_id=game_id,
                    source=source,
                    row=row,
                    plan=plan,
                ),
            )
            latest_cache[(game_id, source)] = (plan.pgn_hash, plan.pgn_version)
            inserted += 1
        return inserted

    def fetch_latest_pgn_hashes(
        self,
        game_ids: list[str],
        source: str | None,
    ) -> dict[str, str]:
        """Fetch latest PGN hashes for the provided game ids."""
        if not game_ids:
            return {}
        placeholders = ", ".join(["?"] * len(game_ids))
        params: list[object] = list(game_ids)
        sql = f"""
            WITH latest_pgns AS (
                {self._dependencies.latest_raw_pgns_query(f"WHERE game_id IN ({placeholders})")}
            )
            SELECT game_id, pgn_hash
            FROM latest_pgns
        """
        if source:
            sql += " WHERE source = ?"
            params.append(source)
        rows = self._conn.execute(sql, params).fetchall()
        return {str(game_id): str(pgn_hash) for game_id, pgn_hash in rows if pgn_hash}

    def fetch_latest_raw_pgns(
        self,
        source: str | None = None,
        limit: int | None = None,
    ) -> list[dict[str, object]]:
        """Fetch the latest raw PGN rows for a source."""
        return self._dependencies.fetch_latest_raw_pgns(self._conn, source, limit)

    def fetch_raw_pgns_summary(
        self,
        *,
        source: str | None = None,
    ) -> dict[str, object]:
        """Return raw PGN summary payload for the given source."""
        deps = self._dependencies
        where_clause = ""
        params: list[object] = []
        if source:
            where_clause = "WHERE source = ?"
            params.append(source)
        sources_result = self._conn.execute(
            f"""
            SELECT
                source,
                COUNT(*) AS total_rows,
                COUNT(DISTINCT game_id) AS distinct_games,
                MAX(ingested_at) AS latest_ingested_at
            FROM raw_pgns
            {where_clause}
            GROUP BY source
            ORDER BY source
            """,
            params,
        )
        sources = deps.rows_to_dicts(sources_result)
        totals_result = self._conn.execute(
            f"""
            SELECT
                COUNT(*) AS total_rows,
                COUNT(DISTINCT game_id) AS distinct_games,
                MAX(ingested_at) AS latest_ingested_at
            FROM raw_pgns
            {where_clause}
            """,
            params,
        )
        totals_row = totals_result.fetchone()
        totals_dict: dict[str, object] = {}
        if totals_row:
            columns = [desc[0] for desc in totals_result.description]
            totals_dict = dict(zip(columns, totals_row, strict=False))
        return deps.build_raw_pgn_summary_payload(
            sources=deps.coerce_raw_pgn_summary_rows(sources),
            totals=totals_dict,
        )


def default_raw_pgn_dependencies() -> DuckDbRawPgnDependencies:
    """Return default dependency wiring for DuckDB raw PGN operations."""
    return DuckDbRawPgnDependencies(
        fetch_next_raw_pgn_id=_fetch_next_raw_pgn_id,
        build_raw_pgn_upsert_plan=_build_raw_pgn_upsert_plan,
        insert_raw_pgn_plan=_insert_raw_pgn_plan,
        fetch_latest_raw_pgns=_fetch_latest_raw_pgns,
        latest_raw_pgns_query=latest_raw_pgns_query,
        rows_to_dicts=_rows_to_dicts,
        build_raw_pgn_summary_payload=build_raw_pgn_summary_payload,
        coerce_raw_pgn_summary_rows=coerce_raw_pgn_summary_rows,
    )
</file>

<file path="db/duckdb_store.py">
"""DuckDB-backed data store implementation."""

from __future__ import annotations

import os
from pathlib import Path

import duckdb

from tactix.dashboard_query import (
    DashboardQuery,
    clone_dashboard_query,
    resolve_dashboard_query,
)
from tactix.db._build_legacy_raw_pgn_inserts import _build_legacy_raw_pgn_inserts
from tactix.db._drop_table_if_exists import _drop_table_if_exists
from tactix.db._migration_add_columns import _migration_add_columns
from tactix.db._migration_add_pipeline_views import _migration_add_pipeline_views
from tactix.db._migration_add_position_legality import _migration_add_position_legality
from tactix.db._migration_add_tactic_explanations import _migration_add_tactic_explanations
from tactix.db._migration_add_training_attempt_latency import (
    _migration_add_training_attempt_latency,
)
from tactix.db._migration_base_tables import _migration_base_tables
from tactix.db._migration_raw_pgns_versioning import _migration_raw_pgns_versioning
from tactix.db.duckdb_dashboard_reader import (
    DuckDbDashboardDependencies,
    DuckDbDashboardFetchers,
    DuckDbDashboardReader,
)
from tactix.db.duckdb_dashboard_repository import (
    DuckDbDashboardRepository,
    default_dashboard_repository_dependencies,
)
from tactix.define_base_db_store__db_store import BaseDbStore
from tactix.define_base_db_store_context__db_store import BaseDbStoreContext

RAW_PGNS_SCHEMA = """
CREATE TABLE IF NOT EXISTS raw_pgns (
    raw_pgn_id BIGINT PRIMARY KEY,
    game_id TEXT,
    user TEXT,
    source TEXT,
    fetched_at TIMESTAMP,
    pgn TEXT,
    pgn_hash TEXT,
    pgn_version INTEGER,
    user_rating INTEGER,
    time_control TEXT,
    ingested_at TIMESTAMP,
    last_timestamp_ms BIGINT,
    cursor TEXT
);
"""

POSITIONS_SCHEMA = """
CREATE TABLE IF NOT EXISTS positions (
    position_id BIGINT PRIMARY KEY,
    game_id TEXT,
    user TEXT,
    source TEXT,
    fen TEXT,
    ply INTEGER,
    move_number INTEGER,
    side_to_move TEXT,
    user_to_move BOOLEAN DEFAULT TRUE,
    uci TEXT,
    san TEXT,
    clock_seconds DOUBLE,
    is_legal BOOLEAN,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

TACTICS_SCHEMA = """
CREATE TABLE IF NOT EXISTS tactics (
    tactic_id BIGINT PRIMARY KEY,
    game_id TEXT,
    position_id BIGINT,
    motif TEXT,
    severity DOUBLE,
    best_uci TEXT,
    best_san TEXT,
    explanation TEXT,
    eval_cp INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

TACTIC_OUTCOMES_SCHEMA = """
CREATE TABLE IF NOT EXISTS tactic_outcomes (
    outcome_id BIGINT PRIMARY KEY,
    tactic_id BIGINT,
    result TEXT,
    user_uci TEXT,
    eval_delta INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

METRICS_VERSION_SCHEMA = """
CREATE TABLE IF NOT EXISTS metrics_version (
    version BIGINT,
    updated_at TIMESTAMP
);
"""

METRICS_SUMMARY_SCHEMA = """
CREATE TABLE IF NOT EXISTS metrics_summary (
    source TEXT,
    metric_type TEXT,
    motif TEXT,
    window_days INTEGER,
    trend_date DATE,
    rating_bucket TEXT,
    time_control TEXT,
    total BIGINT,
    found BIGINT,
    missed BIGINT,
    failed_attempt BIGINT,
    unclear BIGINT,
    found_rate DOUBLE,
    miss_rate DOUBLE,
    updated_at TIMESTAMP
);
"""

TRAINING_ATTEMPTS_SCHEMA = """
CREATE TABLE IF NOT EXISTS training_attempts (
    attempt_id BIGINT PRIMARY KEY,
    tactic_id BIGINT,
    position_id BIGINT,
    source TEXT,
    attempted_uci TEXT,
    correct BOOLEAN,
    success BOOLEAN,
    best_uci TEXT,
    motif TEXT,
    severity DOUBLE,
    eval_delta INTEGER,
    latency_ms BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""

SCHEMA_VERSION_SCHEMA = """
CREATE TABLE IF NOT EXISTS schema_version (
    version INTEGER,
    updated_at TIMESTAMP
);
"""

SCHEMA_VERSION = 7


def _should_attempt_wal_recovery(exc: BaseException) -> bool:
    """Return True when WAL recovery is allowed for the given exception."""
    message = str(exc).lower()
    if "wal replay failed" not in message and "wal error" not in message:
        return False
    return (
        bool(os.environ.get("PYTEST_CURRENT_TEST"))
        or os.environ.get("TACTIX_ENV", "").lower() == "dev"
        or os.environ.get("TACTIX_ALLOW_WAL_RECOVERY", "").lower() == "true"
    )


def get_connection(db_path: Path | str) -> duckdb.DuckDBPyConnection:
    """Open a DuckDB connection, recovering from WAL errors when needed."""
    db_path = Path(db_path)
    try:
        return duckdb.connect(str(db_path))
    except duckdb.InternalException as exc:
        if _should_attempt_wal_recovery(exc):
            wal_paths = list(db_path.parent.glob("*.wal"))
            if wal_paths:
                for wal_path in wal_paths:
                    wal_path.unlink()
                return duckdb.connect(str(db_path))
        raise


def get_schema_version(conn: duckdb.DuckDBPyConnection) -> int:
    """Return the current schema version."""
    conn.execute(SCHEMA_VERSION_SCHEMA)
    row = conn.execute(
        "SELECT version FROM schema_version ORDER BY updated_at DESC LIMIT 1"
    ).fetchone()
    return int(row[0]) if row else 0


def migrate_schema(conn: duckdb.DuckDBPyConnection) -> None:
    """Run schema migrations to the latest version."""
    conn.execute(SCHEMA_VERSION_SCHEMA)
    _ensure_raw_pgns_versioned(conn)
    current_version = get_schema_version(conn)
    for version, migration in _SCHEMA_MIGRATIONS:
        if version <= current_version:
            continue
        migration(conn)
        conn.execute(
            "INSERT INTO schema_version (version, updated_at) VALUES (?, CURRENT_TIMESTAMP)",
            [version],
        )


def init_schema(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure all required tables exist and migrations are applied."""
    migrate_schema(conn)


def _get_raw_pgn_columns(conn: duckdb.DuckDBPyConnection) -> set[str]:
    try:
        return {row[1] for row in conn.execute("PRAGMA table_info('raw_pgns')").fetchall()}
    except duckdb.Error:
        return set()


def _ensure_raw_pgns_versioned(conn: duckdb.DuckDBPyConnection) -> None:
    """Ensure raw PGN rows include versioning columns."""
    columns = _get_raw_pgn_columns(conn)
    if not columns:
        return
    if "raw_pgn_id" not in columns or "pgn_version" not in columns:
        _migrate_raw_pgns_legacy(conn)


def _rating_bucket_for_rating(rating: int | None) -> str | None:
    """Return the rating bucket label for a rating."""
    if rating is None:
        return "unknown"
    bucket_size = 200
    start = (rating // bucket_size) * bucket_size
    end = start + bucket_size - 1
    return f"{start}-{end}"


def _dashboard_repository(conn: duckdb.DuckDBPyConnection) -> DuckDbDashboardRepository:
    return DuckDbDashboardRepository(
        conn,
        dependencies=default_dashboard_repository_dependencies(),
    )


class DuckDbStore(BaseDbStore):
    """DuckDB-backed store implementation."""

    def __init__(self, context: BaseDbStoreContext, db_path: Path | None = None) -> None:
        super().__init__(context)
        self._db_path = db_path or context.settings.duckdb_path

    @property
    def db_path(self) -> Path:
        return self._db_path

    def get_dashboard_payload(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, object]:
        conn = get_connection(self._db_path)
        repository = _dashboard_repository(conn)

        def _metrics_fetcher(
            _conn: duckdb.DuckDBPyConnection,
            dashboard_query: DashboardQuery,
            **kwargs: object,
        ) -> list[dict[str, object]]:
            return repository.fetch_metrics(dashboard_query, **kwargs)

        def _recent_games_fetcher(
            _conn: duckdb.DuckDBPyConnection,
            dashboard_query: DashboardQuery,
            **kwargs: object,
        ) -> list[dict[str, object]]:
            return repository.fetch_recent_games(dashboard_query, **kwargs)

        def _recent_positions_fetcher(
            _conn: duckdb.DuckDBPyConnection,
            dashboard_query: DashboardQuery,
            **kwargs: object,
        ) -> list[dict[str, object]]:
            return repository.fetch_recent_positions(dashboard_query, **kwargs)

        def _recent_tactics_fetcher(
            _conn: duckdb.DuckDBPyConnection,
            dashboard_query: DashboardQuery,
            **kwargs: object,
        ) -> list[dict[str, object]]:
            return repository.fetch_recent_tactics(dashboard_query, **kwargs)

        reader = DuckDbDashboardReader(
            conn,
            user=self.settings.user,
            dependencies=DuckDbDashboardDependencies(
                resolve_query=resolve_dashboard_query,
                clone_query=clone_dashboard_query,
                fetchers=DuckDbDashboardFetchers(
                    metrics=_metrics_fetcher,
                    recent_games=_recent_games_fetcher,
                    recent_positions=_recent_positions_fetcher,
                    recent_tactics=_recent_tactics_fetcher,
                ),
                fetch_version=fetch_version,
                init_schema=init_schema,
            ),
        )
        return reader.get_dashboard_payload(query, filters=filters, **legacy)


_SCHEMA_MIGRATIONS = [
    (1, _migration_base_tables),
    (2, _migration_raw_pgns_versioning),
    (3, _migration_add_columns),
    (4, _migration_add_training_attempt_latency),
    (5, _migration_add_position_legality),
    (6, _migration_add_tactic_explanations),
    (7, _migration_add_pipeline_views),
]


def _migrate_raw_pgns_legacy(conn: duckdb.DuckDBPyConnection) -> None:
    """Migrate legacy raw PGN tables into the versioned schema."""
    conn.execute("BEGIN TRANSACTION")
    try:
        _drop_table_if_exists(conn, "raw_pgns_legacy")
        conn.execute("ALTER TABLE raw_pgns RENAME TO raw_pgns_legacy")
        conn.execute(RAW_PGNS_SCHEMA)
        legacy_rows = conn.execute(
            """
            SELECT game_id, user, source, fetched_at, pgn, last_timestamp_ms, cursor
            FROM raw_pgns_legacy
            """
        ).fetchall()
        inserts = _build_legacy_raw_pgn_inserts(legacy_rows)
        if inserts:
            conn.executemany(
                """
                INSERT INTO raw_pgns (
                    raw_pgn_id,
                    game_id,
                    user,
                    source,
                    fetched_at,
                    pgn,
                    pgn_hash,
                    pgn_version,
                    user_rating,
                    time_control,
                    ingested_at,
                    last_timestamp_ms,
                    cursor
                )
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                inserts,
            )
        conn.execute("DROP TABLE raw_pgns_legacy")
        conn.execute("COMMIT")
    except duckdb.Error:
        conn.execute("ROLLBACK")
        raise


def _ensure_column(
    conn: duckdb.DuckDBPyConnection, table: str, column: str, definition: str
) -> None:
    """Add a column to a table when missing."""
    columns = {row[1] for row in conn.execute(f"PRAGMA table_info('{table}')").fetchall()}
    if column not in columns:
        conn.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")


def write_metrics_version(conn: duckdb.DuckDBPyConnection) -> int:
    """Increment and persist the metrics version."""
    current = fetch_version(conn)
    new_version = current + 1
    conn.execute(
        "INSERT INTO metrics_version (version, updated_at) VALUES (?, CURRENT_TIMESTAMP)",
        [new_version],
    )
    return new_version


def fetch_version(conn: duckdb.DuckDBPyConnection) -> int:
    """Fetch the latest metrics version."""
    version_row = conn.execute("SELECT MAX(version) FROM metrics_version").fetchone()
    if not version_row or version_row[0] is None:
        return 0
    return int(version_row[0])


_VULTURE_USED = (SCHEMA_VERSION, _ensure_column)
</file>

<file path="db/duckdb_tactic_repository.py">
"""Tactic and practice repository for DuckDB-backed storage."""

from __future__ import annotations

from collections.abc import Callable, Mapping
from dataclasses import dataclass

import duckdb

from tactix.db._rows_to_dicts import _rows_to_dicts
from tactix.db.raw_pgns_queries import latest_raw_pgns_query
from tactix.db.record_training_attempt import record_training_attempt
from tactix.define_base_db_store__db_store import BaseDbStore
from tactix.extract_pgn_metadata import extract_pgn_metadata
from tactix.format_tactics__explanation import format_tactic_explanation
from tactix.sql_tactics import (
    OUTCOME_COLUMNS,
    TACTIC_ANALYSIS_COLUMNS,
    TACTIC_COLUMNS,
    TACTIC_QUEUE_COLUMNS,
)


@dataclass(frozen=True)
class DuckDbTacticDependencies:
    """Dependencies used by the tactic repository."""

    rows_to_dicts: Callable[[duckdb.DuckDBPyRelation], list[dict[str, object]]]
    format_tactic_explanation: Callable[
        [str | None, str, str | None], tuple[str | None, str | None]
    ]
    record_training_attempt: Callable[[duckdb.DuckDBPyConnection, Mapping[str, object]], int]
    extract_pgn_metadata: Callable[[str, str], dict[str, object]]
    require_position_id: Callable[[Mapping[str, object], str], None]
    latest_raw_pgns_query: Callable[[], str]


@dataclass(frozen=True)
class PracticeAttemptInputs:
    """Grouped inputs for practice attempt payloads."""

    tactic: Mapping[str, object]
    tactic_id: int
    position_id: int
    attempted_uci: str
    best_uci: str
    correct: bool
    latency_ms: int | None


class DuckDbTacticRepository:
    """Encapsulates tactic insertion, outcomes, and practice flows for DuckDB."""

    def __init__(
        self,
        conn: duckdb.DuckDBPyConnection,
        *,
        dependencies: DuckDbTacticDependencies,
    ) -> None:
        self._conn = conn
        self._dependencies = dependencies

    def insert_tactics(self, rows: list[Mapping[str, object]]) -> list[int]:
        """Insert tactic rows and return ids."""
        if not rows:
            return []
        row = self._conn.execute("SELECT MAX(tactic_id) FROM tactics").fetchone()
        next_id = int(row[0] or 0) if row else 0
        ids: list[int] = []
        for tactic in rows:
            next_id += 1
            self._conn.execute(
                """
                INSERT INTO tactics (
                    tactic_id,
                    game_id,
                    position_id,
                    motif,
                    severity,
                    best_uci,
                    best_san,
                    explanation,
                    eval_cp
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                [
                    next_id,
                    tactic.get("game_id"),
                    tactic.get("position_id"),
                    tactic.get("motif"),
                    tactic.get("severity"),
                    tactic.get("best_uci"),
                    tactic.get("best_san"),
                    tactic.get("explanation"),
                    tactic.get("eval_cp"),
                ],
            )
            ids.append(next_id)
        return ids

    def insert_tactic_outcomes(self, rows: list[Mapping[str, object]]) -> list[int]:
        """Insert tactic outcome rows and return ids."""
        if not rows:
            return []
        row = self._conn.execute("SELECT MAX(outcome_id) FROM tactic_outcomes").fetchone()
        next_id = int(row[0] or 0) if row else 0
        ids: list[int] = []
        for outcome in rows:
            next_id += 1
            self._conn.execute(
                """
                INSERT INTO tactic_outcomes (
                    outcome_id,
                    tactic_id,
                    result,
                    user_uci,
                    eval_delta
                ) VALUES (?, ?, ?, ?, ?)
                """,
                [
                    next_id,
                    outcome.get("tactic_id"),
                    outcome.get("result"),
                    outcome.get("user_uci"),
                    outcome.get("eval_delta"),
                ],
            )
            ids.append(next_id)
        return ids

    def upsert_tactic_with_outcome(
        self,
        tactic_row: Mapping[str, object],
        outcome_row: Mapping[str, object],
    ) -> int:
        """Insert a tactic with its outcome and return the tactic id."""
        deps = self._dependencies
        deps.require_position_id(
            tactic_row,
            "position_id is required when inserting tactics",
        )
        tactic_ids = self.insert_tactics([tactic_row])
        tactic_id = tactic_ids[0]
        self.insert_tactic_outcomes(
            [
                {
                    **outcome_row,
                    "tactic_id": tactic_id,
                }
            ]
        )
        return tactic_id

    def fetch_practice_tactic(self, tactic_id: int) -> dict[str, object] | None:
        """Fetch a single tactic for practice flows."""
        result = self._conn.execute(
            f"""
            SELECT
                {TACTIC_COLUMNS},
                {OUTCOME_COLUMNS},
                p.source,
                p.fen,
                p.uci AS position_uci,
                p.san,
                p.ply,
                p.move_number,
                p.side_to_move,
                p.clock_seconds
            FROM tactics t
            LEFT JOIN tactic_outcomes o ON o.tactic_id = t.tactic_id
            LEFT JOIN positions p ON p.position_id = t.position_id
            WHERE t.tactic_id = ?
            """,
            [tactic_id],
        )
        rows = self._dependencies.rows_to_dicts(result)
        return rows[0] if rows else None

    def fetch_practice_queue(
        self,
        limit: int = 20,
        source: str | None = None,
        include_failed_attempt: bool = False,
        exclude_seen: bool = False,
    ) -> list[dict[str, object]]:
        """Return a queue of practice tactics."""
        results = ["missed"]
        if include_failed_attempt:
            results.append("failed_attempt")
        placeholders = ", ".join(["?"] * len(results))
        query = f"""
            SELECT
                {TACTIC_QUEUE_COLUMNS},
                {OUTCOME_COLUMNS},
                p.source,
                p.fen,
                p.uci AS position_uci,
                p.san,
                p.ply,
                p.move_number,
                p.side_to_move,
                p.clock_seconds
            FROM tactics t
            INNER JOIN tactic_outcomes o ON o.tactic_id = t.tactic_id
            INNER JOIN positions p ON p.position_id = t.position_id
            WHERE o.result IN ({placeholders})
        """
        params: list[object] = list(results)
        if source:
            query += " AND p.source = ?"
            params.append(source)
        if exclude_seen:
            query += " AND t.tactic_id NOT IN (SELECT tactic_id FROM training_attempts"
            if source:
                query += " WHERE source = ?"
                params.append(source)
            query += ")"
        query += " ORDER BY t.created_at DESC LIMIT ?"
        params.append(limit)
        result = self._conn.execute(query, params)
        return self._dependencies.rows_to_dicts(result)

    def grade_practice_attempt(
        self,
        tactic_id: int,
        position_id: int,
        attempted_uci: str,
        latency_ms: int | None = None,
    ) -> dict[str, object]:
        """Grade a practice attempt and persist the result."""
        tactic = self._require_practice_tactic(tactic_id, position_id)
        trimmed_attempt = self._normalize_attempted_uci(attempted_uci)
        best_uci = self._normalize_best_uci(tactic)
        correct = bool(best_uci) and trimmed_attempt.lower() == best_uci.lower()
        best_san, explanation = self._resolve_practice_explanation(tactic, best_uci)
        attempt_payload = self._build_practice_attempt_payload(
            PracticeAttemptInputs(
                tactic=tactic,
                tactic_id=tactic_id,
                position_id=position_id,
                attempted_uci=trimmed_attempt,
                best_uci=best_uci,
                correct=correct,
                latency_ms=latency_ms,
            )
        )
        attempt_id = self._dependencies.record_training_attempt(self._conn, attempt_payload)
        message = self._build_practice_message(correct, tactic, best_uci)
        return {
            "attempt_id": attempt_id,
            "tactic_id": tactic_id,
            "position_id": position_id,
            "source": tactic.get("source"),
            "attempted_uci": trimmed_attempt,
            "best_uci": best_uci,
            "correct": correct,
            "success": correct,
            "motif": tactic.get("motif", "unknown"),
            "severity": tactic.get("severity", 0.0),
            "eval_delta": tactic.get("eval_delta", 0) or 0,
            "message": message,
            "best_san": best_san,
            "explanation": explanation,
            "latency_ms": latency_ms,
        }

    def record_training_attempt(self, payload: Mapping[str, object]) -> int:
        """Persist a training attempt record."""
        return self._dependencies.record_training_attempt(self._conn, payload)

    def fetch_game_detail(
        self,
        game_id: str,
        user: str,
        source: str | None = None,
    ) -> dict[str, object]:
        """Fetch a detailed game payload including analysis rows."""
        row, result = self._fetch_latest_pgn_row(game_id, source)
        if not row:
            return {
                "game_id": game_id,
                "source": source,
                "pgn": None,
                "metadata": {},
                "analysis": [],
            }
        pgn_row = self._row_to_dict(result, row)
        pgn_value = pgn_row.get("pgn")
        pgn = str(pgn_value) if pgn_value is not None else None
        metadata = self._dependencies.extract_pgn_metadata(pgn or "", user)
        analysis_rows = self._fetch_game_analysis_rows(game_id, source)
        return {
            "game_id": game_id,
            "source": pgn_row.get("source") or source,
            "pgn": pgn,
            "metadata": metadata,
            "analysis": analysis_rows,
        }

    def _latest_pgns_query(self) -> str:
        return f"""
            WITH latest_pgns AS (
                {self._dependencies.latest_raw_pgns_query()}
            )
            SELECT *
            FROM latest_pgns
            WHERE game_id = ?
        """

    def _fetch_latest_pgn_row(
        self,
        game_id: str,
        source: str | None,
    ) -> tuple[tuple[object, ...] | None, duckdb.DuckDBPyConnection]:
        query = self._latest_pgns_query()
        params: list[object] = [game_id]
        if source:
            query += " AND source = ?"
            params.append(source)
        result = self._conn.execute(query, params)
        row = result.fetchone()
        if row or not source:
            return row, result
        fallback_result = self._conn.execute(self._latest_pgns_query(), [game_id])
        return fallback_result.fetchone(), fallback_result

    def _fetch_game_analysis_rows(
        self,
        game_id: str,
        source: str | None,
    ) -> list[dict[str, object]]:
        analysis_query = f"""
            SELECT
                {TACTIC_ANALYSIS_COLUMNS},
                {OUTCOME_COLUMNS},
                p.move_number,
                p.ply,
                p.san,
                p.uci,
                p.side_to_move,
                p.fen
            FROM tactics t
            LEFT JOIN tactic_outcomes o ON o.tactic_id = t.tactic_id
            LEFT JOIN positions p ON p.position_id = t.position_id
            WHERE t.game_id = ?
        """
        analysis_params: list[object] = [game_id]
        if source:
            analysis_query += " AND p.source = ?"
            analysis_params.append(source)
        analysis_query += " ORDER BY p.ply ASC, t.created_at ASC"
        return self._dependencies.rows_to_dicts(self._conn.execute(analysis_query, analysis_params))

    def _row_to_dict(
        self,
        result: duckdb.DuckDBPyConnection | duckdb.DuckDBPyRelation,
        row: tuple[object, ...],
    ) -> dict[str, object]:
        columns = [desc[0] for desc in result.description]
        return dict(zip(columns, row, strict=True))

    def _require_practice_tactic(self, tactic_id: int, position_id: int) -> dict[str, object]:
        tactic = self.fetch_practice_tactic(tactic_id)
        if not tactic or tactic.get("position_id") != position_id:
            raise ValueError("Tactic not found for position")
        return tactic

    def _normalize_attempted_uci(self, attempted_uci: str) -> str:
        trimmed = attempted_uci.strip()
        if not trimmed:
            raise ValueError("attempted_uci is required")
        return trimmed

    def _normalize_best_uci(self, tactic: Mapping[str, object]) -> str:
        best_uci_raw = tactic.get("best_uci")
        return str(best_uci_raw).strip() if best_uci_raw is not None else ""

    def _resolve_practice_explanation(
        self,
        tactic: Mapping[str, object],
        best_uci: str,
    ) -> tuple[str | None, str | None]:
        fen = self._string_or_none(tactic.get("fen"))
        motif = self._string_or_none(tactic.get("motif"))
        best_san = self._string_or_none(tactic.get("best_san"))
        explanation = self._string_or_none(tactic.get("explanation"))
        generated_san, generated_explanation = self._dependencies.format_tactic_explanation(
            fen,
            best_uci,
            motif,
        )
        return self._resolve_explanation(
            best_san,
            explanation,
            generated_san,
            generated_explanation,
        )

    def _string_or_none(self, value: object) -> str | None:
        if value is None:
            return None
        text = str(value).strip()
        return text or None

    def _resolve_explanation(
        self,
        best_san: str | None,
        explanation: str | None,
        generated_san: str | None,
        generated_explanation: str | None,
    ) -> tuple[str | None, str | None]:
        if not best_san:
            best_san = generated_san or None
        if not explanation:
            explanation = generated_explanation or None
        return best_san, explanation

    def _build_practice_attempt_payload(
        self,
        inputs: PracticeAttemptInputs,
    ) -> dict[str, object]:
        return {
            "tactic_id": inputs.tactic_id,
            "position_id": inputs.position_id,
            "source": inputs.tactic.get("source"),
            "attempted_uci": inputs.attempted_uci,
            "correct": inputs.correct,
            "success": inputs.correct,
            "best_uci": inputs.best_uci,
            "motif": inputs.tactic.get("motif", "unknown"),
            "severity": inputs.tactic.get("severity", 0.0),
            "eval_delta": inputs.tactic.get("eval_delta", 0) or 0,
            "latency_ms": inputs.latency_ms,
        }

    def _build_practice_message(
        self,
        correct: bool,
        tactic: Mapping[str, object],
        best_uci: str,
    ) -> str:
        if correct:
            return f"Correct! {tactic.get('motif', 'tactic')} found."
        return f"Missed it. Best move was {best_uci or '--'}."


def default_tactic_dependencies() -> DuckDbTacticDependencies:
    """Return default dependency wiring for DuckDB tactic operations."""
    return DuckDbTacticDependencies(
        rows_to_dicts=_rows_to_dicts,
        format_tactic_explanation=format_tactic_explanation,
        record_training_attempt=record_training_attempt,
        extract_pgn_metadata=extract_pgn_metadata,
        require_position_id=BaseDbStore.require_position_id,
        latest_raw_pgns_query=latest_raw_pgns_query,
    )
</file>

<file path="db/fetch_latest_raw_pgns.py">
"""Fetch latest raw PGN rows from DuckDB."""

import duckdb

from tactix.db._rows_to_dicts import _rows_to_dicts
from tactix.db.query_helpers import apply_limit_clause
from tactix.db.raw_pgns_queries import latest_raw_pgns_query


def fetch_latest_raw_pgns(
    conn: duckdb.DuckDBPyConnection,
    source: str | None = None,
    limit: int | None = None,
) -> list[dict[str, object]]:
    """Return the latest raw PGN rows by game and source."""
    params: list[object] = []
    filters = []
    if source:
        filters.append("source = ?")
        params.append(source)
    where_clause = f"WHERE {' AND '.join(filters)}" if filters else ""
    limit_clause = apply_limit_clause(params, limit)
    result = conn.execute(
        f"""
        {latest_raw_pgns_query(where_clause)}
        ORDER BY last_timestamp_ms DESC, game_id
        {limit_clause}
        """,
        params,
    )
    return _rows_to_dicts(result)
</file>

<file path="db/fetch_unanalyzed_positions.py">
"""Fetch positions without analysis entries."""

import duckdb

from tactix.db._rows_to_dicts import _rows_to_dicts
from tactix.db.query_helpers import apply_limit_clause


def fetch_unanalyzed_positions(
    conn: duckdb.DuckDBPyConnection,
    game_ids: list[str] | None = None,
    source: str | None = None,
    limit: int | None = None,
) -> list[dict[str, object]]:
    """Return positions that have not yet been analyzed."""
    params: list[object] = []
    filters: list[str] = ["t.position_id IS NULL"]
    if game_ids:
        placeholders = ", ".join(["?"] * len(game_ids))
        filters.append(f"p.game_id IN ({placeholders})")
        params.extend(game_ids)
    if source:
        filters.append("p.source = ?")
        params.append(source)
    where_clause = f"WHERE {' AND '.join(filters)}"
    limit_clause = apply_limit_clause(params, limit)
    result = conn.execute(
        f"""
        SELECT p.*
        FROM positions AS p
        LEFT JOIN tactics AS t
            ON p.position_id = t.position_id
        {where_clause}
        ORDER BY p.position_id
        {limit_clause}
        """,
        params,
    )
    return _rows_to_dicts(result)
</file>

<file path="db/metrics_repository_provider.py">
"""Factory helpers for DuckDB metrics repository access."""

from __future__ import annotations

import duckdb

from tactix.db.duckdb_metrics_repository import (
    DuckDbMetricsRepository,
    default_metrics_dependencies,
)


def metrics_repository(conn: duckdb.DuckDBPyConnection) -> DuckDbMetricsRepository:
    """Return a DuckDbMetricsRepository bound to the provided connection."""
    return DuckDbMetricsRepository(conn, dependencies=default_metrics_dependencies())


def update_metrics_summary(conn: duckdb.DuckDBPyConnection) -> None:
    """Recompute metrics summary rows."""
    metrics_repository(conn).update_metrics_summary()
</file>

<file path="db/position_repository_provider.py">
"""Factory helpers for DuckDB position repository access."""

from __future__ import annotations

from collections.abc import Mapping

import duckdb

from tactix.db.duckdb_position_repository import (
    DuckDbPositionRepository,
    default_position_dependencies,
)


def position_repository(conn: duckdb.DuckDBPyConnection) -> DuckDbPositionRepository:
    """Return a DuckDbPositionRepository bound to the provided connection."""
    return DuckDbPositionRepository(conn, dependencies=default_position_dependencies())


def fetch_position_counts(
    conn: duckdb.DuckDBPyConnection,
    game_ids: list[str],
    source: str | None,
) -> dict[str, int]:
    """Return position counts keyed by game id."""
    return position_repository(conn).fetch_position_counts(game_ids, source)


def fetch_positions_for_games(
    conn: duckdb.DuckDBPyConnection,
    game_ids: list[str],
) -> list[dict[str, object]]:
    """Return stored positions for the provided games."""
    return position_repository(conn).fetch_positions_for_games(game_ids)


def insert_positions(
    conn: duckdb.DuckDBPyConnection,
    positions: list[Mapping[str, object]],
) -> list[int]:
    """Insert position rows and return new ids."""
    return position_repository(conn).insert_positions(positions)
</file>

<file path="db/postgres_repository.py">
"""Postgres repository adapter for read-only endpoints."""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass
from typing import Any

from tactix.config import Settings
from tactix.fetch_analysis_tactics import fetch_analysis_tactics
from tactix.fetch_ops_events import fetch_ops_events
from tactix.fetch_postgres_raw_pgns_summary import fetch_postgres_raw_pgns_summary
from tactix.get_postgres_status import get_postgres_status
from tactix.postgres_status import PostgresStatus


@dataclass(frozen=True)
class PostgresRepositoryDependencies:
    """Dependencies used by the Postgres repository."""

    get_status: Callable[[Settings], PostgresStatus]
    fetch_ops_events: Callable[[Settings, int], list[dict[str, Any]]]
    fetch_analysis_tactics: Callable[[Settings, int], list[dict[str, Any]]]
    fetch_raw_pgns_summary: Callable[[Settings], dict[str, Any]]


class PostgresRepository:
    """Encapsulates Postgres-backed read operations."""

    def __init__(
        self,
        settings: Settings,
        *,
        dependencies: PostgresRepositoryDependencies,
    ) -> None:
        self._settings = settings
        self._dependencies = dependencies

    def get_status(self) -> PostgresStatus:
        """Return the current Postgres status snapshot."""
        return self._dependencies.get_status(self._settings)

    def fetch_ops_events(self, limit: int = 10) -> list[dict[str, Any]]:
        """Fetch recent ops events from Postgres."""
        return self._dependencies.fetch_ops_events(self._settings, limit)

    def fetch_analysis_tactics(self, limit: int = 10) -> list[dict[str, Any]]:
        """Fetch recent analyzed tactics from Postgres."""
        return self._dependencies.fetch_analysis_tactics(self._settings, limit)

    def fetch_raw_pgns_summary(self) -> dict[str, Any]:
        """Fetch raw PGN summary data from Postgres."""
        return self._dependencies.fetch_raw_pgns_summary(self._settings)


def default_postgres_repository_dependencies() -> PostgresRepositoryDependencies:
    """Return default dependency wiring for Postgres repository operations."""
    return PostgresRepositoryDependencies(
        get_status=get_postgres_status,
        fetch_ops_events=fetch_ops_events,
        fetch_analysis_tactics=fetch_analysis_tactics,
        fetch_raw_pgns_summary=fetch_postgres_raw_pgns_summary,
    )
</file>

<file path="db/query_helpers.py">
"""Shared helpers for SQL query assembly."""


def apply_limit_clause(params: list[object], limit: int | None) -> str:
    """Append a LIMIT clause and bind parameter when requested."""
    if limit is None:
        return ""
    params.append(int(limit))
    return "LIMIT ?"
</file>

<file path="db/raw_pgn_repository_provider.py">
"""Factory helpers for DuckDB raw PGN repository access."""

from __future__ import annotations

from collections.abc import Iterable, Mapping

import duckdb

from tactix.db.duckdb_raw_pgn_repository import (
    DuckDbRawPgnRepository,
    default_raw_pgn_dependencies,
)
from tactix.define_base_db_store__db_store import BaseDbStore


def raw_pgn_repository(conn: duckdb.DuckDBPyConnection) -> DuckDbRawPgnRepository:
    """Return a DuckDbRawPgnRepository bound to the provided connection."""
    return DuckDbRawPgnRepository(conn, dependencies=default_raw_pgn_dependencies())


def hash_pgn(pgn: str) -> str:
    """Return the canonical hash for PGN content."""
    return BaseDbStore.hash_pgn(pgn)


def upsert_raw_pgns(
    conn: duckdb.DuckDBPyConnection,
    rows: Iterable[Mapping[str, object]],
) -> int:
    """Insert raw PGN rows with version tracking."""
    return raw_pgn_repository(conn).upsert_raw_pgns(rows)


def fetch_latest_pgn_hashes(
    conn: duckdb.DuckDBPyConnection,
    game_ids: list[str],
    source: str | None,
) -> dict[str, str]:
    """Fetch latest PGN hashes for the provided game ids."""
    return raw_pgn_repository(conn).fetch_latest_pgn_hashes(game_ids, source)


def fetch_latest_raw_pgns(
    conn: duckdb.DuckDBPyConnection,
    source: str | None = None,
    limit: int | None = None,
) -> list[dict[str, object]]:
    """Fetch the latest raw PGN rows for a source."""
    return raw_pgn_repository(conn).fetch_latest_raw_pgns(source, limit)


def fetch_raw_pgns_summary(
    conn: duckdb.DuckDBPyConnection,
    *,
    source: str | None = None,
) -> dict[str, object]:
    """Return raw PGN summary payload for the given source."""
    return raw_pgn_repository(conn).fetch_raw_pgns_summary(source=source)
</file>

<file path="db/raw_pgn_summary.py">
"""Helpers for raw PGN summary payloads."""

from collections.abc import Iterable, Mapping
from typing import Any


def coerce_raw_pgn_summary_rows(
    rows: Iterable[Mapping[str, Any]],
) -> list[dict[str, Any]]:
    """Coerce summary rows into dictionaries."""
    return [dict(row) for row in rows]


def build_raw_pgn_summary_sources(
    rows: Iterable[Mapping[str, Any]],
) -> list[dict[str, Any]]:
    """Build source summaries for raw PGN payloads."""
    return coerce_raw_pgn_summary_rows(rows)


def build_raw_pgn_summary_payload(
    *,
    sources: Iterable[Mapping[str, Any]],
    totals: Mapping[str, Any],
    status: str = "ok",
) -> dict[str, Any]:
    """Build the raw PGN summary payload."""
    return {
        "status": status,
        "total_rows": totals.get("total_rows", 0),
        "distinct_games": totals.get("distinct_games", 0),
        "latest_ingested_at": totals.get("latest_ingested_at"),
        "sources": build_raw_pgn_summary_sources(sources),
    }
</file>

<file path="db/raw_pgns_queries.py">
"""Shared SQL builders for raw PGN queries."""


def latest_raw_pgns_query(where_clause: str = "") -> str:
    """Return the latest raw PGN subquery with an optional WHERE clause."""
    return f"""
        SELECT * EXCLUDE (rn)
        FROM (
            SELECT
                *,
                ROW_NUMBER() OVER (
                    PARTITION BY game_id, source
                    ORDER BY pgn_version DESC
                ) AS rn
            FROM raw_pgns
            {where_clause}
        )
        WHERE rn = 1
    """
</file>

<file path="db/RawPgnInsertInputs.py">
"""Inputs used to insert raw PGN rows."""

# pylint: disable=invalid-name

from __future__ import annotations

from collections.abc import Mapping
from dataclasses import dataclass

from tactix.PgnUpsertPlan import PgnUpsertPlan


@dataclass(frozen=True)
class RawPgnInsertInputs:
    """Grouped inputs for raw PGN inserts."""

    raw_pgn_id: int
    game_id: str
    source: str
    row: Mapping[str, object]
    plan: PgnUpsertPlan
</file>

<file path="db/record_training_attempt.py">
"""Persist training attempts in DuckDB."""

from __future__ import annotations

from collections.abc import Mapping

import duckdb


def record_training_attempt(
    conn: duckdb.DuckDBPyConnection,
    attempt: Mapping[str, object],
) -> int:
    """Insert a training attempt row and return its id."""
    row = conn.execute("SELECT MAX(attempt_id) FROM training_attempts").fetchone()
    next_id = int(row[0] or 0) + 1 if row else 1
    conn.execute(
        """
        INSERT INTO training_attempts (
            attempt_id,
            tactic_id,
            position_id,
            source,
            attempted_uci,
            correct,
            success,
            best_uci,
            motif,
            severity,
            eval_delta,
            latency_ms
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
        [
            next_id,
            attempt.get("tactic_id"),
            attempt.get("position_id"),
            attempt.get("source"),
            attempt.get("attempted_uci"),
            attempt.get("correct"),
            attempt.get("success"),
            attempt.get("best_uci"),
            attempt.get("motif"),
            attempt.get("severity"),
            attempt.get("eval_delta"),
            attempt.get("latency_ms"),
        ],
    )
    return next_id
</file>

<file path="db/tactic_repository_provider.py">
"""Factory helpers for DuckDB tactic repository access."""

from __future__ import annotations

from collections.abc import Mapping

import duckdb

from tactix.db.duckdb_tactic_repository import (
    DuckDbTacticRepository,
    default_tactic_dependencies,
)


def tactic_repository(conn: duckdb.DuckDBPyConnection) -> DuckDbTacticRepository:
    """Return a DuckDbTacticRepository bound to the provided connection."""
    return DuckDbTacticRepository(conn, dependencies=default_tactic_dependencies())


def insert_tactics(
    conn: duckdb.DuckDBPyConnection,
    rows: list[Mapping[str, object]],
) -> list[int]:
    """Insert tactic rows and return ids."""
    return tactic_repository(conn).insert_tactics(rows)


def insert_tactic_outcomes(
    conn: duckdb.DuckDBPyConnection,
    rows: list[Mapping[str, object]],
) -> list[int]:
    """Insert tactic outcome rows and return ids."""
    return tactic_repository(conn).insert_tactic_outcomes(rows)


def upsert_tactic_with_outcome(
    conn: duckdb.DuckDBPyConnection,
    tactic_row: Mapping[str, object],
    outcome_row: Mapping[str, object],
) -> int:
    """Insert a tactic with its outcome and return the tactic id."""
    return tactic_repository(conn).upsert_tactic_with_outcome(tactic_row, outcome_row)


def record_training_attempt(
    conn: duckdb.DuckDBPyConnection,
    payload: Mapping[str, object],
) -> int:
    """Persist a training attempt record."""
    return tactic_repository(conn).record_training_attempt(payload)
</file>

<file path="domain/__init__.py">

</file>

<file path="errors/__init__.py">
# pylint: disable=duplicate-code,R0801
"""Custom error types used in tactix."""

import requests


class RateLimitError(requests.HTTPError):
    """HTTP rate limit error."""
</file>

<file path="infra/clients/__init__.py">
"""Infrastructure client adapters."""

from tactix.infra.clients.chesscom_client import ChesscomClient, ChesscomClientContext
from tactix.infra.clients.lichess_client import LichessClient, LichessClientContext

__all__ = [
    "ChesscomClient",
    "ChesscomClientContext",
    "LichessClient",
    "LichessClientContext",
]
</file>

<file path="infra/clients/chesscom_client.py">
"""Chess.com client implementation and helpers (adapter layer)."""

from __future__ import annotations

import time

# pylint: disable=fixme,broad-exception-caught,protected-access,undefined-all-variable
from collections.abc import Callable, Iterable, Mapping
from dataclasses import dataclass
from datetime import UTC, datetime
from email.utils import parsedate_to_datetime
from pathlib import Path
from typing import Any, cast
from urllib.parse import parse_qs, urlencode, urlparse, urlunparse

import requests

from tactix.chess_clients.base_chess_client import (
    BaseChessClient,
    BaseChessClientContext,
    ChessFetchRequest,
    ChessFetchResult,
)
from tactix.chess_clients.chess_game_row import (
    ChessGameRow,
    coerce_game_row_dict,
    coerce_rows_for_model,
)
from tactix.chess_clients.fetch_helpers import run_incremental_fetch, use_fixture_games
from tactix.config import Settings
from tactix.errors import RateLimitError
from tactix.extract_game_id import extract_game_id
from tactix.extract_last_timestamp_ms import extract_last_timestamp_ms
from tactix.latest_timestamp import latest_timestamp
from tactix.load_fixture_games import FixtureGamesRequest, load_fixture_games
from tactix.read_optional_text__filesystem import _read_optional_text
from tactix.utils import Logger, to_int

logger = Logger(__name__)

ChesscomFetchRequest = ChessFetchRequest
ChesscomFetchResult = ChessFetchResult
ChesscomRateLimitError = RateLimitError

ARCHIVES_URL = "https://api.chess.com/pub/player/{username}/games/archives"
HTTP_STATUS_TOO_MANY_REQUESTS = 429


def _auth_headers(token: str | None) -> dict[str, str]:
    """Build authorization headers.

    Args:
        token: API token if available.

    Returns:
        Headers dict for the request.
    """

    if not token:
        return {}
    return {"Authorization": f"Bearer {token}"}


def _build_cursor(last_ts: int, game_id: str) -> str:
    """Build a cursor token.

    Args:
        last_ts: Last timestamp in milliseconds.
        game_id: Game identifier.

    Returns:
        Cursor token.
    """

    return f"{last_ts}:{game_id}"


def _parse_cursor(cursor: str | None) -> tuple[int, str]:
    """Parse a cursor token into timestamp and id.

    Args:
        cursor: Cursor token.

    Returns:
        Tuple of timestamp and game id.
    """

    if not cursor:
        return 0, ""
    if ":" in cursor:
        prefix, suffix = cursor.split(":", 1)
        try:
            return int(prefix), suffix
        except ValueError:
            return 0, cursor
    if cursor.isdigit():
        return int(cursor), ""
    return 0, cursor


def _cursor_allows_game(game: dict, since_ts: int, since_game: str) -> bool:
    """Return True if the game is beyond the given cursor position."""

    last_ts = int(game.get("last_timestamp_ms", 0))
    return (
        (not since_ts)
        or (last_ts > since_ts)
        or (last_ts == since_ts and str(game.get("game_id", "")) > since_game)
    )


def _filter_by_cursor(rows: list[dict], cursor: str | None) -> list[dict]:
    """Filter rows using a cursor token.

    Args:
        rows: Candidate rows to filter.
        cursor: Cursor token to apply.

    Returns:
        Filtered list of rows.
    """

    since_ts, since_game = _parse_cursor(cursor)
    ordered = sorted(rows, key=lambda g: int(g.get("last_timestamp_ms", 0)))
    return [game for game in ordered if _cursor_allows_game(game, since_ts, since_game)]


def read_cursor(path: Path) -> str | None:
    """Read a cursor token from disk.

    Args:
        path: Cursor path.

    Returns:
        Cursor token or None.
    """

    return _read_optional_text(path)


def write_cursor(path: Path, cursor: str | None) -> None:
    """Write a cursor token to disk.

    Args:
        path: Cursor file path.
        cursor: Cursor token to persist.
    """

    path.parent.mkdir(parents=True, exist_ok=True)
    if cursor is None:
        path.write_text("")
        return
    path.write_text(cursor)


def _parse_retry_after_seconds(value: str) -> float | None:
    """Parse Retry-After as a numeric duration.

    Args:
        value: Retry-After header value.

    Returns:
        Parsed seconds or None.
    """

    try:
        seconds = float(value)
    except ValueError:
        return None
    return max(seconds, 0.0)


def _parse_retry_after_date(value: str) -> float | None:
    """Parse Retry-After as an HTTP date.

    Args:
        value: Retry-After header value.

    Returns:
        Parsed seconds or None.
    """

    try:
        dt = parsedate_to_datetime(value)
    except (TypeError, ValueError, OverflowError):
        return None
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=UTC)
    delta = (dt - datetime.now(UTC)).total_seconds()
    return max(delta, 0.0)


def _parse_retry_after(value: str | None) -> float | None:
    """Parse Retry-After header values.

    Args:
        value: Retry-After header value.

    Returns:
        Number of seconds to wait, or None.
    """

    if not value:
        return None
    seconds = _parse_retry_after_seconds(value)
    if seconds is not None:
        return seconds
    return _parse_retry_after_date(value)


def _non_empty_str(value: str) -> str | None:
    return value if value else None


def _first_string(mapping: Mapping[str, object], keys: Iterable[str]) -> str | None:
    for key in keys:
        value = mapping.get(key)
        if isinstance(value, str) and value:
            return value
    return None


def _extract_candidate_href(candidate: object) -> str | None:
    """Return a candidate href from a pagination payload."""

    if isinstance(candidate, str):
        return _non_empty_str(candidate)
    if isinstance(candidate, Mapping):
        candidate_map = cast(Mapping[str, object], candidate)
        return _first_string(candidate_map, ("href", "url"))
    return None


def _next_page_candidate(data: dict) -> str | None:
    for key in ("next_page", "next", "next_url", "nextPage"):
        href = _extract_candidate_href(data.get(key))
        if href:
            return href
    return None


def _pagination_values(data: Mapping[str, object]) -> tuple[int, int] | None:
    page = to_int(data.get("page") or data.get("current_page"))
    total_pages = to_int(data.get("total_pages") or data.get("totalPages"))
    if page is None or total_pages is None:
        return None
    return page, total_pages


def _page_url_for(current_url: str, page: int) -> str:
    parsed = urlparse(current_url)
    query = parse_qs(parsed.query)
    query["page"] = [str(page)]
    return urlunparse(parsed._replace(query=urlencode(query, doseq=True)))


def _page_from_numbers(data: dict, current_url: str) -> str | None:
    """Compute a next page URL from numeric pagination fields.

    Args:
        data: Response payload.
        current_url: Current URL used for pagination.

    Returns:
        Next page URL if determinable.
    """

    page_info = _pagination_values(data)
    if page_info is None:
        return None
    page, total_pages = page_info
    if page >= total_pages:
        return None
    return _page_url_for(current_url, page + 1)


def _next_page_url(data: dict, current_url: str) -> str | None:
    """Resolve the next page URL from a response payload.

    Args:
        data: Response payload.
        current_url: Current URL used for pagination.

    Returns:
        Next page URL if available.
    """

    candidate = _next_page_candidate(data)
    return candidate or _page_from_numbers(data, current_url)


def _should_stop_archive_fetch(since_ms: int, archive_max_ts: int) -> bool:
    return bool(since_ms and archive_max_ts and archive_max_ts <= since_ms)


@dataclass(slots=True)
class ChesscomClientContext(BaseChessClientContext):
    """Context for Chess.com API interactions."""


class ChesscomClient(BaseChessClient):
    """Client for Chess.com API interactions."""

    def __init__(self, context: ChesscomClientContext) -> None:
        """Initialize the client with Chess.com-specific context.

        Args:
            context: Client context containing settings and logger.
        """

        super().__init__(context)

    def fetch_incremental_games(self, request: ChessFetchRequest) -> ChessFetchResult:
        """Fetch Chess.com games incrementally.

        Args:
            request: Parameters for the incremental fetch.

        Returns:
            Chess.com fetch result with games and cursor metadata.

        Example:
            >>> client.fetch_incremental_games(ChessFetchRequest(cursor=None))
        """

        raw_games = self._fetch_raw_games(request)
        games = self._attach_cursors(_filter_by_cursor(raw_games, request.cursor))
        last_ts = self._resolve_last_timestamp(games, request.cursor)
        next_cursor = self._resolve_next_cursor(games, request.cursor)
        self._log_fetch_summary(request.cursor, next_cursor, len(games))
        return ChessFetchResult(
            games=games,
            next_cursor=next_cursor,
            last_timestamp_ms=last_ts,
        )

    def _fetch_raw_games(self, request: ChessFetchRequest) -> list[dict]:
        """Fetch raw games from fixtures or the remote API.

        Args:
            request: Parameters for the incremental fetch.

        Returns:
            List of raw Chess.com game rows.
        """

        if use_fixture_games(
            self.settings.chesscom.token,
            self.settings.chesscom_use_fixture_when_no_token,
        ):
            return self._load_fixture_games(request.since_ms)
        return self._fetch_remote_games(request.since_ms, request.full_history)

    def _load_fixture_games(self, since_ms: int) -> list[dict]:
        """Load Chess.com fixture games.

        Args:
            since_ms: Minimum timestamp for included games.

        Returns:
            Fixture game rows.
        """

        return load_fixture_games(
            FixtureGamesRequest(
                fixture_path=self.settings.chesscom_fixture_pgn_path,
                user=self.settings.user,
                source=self.settings.source,
                since_ms=since_ms,
                logger=self.logger,
                missing_message="Chess.com fixture PGN path missing: %s",
                loaded_message="Loaded %s Chess.com fixture PGNs from %s",
                coerce_rows=coerce_rows_for_model(ChessGameRow),
            )
        )

    def _fetch_remote_games(self, since_ms: int, full_history: bool) -> list[dict]:
        """Fetch Chess.com games from the remote API.

        Args:
            since_ms: Minimum timestamp for included games.
            full_history: Whether to fetch full history.

        Returns:
            Remote game rows.
        """

        archives = self._fetch_archive_index()
        if archives is None:
            return self._load_fixture_games(since_ms)
        if not archives:
            return []
        archives_to_fetch = archives if full_history else archives[-6:]
        return self._collect_archives(archives_to_fetch, since_ms)

    def _fetch_archive_index(self) -> list[str] | None:
        """Fetch the archive list for the configured user.

        Returns:
            List of archive URLs.
        """

        url = ARCHIVES_URL.format(username=self.settings.user)
        try:
            response = self._get_with_backoff(url, timeout=15)
        except Exception as exc:
            self.logger.warning("Falling back to fixtures; archive fetch failed: %s", exc)
            return None
        archives = response.json().get("archives", [])
        if not archives:
            self.logger.info("No archives returned for %s", self.settings.user)
        return list(archives)

    def _collect_archives(self, archives: list[str], since_ms: int) -> list[dict]:
        """Collect games across multiple archives.

        Args:
            archives: Archive URLs to fetch.
            since_ms: Minimum timestamp for included games.

        Returns:
            Aggregated Chess.com game rows.
        """

        games: list[dict] = []
        for archive_url in reversed(archives):
            archive_games = self._safe_fetch_archive(archive_url)
            if not archive_games:
                continue
            if self._append_archive_games(games, archive_games, since_ms):
                break
        self.logger.info("Fetched %s Chess.com PGNs", len(games))
        return games

    def _safe_fetch_archive(self, archive_url: str) -> list[dict]:
        """Fetch a single archive page with error handling.

        Args:
            archive_url: Archive endpoint URL.

        Returns:
            List of raw game dictionaries.
        """

        try:
            return self._fetch_archive_pages(archive_url)
        except Exception as exc:
            self.logger.warning("Failed to fetch archive %s: %s", archive_url, exc)
            return []

    def _append_archive_games(
        self, games: list[dict], archive_games: list[dict], since_ms: int
    ) -> bool:
        """Append archive games to the accumulator.

        Args:
            games: Accumulator list for games.
            archive_games: Raw games from the archive.
            since_ms: Minimum timestamp for included games.

        Returns:
            True if the caller should stop fetching older archives.
        """

        archive_max_ts = 0
        seen_game_ids: set[str] = set()
        for game in archive_games:
            row, last_ts = self._coerce_game(game)
            if row is None:
                continue
            archive_max_ts = max(archive_max_ts, last_ts)
            if self._should_skip_game(row, last_ts, since_ms, seen_game_ids):
                continue
            seen_game_ids.add(str(row.get("game_id", "")))
            games.append(row)
        return _should_stop_archive_fetch(since_ms, archive_max_ts)

    def _coerce_game(self, game: dict) -> tuple[dict | None, int]:
        """Coerce a raw API game dict into a model.

        Args:
            game: Raw game dictionary.

        Returns:
            Tuple of model and last timestamp.
        """

        if game.get("time_class") != self.settings.chesscom.time_class:
            return None, 0
        pgn = game.get("pgn")
        if not pgn:
            return None, 0
        last_ts = extract_last_timestamp_ms(pgn)
        game_id = str(game.get("uuid") or extract_game_id(pgn))
        row = self._build_game_row(game_id, pgn, last_ts)
        return coerce_game_row_dict(row, model_cls=ChessGameRow), last_ts

    # TODO: Pass ChessGame instance instead of raw dict
    def _should_skip_game(
        self,
        row: dict,
        last_ts: int,
        since_ms: int,
        seen_game_ids: set[str],
    ) -> bool:
        """Determine whether a game should be skipped.

        Args:
            row: Normalized game row.
            last_ts: Game timestamp.
            since_ms: Minimum timestamp filter.
            seen_game_ids: Previously seen game identifiers.

        Returns:
            True if the game should be skipped.
        """

        if since_ms and last_ts <= since_ms:
            return True
        return str(row.get("game_id", "")) in seen_game_ids

    def _fetch_archive_pages(self, archive_url: str) -> list[dict]:
        """Fetch all pages for a given archive URL.

        Args:
            archive_url: Archive endpoint URL.

        Returns:
            List of raw game dictionaries.
        """

        games: list[dict] = []
        next_url: str | None = archive_url
        seen_urls: set[str] = set()
        while next_url:
            if next_url in seen_urls:
                self.logger.warning("Pagination loop detected for %s", archive_url)
                break
            seen_urls.add(next_url)
            payload = self._get_with_backoff(next_url, timeout=20).json()
            games.extend(payload.get("games", []))
            next_url = _next_page_url(payload, next_url)
        return games

    def _get_with_backoff(self, url: str, timeout: int) -> requests.Response:
        """Fetch a URL with exponential backoff on 429 responses.

        Args:
            url: URL to request.
            timeout: Timeout in seconds.

        Returns:
            Response object.

        Raises:
            RateLimitError: When retries are exhausted.
        """

        max_retries = max(self.settings.chesscom.max_retries, 0)
        base_backoff = max(self.settings.chesscom.retry_backoff_ms, 0) / 1000.0
        attempt = 0
        while True:
            response = requests.get(
                url,
                headers=_auth_headers(self.settings.chesscom.token),
                timeout=timeout,
            )
            if response.status_code != HTTP_STATUS_TOO_MANY_REQUESTS:
                response.raise_for_status()
                return response
            attempt = self._handle_rate_limit(response, attempt, max_retries, base_backoff)

    def _handle_rate_limit(
        self,
        response: requests.Response,
        attempt: int,
        max_retries: int,
        base_backoff: float,
    ) -> int:
        """Handle a rate-limited response.

        Args:
            response: HTTP response.
            attempt: Current attempt count.
            max_retries: Maximum retry count.
            base_backoff: Base backoff in seconds.

        Returns:
            Next attempt count.
        """

        retry_after = _parse_retry_after(response.headers.get("Retry-After"))
        if attempt >= max_retries:
            message = "Chess.com rate limit exceeded"
            raise ChesscomRateLimitError(message, response=response)
        wait_seconds = max(base_backoff * (2**attempt), retry_after or 0.0)
        self.logger.warning(
            "Chess.com rate limited (429). Retrying in %.2fs (attempt %s/%s).",
            wait_seconds,
            attempt + 1,
            max_retries,
        )
        if wait_seconds:
            time.sleep(wait_seconds)
        return attempt + 1

    @staticmethod
    def _attach_cursors(games: list[dict]) -> list[dict]:
        """Attach cursor values to each game.

        Args:
            games: Games to decorate with cursors.

        Returns:
            Games with cursor values populated.
        """

        for game in games:
            game["cursor"] = _build_cursor(
                int(game.get("last_timestamp_ms", 0)), str(game.get("game_id", ""))
            )
        return games

    @staticmethod
    def _resolve_last_timestamp(games: list[dict], cursor: str | None) -> int:
        """Resolve the latest timestamp for the response.

        Args:
            games: Fetched games.
            cursor: Cursor token to fall back on.

        Returns:
            Latest timestamp value.
        """

        if games:
            return latest_timestamp(games)
        return _parse_cursor(cursor)[0]

    @staticmethod
    def _resolve_next_cursor(games: list[dict], cursor: str | None) -> str | None:
        """Resolve the next cursor for pagination.

        Args:
            games: Fetched games.
            cursor: Cursor token to fall back on.

        Returns:
            Next cursor value.
        """

        if games:
            newest = max(games, key=lambda g: int(g.get("last_timestamp_ms", 0)))
            return _build_cursor(
                int(newest.get("last_timestamp_ms", 0)), str(newest.get("game_id", ""))
            )
        return cursor if cursor else None

    def _log_fetch_summary(self, cursor: str | None, next_cursor: str | None, count: int) -> None:
        """Log a summary for a fetch.

        Args:
            cursor: Incoming cursor.
            next_cursor: Outgoing cursor.
            count: Number of games fetched.
        """

        self.logger.info(
            "Fetched %s Chess.com PGNs with cursor=%s next_cursor=%s",
            count,
            cursor,
            next_cursor,
        )


def _client_for_settings(settings: Settings) -> ChesscomClient:
    """Return a Chess.com client for the given settings."""

    context = ChesscomClientContext(settings=settings, logger=logger)
    return ChesscomClient(context)


def _client_method[T](settings: Settings, method: Callable[..., T], *args: Any, **kwargs: Any) -> T:
    """Instantiate a client and call the provided method."""

    client = _client_for_settings(settings)
    return method(client, *args, **kwargs)


def _fetch_archive_pages(settings: Settings, archive_url: str) -> list[dict]:
    """Fetch all pages for a given archive URL.

    Args:
        settings: Settings for the request.
        archive_url: Archive endpoint URL.

    Returns:
        List of raw game dictionaries.
    """

    return _client_method(settings, ChesscomClient._fetch_archive_pages, archive_url)


def _fetch_remote_games(
    settings: Settings, since_ms: int, *, full_history: bool = False
) -> list[dict]:
    """Fetch Chess.com games from the remote API.

    Args:
        settings: Settings for the request.
        since_ms: Minimum timestamp for included games.
        full_history: Whether to fetch full history.

    Returns:
        Raw game rows.
    """

    return _client_method(
        settings,
        ChesscomClient._fetch_remote_games,
        since_ms,
        full_history,
    )


def _load_fixture_games(settings: Settings, since_ms: int) -> list[dict]:
    """Load Chess.com fixture games.

    Args:
        settings: Settings for fixtures.
        since_ms: Minimum timestamp for included games.

    Returns:
        Raw fixture games.
    """

    return _client_method(settings, ChesscomClient._load_fixture_games, since_ms)


def _get_with_backoff(settings: Settings, url: str, timeout: int) -> requests.Response:
    """Fetch a URL with exponential backoff on 429 responses.

    Args:
        settings: Settings for the request.
        url: URL to request.
        timeout: Timeout in seconds.

    Returns:
        Response object.
    """

    return _client_method(settings, ChesscomClient._get_with_backoff, url, timeout)


def fetch_incremental_games(
    settings: Settings, cursor: str | None, *, full_history: bool = False
) -> ChessFetchResult:
    """Fetch Chess.com games incrementally.

    Args:
        settings: Settings for the request.
        cursor: Cursor token.
        full_history: Whether to fetch full history.

    Returns:
        Chess.com fetch result containing games and cursor metadata.
    """

    request = ChessFetchRequest(cursor=cursor, full_history=full_history)
    return run_incremental_fetch(
        build_client=lambda: _client_for_settings(settings),
        request=request,
    )


CHESSCOM_PUBLIC_EXPORTS = [
    "ARCHIVES_URL",
    "ChesscomFetchRequest",
    "ChesscomFetchResult",
    "ChesscomRateLimitError",
    "ChesscomClient",
    "ChesscomClientContext",
    "_auth_headers",
    "_build_cursor",
    "_fetch_archive_pages",
    "_fetch_remote_games",
    "_filter_by_cursor",
    "_get_with_backoff",
    "_load_fixture_games",
    "_next_page_url",
    "_parse_cursor",
    "_parse_retry_after",
    "fetch_incremental_games",
    "read_cursor",
    "to_int",
    "write_cursor",
]

__all__ = list(CHESSCOM_PUBLIC_EXPORTS)
</file>

<file path="infra/clients/lichess_client.py">
"""Lichess client implementation and helpers (adapter layer)."""

# pylint: disable=protected-access,undefined-all-variable

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import cast

import berserk
import requests
from berserk.types.common import PerfType
from pydantic import Field
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from tactix.chess_clients.base_chess_client import (
    BaseChessClient,
    BaseChessClientContext,
    ChessFetchRequest,
    ChessFetchResult,
)
from tactix.chess_clients.chess_game_row import (
    ChessGameRow,
    build_game_row_dict,
    coerce_rows_for_model,
)
from tactix.chess_clients.fetch_helpers import run_incremental_fetch, use_fixture_games
from tactix.chess_clients.GameRowInputs import GameRowInputs
from tactix.config import Settings
from tactix.extract_game_id import extract_game_id
from tactix.extract_last_timestamp_ms import extract_last_timestamp_ms
from tactix.latest_timestamp import latest_timestamp
from tactix.load_fixture_games import FixtureGamesRequest, load_fixture_games
from tactix.read_optional_text__filesystem import _read_optional_text
from tactix.utils.logger import Logger

logger = Logger(__name__)

_PERF_TYPES: set[str] = {
    "ultraBullet",
    "bullet",
    "blitz",
    "rapid",
    "classical",
    "correspondence",
    "chess960",
    "kingOfTheHill",
    "threeCheck",
    "antichess",
    "atomic",
    "horde",
    "racingKings",
    "crazyhouse",
    "fromPosition",
}


def _coerce_perf_type(value: str | None) -> PerfType | None:
    """Coerce a string to a Lichess perf type.

    Args:
        value: Perf type string.

    Returns:
        Perf type if valid, otherwise None.
    """

    if not value:
        return None
    if value in _PERF_TYPES:
        return cast(PerfType, value)
    return None


def _coerce_pgn_text(pgn: object) -> str:
    """Coerce PGN payloads to text.

    Args:
        pgn: PGN payload from the API.

    Returns:
        PGN text.
    """

    if isinstance(pgn, (bytes, bytearray)):
        return pgn.decode("utf-8", errors="replace")
    return str(pgn)


def _extract_status_code(exc: BaseException) -> int | None:
    """Extract status codes from Lichess API exceptions.

    Args:
        exc: Exception raised by the API.

    Returns:
        Status code if available.
    """

    for attr in ("status", "status_code"):
        value = getattr(exc, attr, None)
        if isinstance(value, int):
            return value
    response = getattr(exc, "response", None)
    status_code = getattr(response, "status_code", None)
    return status_code if isinstance(status_code, int) else None


def _is_auth_error(exc: BaseException) -> bool:
    """Check whether an exception represents an auth error.

    Args:
        exc: Exception raised by the API.

    Returns:
        True for auth errors, otherwise False.
    """

    status_code = _extract_status_code(exc)
    return status_code in {401, 403}


def _resolve_perf_value(settings: Settings) -> str:
    """Resolve the perf filter value for a Lichess request.

    Args:
        settings: Settings for the request.

    Returns:
        Perf value string.
    """

    return settings.lichess_profile or settings.rapid_perf


def _parse_cached_payload(raw: str) -> dict[str, object] | None:
    try:
        payload = json.loads(raw)
    except json.JSONDecodeError:
        return None
    return payload if isinstance(payload, dict) else None


def _parse_cached_token(raw: str) -> str | None:
    """Return the access token from cached JSON payloads."""

    payload = _parse_cached_payload(raw)
    if payload is None:
        return raw
    token = payload.get("access_token")
    return str(token) if token else None


def _read_cached_token_text(path: Path) -> str | None:
    """Return cached token text if present."""

    return _read_optional_text(path)


def _read_cached_token(path: Path) -> str | None:
    """Read a cached OAuth token from disk.

    Args:
        path: Token cache path.

    Returns:
        Cached token string if available.
    """

    raw = _read_cached_token_text(path)
    if raw is None:
        return None
    return _parse_cached_token(raw)


def _resolve_access_token(settings: Settings) -> str:
    """Resolve the active access token.

    Args:
        settings: Settings for the request.

    Returns:
        Access token string (empty if missing).
    """

    if settings.lichess.token:
        return settings.lichess.token
    cached = _read_cached_token(settings.lichess_token_cache_path)
    return cached or ""


def _write_cached_token(path: Path, token: str) -> None:
    """Write a cached OAuth token to disk.

    Args:
        path: Token cache path.
        token: Token value to persist.
    """

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(
        json.dumps(
            {
                "access_token": token,
                "updated_at": datetime.now(UTC).isoformat(),
            }
        )
    )
    try:
        os.chmod(path, 0o600)
    except OSError:
        logger.warning("Unable to set permissions on token cache: %s", path)


class LichessTokenError(ValueError):
    """Raised when Lichess OAuth token refresh fails."""


def _refresh_lichess_token(settings: Settings) -> str:
    """Refresh the OAuth token using the configured refresh token.

    Args:
        settings: Settings for the request.

    Returns:
        Newly refreshed access token.

    Raises:
        LichessTokenError: When refresh configuration or response is invalid.
    """

    refresh_token, client_id, client_secret = (
        settings.lichess.oauth_refresh_token,
        settings.lichess.oauth_client_id,
        settings.lichess.oauth_client_secret,
    )
    token_url = settings.lichess.oauth_token_url
    if not all([refresh_token, client_id, client_secret]):
        raise LichessTokenError("Missing Lichess OAuth refresh token configuration")
    response = requests.post(
        token_url,
        data={
            "grant_type": "refresh_token",
            "refresh_token": refresh_token,
            "client_id": client_id,
            "client_secret": client_secret,
        },
        timeout=15,
    )
    response.raise_for_status()
    access_token = response.json().get("access_token")
    if not access_token:
        raise LichessTokenError("Missing access_token in Lichess OAuth response")
    settings.lichess.token = access_token
    _write_cached_token(settings.lichess_token_cache_path, access_token)
    return access_token


def read_checkpoint(path: Path) -> int:
    """Read a Lichess checkpoint value from disk.

    Args:
        path: Checkpoint path.

    Returns:
        Checkpoint timestamp in milliseconds.
    """

    try:
        return int(path.read_text().strip())
    except FileNotFoundError:
        return 0
    except ValueError:
        logger.warning("Invalid checkpoint file, resetting to 0: %s", path)
        return 0


def write_checkpoint(path: Path, since_ms: int) -> None:
    """Write a Lichess checkpoint value to disk.

    Args:
        path: Checkpoint path.
        since_ms: Timestamp value to persist.
    """

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(str(since_ms))


def _pgn_to_game_row(pgn: object, settings: Settings) -> dict | None:
    """Convert a PGN payload into a game row.

    Args:
        pgn: PGN payload from the API.
        settings: Settings for the request.

    Returns:
        Game row dictionary or None when empty.
    """

    if pgn is None:
        return None
    pgn_text = _coerce_pgn_text(pgn)
    game_id = extract_game_id(pgn_text)
    last_ts = extract_last_timestamp_ms(pgn_text)
    return build_game_row_dict(
        GameRowInputs(
            game_id=game_id,
            pgn=pgn_text,
            last_timestamp_ms=last_ts,
            user=settings.user,
            source=settings.source,
            fetched_at=datetime.now(UTC),
            model_cls=LichessGameRow,
        )
    )


def build_client(settings: Settings) -> berserk.Client:
    """Build a Berserk client for the Lichess API.

    Args:
        settings: Settings for the request.

    Returns:
        Berserk client instance.
    """

    token = _resolve_access_token(settings)
    session = berserk.TokenSession(token)
    return berserk.Client(session=session)


@dataclass(slots=True)
class LichessClientContext(BaseChessClientContext):
    """Context for Lichess API interactions."""


class LichessFetchRequest(ChessFetchRequest):
    """Request parameters for Lichess incremental fetches."""


class LichessFetchResult(ChessFetchResult):
    """Response payload for Lichess incremental fetches."""

    games: list[dict] = Field(default_factory=list)


class LichessGameRow(ChessGameRow):
    """Lichess game row model."""


class LichessClient(BaseChessClient):
    """Client for Lichess API interactions."""

    def __init__(self, context: LichessClientContext) -> None:
        """Initialize the client with Lichess-specific context.

        Args:
            context: Client context containing settings and logger.
        """

        super().__init__(context)

    def fetch_incremental_games(self, request: ChessFetchRequest) -> ChessFetchResult:
        """Fetch Lichess games incrementally.

        Args:
            request: Parameters for the incremental fetch.

        Returns:
            Lichess fetch result with games payload and timestamp metadata.

        Example:
            >>> client.fetch_incremental_games(LichessFetchRequest(since_ms=0))
        """

        games = self._fetch_games(request)
        last_ts = latest_timestamp(games)
        return LichessFetchResult(
            games=games,
            next_cursor=None,
            last_timestamp_ms=last_ts,
        )

    def _fetch_games(self, request: ChessFetchRequest) -> list[dict]:
        """Fetch games from fixtures or the remote API.

        Args:
            request: Request parameters for the fetch.

        Returns:
            List of game rows.
        """

        if use_fixture_games(
            self.settings.lichess.token,
            self.settings.use_fixture_when_no_token,
        ):
            return self._load_fixture_games(request.since_ms, request.until_ms)
        return self._fetch_remote_games(request.since_ms, request.until_ms)

    def _load_fixture_games(self, since_ms: int, until_ms: int | None) -> list[dict]:
        """Load Lichess fixture games.

        Args:
            since_ms: Minimum timestamp for included games.
            until_ms: Optional upper bound timestamp.

        Returns:
            Fixture game rows.
        """

        return load_fixture_games(
            FixtureGamesRequest(
                fixture_path=self.settings.fixture_pgn_path,
                user=self.settings.user,
                source=self.settings.source,
                since_ms=since_ms,
                until_ms=until_ms,
                logger=self.logger,
                coerce_rows=coerce_rows_for_model(LichessGameRow),
            )
        )

    @retry(
        retry=retry_if_exception_type(Exception),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        reraise=True,
        before_sleep=before_sleep_log(logger, logging.WARNING),
    )
    def _fetch_remote_games(self, since_ms: int, until_ms: int | None) -> list[dict]:
        """Fetch games from the remote API with retry.

        Args:
            since_ms: Minimum timestamp for included games.
            until_ms: Optional upper bound timestamp.

        Returns:
            Remote game rows.
        """

        return self._fetch_remote_games_with_refresh(since_ms, until_ms)

    def _fetch_remote_games_with_refresh(self, since_ms: int, until_ms: int | None) -> list[dict]:
        """Fetch remote games and refresh token on auth errors.

        Args:
            since_ms: Minimum timestamp for included games.
            until_ms: Optional upper bound timestamp.

        Returns:
            Remote game rows.
        """

        try:
            return self._fetch_remote_games_once(since_ms, until_ms)
        except Exception as exc:
            if self._should_refresh_token(exc):
                self.logger.warning("Refreshing Lichess OAuth token after auth failure")
                refresh_lichess_token(self.settings)
                return self._fetch_remote_games_once(since_ms, until_ms)
            raise

    def _should_refresh_token(self, exc: BaseException) -> bool:
        """Determine if a refresh token should be used.

        Args:
            exc: Exception raised by the API.

        Returns:
            True when a token refresh should be attempted.
        """

        return bool(_is_auth_error(exc) and self.settings.lichess.oauth_refresh_token)

    def _fetch_remote_games_once(self, since_ms: int, until_ms: int | None) -> list[dict]:
        """Fetch games from the remote API once.

        Args:
            since_ms: Minimum timestamp for included games.
            until_ms: Optional upper bound timestamp.

        Returns:
            Remote game rows.
        """

        client = build_client(self.settings)
        perf_type = _coerce_perf_type(_resolve_perf_value(self.settings))
        return self._collect_remote_games(client, since_ms, until_ms, perf_type)

    def _collect_remote_games(
        self,
        client: berserk.Client,
        since_ms: int,
        until_ms: int | None,
        perf_type: PerfType | None,
    ) -> list[dict]:
        """Collect game rows from the API stream.

        Args:
            client: Berserk client instance.
            since_ms: Minimum timestamp for included games.
            until_ms: Optional upper bound timestamp.
            perf_type: Lichess performance filter.

        Returns:
            Remote game rows.
        """

        self.logger.info(
            "Fetching Lichess games for user=%s since=%s", self.settings.user, since_ms
        )
        games: list[dict] = []
        for pgn in client.games.export_by_player(
            self.settings.user,
            since=since_ms or None,
            until=until_ms or None,
            perf_type=perf_type,
            evals=False,
            clocks=True,
            moves=True,
            opening=True,
            max=200,
        ):
            row = _pgn_to_game_row(pgn, self.settings)
            if row:
                games.append(row)
        self.logger.info("Fetched %s PGNs", len(games))
        return games


def _fetch_remote_games_once(
    settings: Settings, since_ms: int, until_ms: int | None = None
) -> list[dict]:
    """Fetch games from the remote API once.

    Args:
        settings: Settings for the request.
        since_ms: Minimum timestamp for included games.
        until_ms: Optional upper bound timestamp.

    Returns:
        Remote game rows.
    """

    context = LichessClientContext(settings=settings, logger=logger)
    return LichessClient(context)._fetch_remote_games_once(since_ms, until_ms)


def _fetch_remote_games(
    settings: Settings, since_ms: int, until_ms: int | None = None
) -> list[dict]:
    """Fetch games from the remote API with retry.

    Args:
        settings: Settings for the request.
        since_ms: Minimum timestamp for included games.
        until_ms: Optional upper bound timestamp.

    Returns:
        Remote game rows.
    """

    context = LichessClientContext(settings=settings, logger=logger)
    return LichessClient(context)._fetch_remote_games(since_ms, until_ms)


def fetch_incremental_games(
    settings: Settings, since_ms: int, until_ms: int | None = None
) -> list[dict]:
    """Fetch Lichess games incrementally.

    Args:
        settings: Settings for the request.
        since_ms: Minimum timestamp for included games.
        until_ms: Optional upper bound timestamp.

    Returns:
        List of game rows.
    """

    context = LichessClientContext(settings=settings, logger=logger)
    request = LichessFetchRequest(since_ms=since_ms, until_ms=until_ms)
    return run_incremental_fetch(
        build_client=lambda: LichessClient(context),
        request=request,
    ).games


def refresh_lichess_token(settings: Settings) -> None:
    """Refresh the cached Lichess OAuth token."""

    _refresh_lichess_token(settings)
</file>

<file path="infra/db/__init__.py">

</file>

<file path="infra/__init__.py">

</file>

<file path="models/__init__.py">
"""Public models exported by tactix."""

from .practice_attempt_request import PracticeAttemptRequest

__all__ = ["PracticeAttemptRequest"]
</file>

<file path="models/chess_position.py">
"""Pydantic models for chess positions."""

import chess
from pydantic import BaseModel


class ChessPosition(BaseModel):
    """Model representing a chess position in FEN notation."""

    fen: str
    turn: chess.Color
</file>

<file path="models/practice_attempt_request.py">
"""Request model for practice attempt submissions."""

from pydantic import BaseModel


class PracticeAttemptRequest(BaseModel):
    """Payload describing a user's practice attempt."""

    tactic_id: int
    position_id: int
    attempted_uci: str
    source: str | None = None
    served_at_ms: int | None = None
</file>

<file path="ports/__init__.py">
"""Port interfaces for the tactix application."""
</file>

<file path="ports/game_source_client.py">
"""Port interface for game source clients."""

# pylint: disable=too-few-public-methods

from __future__ import annotations

from typing import Protocol

from tactix.chess_clients.base_chess_client import ChessFetchRequest, ChessFetchResult


class GameSourceClient(Protocol):
    """Stable interface for chess game source clients."""

    def fetch_incremental_games(self, request: ChessFetchRequest) -> ChessFetchResult:
        """Fetch games incrementally for the requested source."""
</file>

<file path="tactics/__init__.py">

</file>

<file path="utils/__init__.py">
"""Utility exports for the tactix package."""

# pylint: disable=redefined-builtin

from .generate_id import generate_id
from .hasher import Hasher, hash, hash_file
from .logger import Logger, funclogger
from .normalize_string import normalize_string
from .now import Now
from .to_int import to_int

__all__ = [
    "Hasher",
    "Logger",
    "Now",
    "funclogger",
    "generate_id",
    "hash",
    "hash_file",
    "normalize_string",
    "to_int",
]
</file>

<file path="utils/generate_id.py">
"""Helpers for generating unique identifiers."""

from uuid import uuid4


def generate_id() -> str:
    """Generate a unique identifier string.

    Returns:
        A unique identifier as a string.
    """
    return str(uuid4())
</file>

<file path="utils/hasher.py">
"""Hashing helpers for strings, bytes, and files."""

# pylint: disable=redefined-builtin

import hashlib


class Hasher:
    """
    Hasher provides static methods for generating SHA256 hashes from strings, bytes, and files.

    Methods
    -------
    hash_string(input_string: str) -> str
        Returns a SHA256 hash of the input string.
    hash_bytes(input_bytes: bytes) -> str
        Returns a SHA256 hash of the input bytes.
    hash_file(file_path: str) -> str
        Returns a SHA256 hash of the contents of the specified file.
    """

    @staticmethod
    def hash_string(input_string: str) -> str:
        """Returns a SHA256 hash of the input string."""

        return hashlib.sha256(input_string.encode("utf-8")).hexdigest()

    @staticmethod
    def hash_bytes(input_bytes: bytes) -> str:
        """Returns a SHA256 hash of the input bytes."""

        return hashlib.sha256(input_bytes).hexdigest()

    @staticmethod
    def hash_file(file_path: str) -> str:
        """Returns a SHA256 hash of the contents of the specified file."""

        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()


def hash(data: str | bytes, is_bytes: bool = False) -> str:
    """
    Hashes the given data using the Hasher utility.

    Depending on the type of input and the `is_bytes` flag, this function delegates
    to either `Hasher.hash_bytes` or `Hasher.hash_string` to compute a hash of the
    input data.

    Parameters
    ----------
    data : str or bytes
        The data to be hashed. If `is_bytes` is True, this should be a bytes object;
        otherwise, it should be a string.
    is_bytes : bool, optional
        If True, treats `data` as bytes and uses `Hasher.hash_bytes`. If False,
        treats `data` as a string and uses `Hasher.hash_string`. Default is False.

    Returns
    -------
    str
        The hexadecimal string representation of the hash of the input data.

    Raises
    ------
    TypeError
        If `is_bytes` is True and `data` is not of type bytes, or if `is_bytes` is
        False and `data` is not of type str.

    Examples
    --------
    >>> hash("hello world")
    '5eb63bbbe01eeed093cb22bb8f5acdc3'

    >>> hash(b"hello world", is_bytes=True)
    '5eb63bbbe01eeed093cb22bb8f5acdc3'

    Commentary
    ----------
    As a standalone function, `hash` serves as a thin wrapper around the `Hasher`
    class, providing a unified interface for hashing both strings and bytes. While
    this can be convenient, in a large project it may be preferable to integrate
    this functionality directly into a broader utility module or class that handles
    all hashing and encoding concerns, rather than maintaining a single-purpose
    module for such a simple wrapper. This would help group related functionality
    and reduce fragmentation of utility code.
    """
    if is_bytes:
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError("Expected bytes for hashing")
        return Hasher.hash_bytes(data)
    if not isinstance(data, str):
        raise TypeError("Expected str for hashing")
    return Hasher.hash_string(data)


def hash_file(file_path: str) -> str:
    """Hash the contents of a file using SHA-256.

    Args:
        file_path: The path to the file to hash.

    Returns:
        The SHA-256 hash of the file contents as a hexadecimal string.
    """
    return Hasher.hash_file(file_path)
</file>

<file path="utils/logger.py">
"""Logger configuration and convenience helpers."""

from __future__ import annotations

import builtins
import logging
import sys
import time
from functools import wraps

_DEFAULT_LOGGER_NAME = "tactix"
_DEFAULT_LOG_LEVEL = logging.DEBUG
_DEFAULT_FORMATTER = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
_DEFAULT_HANDLER = logging.StreamHandler(sys.stdout)
_DEFAULT_HANDLER.setFormatter(_DEFAULT_FORMATTER)


def _configure_logger(logger: logging.Logger, level: int) -> None:
    """
    Configures a logger with the specified logging level and default handler.

    If the logger's level is not set, this function sets it to the provided level.
    It also ensures that a default handler is attached if no handlers are present,
    and disables propagation to ancestor loggers.

    Parameters
    ----------
    logger : logging.Logger
        The logger instance to configure.
    level : int
        The logging level to set if the logger's level is not already set.

    Returns
    -------
    None

    Raises
    ------
    None

    Examples
    --------
    >>> import logging
    >>> from tactix.utils.logger import _configure_logger
    >>> logger = logging.getLogger("my_logger")
    >>> _configure_logger(logger, logging.INFO)
    >>> logger.info("This is an info message.")

    Commentary
    ----------
    This function provides a focused utility for logger configuration, which can be
    useful in projects that require consistent logger setup. However, as a private
    function (indicated by the leading underscore), it may be better suited as part
    of a larger logging utility module rather than as a standalone function. In a
    large project, grouping related logging configuration helpers together would
    promote better organization and maintainability.
    """
    if logger.level == logging.NOTSET:
        logger.setLevel(level)
    if not logger.handlers:
        logger.addHandler(_DEFAULT_HANDLER)
    logger.propagate = False


def get_logger(name: str | None = None, level: int = _DEFAULT_LOG_LEVEL) -> logging.Logger:
    """Return a configured logger for the given name."""
    logger = logging.getLogger(name or _DEFAULT_LOGGER_NAME)
    _configure_logger(logger, level)
    return logger


builtins.get_logger = get_logger


def set_level(level: int, logger_names: list[str] | None = None) -> None:
    """Set the log level for one or more logger names."""
    names = logger_names or [_DEFAULT_LOGGER_NAME, "airflow", "uvicorn"]
    for name in names:
        logging.getLogger(name).setLevel(level)


def Logger(name: str = _DEFAULT_LOGGER_NAME, level: int = _DEFAULT_LOG_LEVEL) -> logging.Logger:
    """Return a configured logger instance."""
    return get_logger(name, level)


def funclogger(func):  # noqa: PLR0915
    """Decorator to add logging to functions:

    Logs the function path/module/name.
    Logs the start and end of the function execution.
    Loops through args and kwargs to log their values.
    """

    @wraps(func)
    def wrapper(*args, **kwargs):  # noqa: PLR0915
        module_name = func.__module__
        function_name = func.__qualname__
        full_name = f"{module_name}.{function_name}"
        function_path = full_name.replace("<", "").replace(">", "")

        logger = Logger(function_path)

        logger.debug("Path: %s", function_path)
        logger.debug("Module: %s", module_name)
        logger.debug("Function: %s", function_name)
        logger.debug("\nArgs:")
        logger.debug("-----")
        for i, arg in enumerate(args):
            logger.debug(" %s. %s (%s)", i, arg, type(arg).__name__)

        logger.debug("\nKwargs:")
        logger.debug("-------")
        for key, value in kwargs.items():
            logger.debug(" - %s (%s): %s", key, type(value).__name__, value)

        logger.debug("Starting %s", function_name)
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        logger.debug("Finished %s in %.4f seconds", function_name, elapsed_time)
        logger.debug("Return Value: %s (%s)", result, type(result).__name__)
        return result

    return wrapper
</file>

<file path="utils/normalize_string.py">
"""String normalization helpers."""

from __future__ import annotations


def normalize_string(value: str | None) -> str:
    """
    Normalizes a string by stripping leading and trailing whitespace and converting to lowercase.

    Parameters
    ----------
    value : str or None
        The input string to normalize. If None, an empty string is used.

    Returns
    -------
    str
        The normalized string, with whitespace removed from both ends and all characters
        converted to lowercase.

    Raises
    ------
    None

    Examples
    --------
    >>> normalize_string("  Hello World  ")
    'hello world'
    >>> normalize_string(None)
    ''
    >>> normalize_string("  PYTHON  ")
    'python'

    Commentary
    ----------
    This function provides a simple utility for normalizing strings, which is a common
    requirement in data cleaning and preprocessing tasks. While useful, its functionality is
    quite basic and generic. In a large project, it may make more sense to group this function
    with other string normalization or utility functions in a single module, rather than
    having it as a standalone module. This would help maintain logical cohesion and reduce
    fragmentation of utility code.
    """
    return (value or "").strip().lower()
</file>

<file path="utils/now.py">
"""Time helper utilities."""

from datetime import UTC, datetime


class Now:
    """Utility methods for current time and UTC conversion."""

    @staticmethod
    def as_datetime() -> datetime:
        """Return the current UTC time as a datetime object."""

        return datetime.now(UTC)

    @staticmethod
    def as_dt() -> datetime:
        """Return the current UTC time as a datetime object. Alias for as_datetime()."""

        return datetime.now(UTC)

    @staticmethod
    def as_seconds(as_int: bool = False) -> float:
        """Return the current UTC time as a float timestamp in seconds."""

        if as_int:
            return int(datetime.now(UTC).timestamp())
        return datetime.now(UTC).timestamp()

    @staticmethod
    def as_milliseconds() -> int:
        """Return the current UTC time as an integer timestamp in milliseconds."""

        return int(datetime.now(UTC).timestamp() * 1000)

    @staticmethod
    def to_utc(dt: datetime | None) -> datetime | None:
        """Convert a datetime object to UTC timezone."""

        try:
            if dt is None:
                return None
            if dt.tzinfo is None:
                return dt.replace(tzinfo=UTC)
            return dt.astimezone(UTC)
        except (TypeError, ValueError):
            return None


_VULTURE_USED = (Now.as_dt, Now.as_seconds, Now.as_milliseconds)
</file>

<file path="utils/to_int.py">
"""Integer coercion helpers."""


def to_int(value: object) -> int | None:
    """Coerce a value to an integer if possible.

    Args:
        value: Value to coerce.

    Returns:
        Integer value or None.
    """

    if isinstance(value, int):
        return value
    if isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            return None
    return None
</file>

<file path="__init__.py">
"""TACTIX package entrypoints."""

import sys
from importlib import import_module
from typing import TYPE_CHECKING

from tactix.chess_game import ChessGame
from tactix.pipeline import run_daily_game_sync
from tactix.sync_contexts import DailyGameSyncRequest

chesscom_client = import_module("tactix.infra.clients.chesscom_client")
duckdb_store = import_module("tactix.db.duckdb_store")
lichess_client = import_module("tactix.infra.clients.lichess_client")

_LEGACY_MODULE_ALIASES = {
    "tactix.DashboardQueryFilters": "tactix.dashboard_query_filters",
    "tactix.CLOCK_PARTS_SHORT": "tactix._normalize_clock_parts",
    "tactix.PGN_SCHEMA": "tactix.define_db_schemas__const",
    "tactix.NoGamesPayloadContext": "tactix.sync_contexts",
    "tactix.NoGamesAfterDedupePayloadContext_1": "tactix.sync_contexts",
    "tactix.duckdb_store": "tactix.db.duckdb_store",
}

for legacy_name, target in _LEGACY_MODULE_ALIASES.items():
    if legacy_name not in sys.modules:
        sys.modules[legacy_name] = import_module(target)


def main() -> None:
    """Run a single end-to-end pipeline execution."""
    result = run_daily_game_sync(DailyGameSyncRequest())
    print(result)


__all__ = [
    "ChessGame",
    "chesscom_client",
    "duckdb_store",
    "lichess_client",
    "main",
    "run_daily_game_sync",
]

if TYPE_CHECKING:
    from tactix.analyzer import TacticsAnalyzer
    from tactix.engine_result import EngineResult
    from tactix.get_material_value import get_material_value
    from tactix.models.chess_position import ChessPosition
    from tactix.PostgresSettings import PostgresSettings
    from tactix.refresh_metrics_result import RefreshMetricsResult

    _ = (
        TacticsAnalyzer,
        TacticsAnalyzer.analyze,
        EngineResult.empty,
        get_material_value,
        ChessPosition,
        PostgresSettings.is_configured,
        RefreshMetricsResult,
        RefreshMetricsResult.metrics_rows,
    )
</file>

<file path="_apply_engine_options.py">
"""Apply Stockfish configuration options."""

from typing import Any

import chess.engine

from tactix._configure_engine_options import _configure_engine_options
from tactix._filter_supported_options import _filter_supported_options


def _apply_engine_options(
    engine: chess.engine.SimpleEngine,
    options: dict[str, Any],
) -> dict[str, Any]:
    """Apply supported engine options and return applied values."""
    applied_options = _filter_supported_options(engine, options)
    return _configure_engine_options(engine, applied_options)
</file>

<file path="_apply_fork_severity_floor.py">
"""Apply fork severity floors to computed scores."""

from tactix._fork_floor_for_settings import _fork_floor_for_settings
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _apply_fork_severity_floor(
    severity: float,
    motif: str,
    settings: Settings | None,
) -> float:
    """Return the severity after applying the fork floor."""
    if motif != "fork":
        return severity
    floor = _fork_floor_for_settings(settings)
    if floor is None:
        return severity
    return max(severity, floor)
</file>

<file path="_apply_mate_overrides.py">
from dataclasses import replace

from tactix._apply_outcome__unclear_mate_in_one import _apply_outcome__unclear_mate_in_one
from tactix._apply_outcome__unclear_mate_in_two import (
    _apply_outcome__unclear_mate_in_two,
)
from tactix._override_mate_motif import _override_mate_motif
from tactix.mate_outcome import MateOutcome

MateOverrideResult = tuple[str, str, int | None]


def _apply_mate_overrides(ctx: MateOutcome) -> MateOverrideResult:
    mate_in = ctx.mate_in
    motif = _override_mate_motif(ctx.outcome.motif, mate_in)
    result = _apply_outcome__unclear_mate_in_one(ctx, mate_in=mate_in)
    updated_context = replace(ctx.outcome, result=result)
    result = _apply_outcome__unclear_mate_in_two(
        replace(ctx, outcome=updated_context),
        mate_in=mate_in,
    )
    if ctx.should_be_upgraded:
        result = "failed_attempt"
    return result, motif, mate_in
</file>

<file path="_apply_outcome__failed_attempt_discovered_attack.py">
"""Apply failed-attempt overrides for discovered attacks."""

from tactix._select_motif__discovered_attack_target import _select_motif__discovered_attack_target
from tactix._should_override__discovered_attack_failed_attempt import (
    _should_override__discovered_attack_failed_attempt,
)


def _apply_outcome__failed_attempt_discovered_attack(
    result: str,
    motif: str,
    best_motif: str | None,
    swing: int | None,
    threshold: int | None,
) -> tuple[str, str]:
    """Apply failed-attempt override for discovered attack motifs."""
    target_motif = _select_motif__discovered_attack_target(motif, best_motif)
    if _should_override__discovered_attack_failed_attempt(result, swing, threshold, target_motif):
        return "failed_attempt", target_motif
    return result, motif
</file>

<file path="_apply_outcome__failed_attempt_discovered_check.py">
from tactix._select_motif__discovered_check_target import _select_motif__discovered_check_target
from tactix._should_override__discovered_check_failed_attempt import (
    _should_override__discovered_check_failed_attempt,
)


def _apply_outcome__failed_attempt_discovered_check(
    result: str,
    motif: str,
    best_motif: str | None,
    swing: int | None,
    threshold: int | None,
) -> tuple[str, str]:
    target_motif = _select_motif__discovered_check_target(motif, best_motif)
    if _should_override__discovered_check_failed_attempt(result, swing, threshold, target_motif):
        return "failed_attempt", target_motif
    return result, motif
</file>

<file path="_apply_outcome__failed_attempt_hanging_piece.py">
from tactix._select_motif__hanging_piece_target import _select_motif__hanging_piece_target
from tactix._should_override__hanging_piece_failed_attempt import (
    _should_override__hanging_piece_failed_attempt,
)


def _apply_outcome__failed_attempt_hanging_piece(
    result: str,
    motif: str,
    best_motif: str | None,
    swing: int | None,
    threshold: int | None,
) -> tuple[str, str]:
    target_motif = _select_motif__hanging_piece_target(motif, best_motif)
    if _should_override__hanging_piece_failed_attempt(result, swing, threshold, target_motif):
        return "failed_attempt", target_motif
    return result, motif
</file>

<file path="_apply_outcome__failed_attempt_line_tactics.py">
"""Apply failed-attempt overrides for line tactics."""

from tactix._apply_outcome__failed_attempt_discovered_attack import (
    _apply_outcome__failed_attempt_discovered_attack,
)
from tactix._apply_outcome__failed_attempt_discovered_check import (
    _apply_outcome__failed_attempt_discovered_check,
)
from tactix._apply_outcome__failed_attempt_pin import _apply_outcome__failed_attempt_pin
from tactix._apply_outcome__failed_attempt_skewer import _apply_outcome__failed_attempt_skewer
from tactix._compute_eval__failed_attempt_threshold import (
    _compute_eval__failed_attempt_threshold,
)
from tactix.config import Settings


def _apply_outcome__failed_attempt_line_tactics(
    result: str,
    motif: str,
    best_motif: str | None,
    swing: int | None,
    settings: Settings | None,
) -> tuple[str, str]:
    """Apply failed-attempt overrides for line tactics."""
    result, motif = _apply_outcome__failed_attempt_pin(
        result,
        motif,
        best_motif,
        swing,
        _compute_eval__failed_attempt_threshold("pin", settings),
    )
    result, motif = _apply_outcome__failed_attempt_skewer(
        result,
        motif,
        best_motif,
        swing,
        _compute_eval__failed_attempt_threshold("skewer", settings),
    )
    result, motif = _apply_outcome__failed_attempt_discovered_attack(
        result,
        motif,
        best_motif,
        swing,
        _compute_eval__failed_attempt_threshold("discovered_attack", settings),
    )
    return _apply_outcome__failed_attempt_discovered_check(
        result,
        motif,
        best_motif,
        swing,
        _compute_eval__failed_attempt_threshold("discovered_check", settings),
    )
</file>

<file path="_apply_outcome__failed_attempt_pin.py">
"""Apply failed-attempt overrides for pin motifs."""

from tactix._select_motif__pin_target import _select_motif__pin_target
from tactix._should_override__pin_failed_attempt import _should_override__pin_failed_attempt


def _apply_outcome__failed_attempt_pin(
    result: str,
    motif: str,
    best_motif: str | None,
    swing: int | None,
    threshold: int | None,
) -> tuple[str, str]:
    target_motif = _select_motif__pin_target(motif, best_motif)
    if _should_override__pin_failed_attempt(result, swing, threshold, target_motif):
        return "failed_attempt", target_motif
    return result, motif
</file>

<file path="_apply_outcome__failed_attempt_skewer.py">
from tactix._select_motif__skewer_target import _select_motif__skewer_target
from tactix._should_override__skewer_failed_attempt import _should_override__skewer_failed_attempt


def _apply_outcome__failed_attempt_skewer(
    result: str,
    motif: str,
    best_motif: str | None,
    swing: int | None,
    threshold: int | None,
) -> tuple[str, str]:
    target_motif = _select_motif__skewer_target(motif, best_motif)
    if _should_override__skewer_failed_attempt(result, swing, threshold, target_motif):
        return "failed_attempt", target_motif
    return result, motif
</file>

<file path="_apply_outcome__unclear_discovered_attack.py">
from tactix._apply_outcome__unclear_variant import _apply_outcome__unclear_variant
from tactix._should_mark_unclear_discovered_attack import _should_mark_unclear_discovered_attack
from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear_discovered_attack(
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    return _apply_outcome__unclear_variant(
        _should_mark_unclear_discovered_attack,
        context,
        *args,
        **kwargs,
    )
</file>

<file path="_apply_outcome__unclear_discovered_check.py">
from tactix._apply_outcome__unclear_variant import _apply_outcome__unclear_variant
from tactix._should_mark_unclear_discovered_check import _should_mark_unclear_discovered_check
from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear_discovered_check(
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    return _apply_outcome__unclear_variant(
        _should_mark_unclear_discovered_check,
        context,
        *args,
        **kwargs,
    )
</file>

<file path="_apply_outcome__unclear_fork.py">
from tactix._apply_outcome__unclear_variant import _apply_outcome__unclear_variant
from tactix._should_mark_unclear_fork import _should_mark_unclear_fork
from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear_fork(
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    return _apply_outcome__unclear_variant(
        _should_mark_unclear_fork,
        context,
        *args,
        **kwargs,
    )
</file>

<file path="_apply_outcome__unclear_hanging_piece.py">
"""Apply unclear outcome logic for hanging piece motifs."""

from tactix._apply_outcome__unclear_variant import _apply_outcome__unclear_variant
from tactix._should_mark_unclear_hanging_piece import _should_mark_unclear_hanging_piece
from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear_hanging_piece(
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    return _apply_outcome__unclear_variant(
        _should_mark_unclear_hanging_piece,
        context,
        *args,
        **kwargs,
    )
</file>

<file path="_apply_outcome__unclear_mate_in_one.py">
"""Apply unclear mate-in-one outcomes."""

from tactix._apply_outcome__unclear_mate import (
    _apply_outcome__unclear_mate,
    _build_mate_outcome_args,
)
from tactix._should_mark_unclear_mate_in_one import _should_mark_unclear_mate_in_one
from tactix.mate_outcome import MateOutcome


def _apply_outcome__unclear_mate_in_one(
    context: MateOutcome | str,
    best_move: str | None = None,
    user_move_uci: str | None = None,
    after_cp: int | None = None,
    mate_in: int | None = None,
) -> str:
    return _apply_outcome__unclear_mate(
        _should_mark_unclear_mate_in_one,
        context,
        args=_build_mate_outcome_args(best_move, user_move_uci, after_cp, mate_in),
    )
</file>

<file path="_apply_outcome__unclear_mate_in_two.py">
"""Apply unclear mate-in-two outcomes."""

from tactix._apply_outcome__unclear_mate import (
    _apply_outcome__unclear_mate,
    _build_mate_outcome_args,
)
from tactix._should_mark_unclear_mate_in_two import _should_mark_unclear_mate_in_two
from tactix.mate_outcome import MateOutcome


def _apply_outcome__unclear_mate_in_two(
    context: MateOutcome | str,
    best_move: str | None = None,
    user_move_uci: str | None = None,
    after_cp: int | None = None,
    mate_in: int | None = None,
) -> str:
    return _apply_outcome__unclear_mate(
        _should_mark_unclear_mate_in_two,
        context,
        args=_build_mate_outcome_args(best_move, user_move_uci, after_cp, mate_in),
    )
</file>

<file path="_apply_outcome__unclear_mate.py">
"""Shared helper for unclear mate outcomes."""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass
from typing import cast

from tactix._apply_outcome__unclear import _apply_outcome__unclear
from tactix.mate_outcome import MateOutcome
from tactix.outcome_context import BaseOutcomeContext


@dataclass(frozen=True)
class MateOutcomeArgs:
    """Arguments for unclear mate outcome handling."""

    best_move: str | int | None = None
    user_move_uci: str | None = None
    after_cp: int | None = None
    mate_in: int | None = None


def _build_mate_outcome(
    result: str,
    best_move: str | None,
    user_move_uci: str,
    after_cp: int,
) -> BaseOutcomeContext:
    return BaseOutcomeContext(
        result=result,
        motif="mate",
        best_move=best_move,
        user_move_uci=user_move_uci,
        swing=None,
        after_cp=after_cp,
    )


def _should_shift_mate_in_arg(
    context: MateOutcome | str,
    args: MateOutcomeArgs,
) -> bool:
    return all(
        (
            isinstance(context, MateOutcome),
            args.mate_in is None,
            args.user_move_uci is None,
            args.after_cp is None,
            isinstance(args.best_move, (int, type(None))),
        )
    )


def _resolve_mate_context(
    context: MateOutcome | str,
    best_move: str | None,
    user_move_uci: str | None,
    after_cp: int | None,
) -> MateOutcome:
    if isinstance(context, MateOutcome):
        return context
    if user_move_uci is None or after_cp is None:
        raise TypeError("result, user_move_uci, and after_cp are required")
    outcome = _build_mate_outcome(context, best_move, user_move_uci, after_cp)
    return MateOutcome(
        outcome=outcome,
        after_cp=after_cp,
        mate_in_one=False,
        mate_in_two=False,
    )


def _apply_outcome__unclear_mate(
    should_mark: Callable[[BaseOutcomeContext, int | None], bool],
    context: MateOutcome | str,
    args: MateOutcomeArgs | None = None,
) -> str:
    if args is None:
        args = MateOutcomeArgs()
    best_move = args.best_move
    mate_in = args.mate_in
    if _should_shift_mate_in_arg(context, args):
        mate_in = cast(int | None, best_move)
        best_move = None
    resolved = _resolve_mate_context(
        context,
        cast(str | None, best_move),
        args.user_move_uci,
        args.after_cp,
    )
    return _apply_outcome__unclear(
        should_mark,
        resolved.outcome,
        mate_in,
    )


def _build_mate_outcome_args(
    best_move: str | None,
    user_move_uci: str | None,
    after_cp: int | None,
    mate_in: int | None,
) -> MateOutcomeArgs:
    return MateOutcomeArgs(
        best_move=best_move,
        user_move_uci=user_move_uci,
        after_cp=after_cp,
        mate_in=mate_in,
    )
</file>

<file path="_apply_outcome__unclear_pin.py">
"""Apply unclear pin outcomes when appropriate."""

from tactix._apply_outcome__unclear_variant import _apply_outcome__unclear_variant
from tactix._should_mark_unclear_pin import _should_mark_unclear_pin
from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear_pin(
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    return _apply_outcome__unclear_variant(
        _should_mark_unclear_pin,
        context,
        *args,
        **kwargs,
    )
</file>

<file path="_apply_outcome__unclear_skewer.py">
"""Apply unclear skewer outcomes when needed."""

from tactix._apply_outcome__unclear_variant import _apply_outcome__unclear_variant

# pylint: disable=redefined-outer-name
from tactix._should_mark_unclear_skewer import _should_mark_unclear_skewer
from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear_skewer(
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    return _apply_outcome__unclear_variant(
        _should_mark_unclear_skewer,
        context,
        *args,
        **kwargs,
    )
</file>

<file path="_apply_outcome__unclear_variant.py">
"""Shared helper for unclear outcome variants."""

from __future__ import annotations

from collections.abc import Callable
from typing import cast

from tactix._apply_outcome__unclear import _apply_outcome__unclear
from tactix.outcome_context import BaseOutcomeContext
from tactix.resolve_unclear_outcome_context import resolve_unclear_outcome_context
from tactix.unclear_outcome_params import UnclearOutcomeParams


def _apply_outcome__unclear_variant(
    should_mark: Callable[[BaseOutcomeContext, int | None], bool],
    context: BaseOutcomeContext | str,
    *args: object,
    **kwargs: object,
) -> str:
    threshold = kwargs.get("threshold")
    if isinstance(context, BaseOutcomeContext) and args:
        if threshold is None:
            threshold = args[0]
        args = ()
    legacy = {
        "motif": kwargs.get("motif"),
        "best_move": kwargs.get("best_move"),
        "user_move_uci": kwargs.get("user_move_uci"),
        "swing": kwargs.get("swing"),
    }
    resolved, resolved_threshold = resolve_unclear_outcome_context(
        context,
        cast(int | str | None, threshold),
        args,
        cast(UnclearOutcomeParams | None, kwargs.get("params")),
        legacy,
    )
    return _apply_outcome__unclear(should_mark, resolved, resolved_threshold)
</file>

<file path="_apply_outcome__unclear.py">
"""Apply unclear outcomes based on heuristic checks."""

from __future__ import annotations

# pylint: disable=redefined-outer-name
from collections.abc import Callable

from tactix.outcome_context import BaseOutcomeContext


def _apply_outcome__unclear(
    should_mark: Callable[[BaseOutcomeContext, int | None], bool],
    ctx: BaseOutcomeContext,
    threshold: int | None,
) -> str:
    if should_mark(ctx, threshold):
        return "unclear"
    return ctx.result
</file>

<file path="_apply_outcome_overrides.py">
"""Apply outcome overrides based on eval thresholds."""

from dataclasses import replace

from tactix._apply_mate_overrides import _apply_mate_overrides
from tactix._apply_outcome__failed_attempt_hanging_piece import (
    _apply_outcome__failed_attempt_hanging_piece,
)
from tactix._apply_outcome__failed_attempt_line_tactics import (
    _apply_outcome__failed_attempt_line_tactics,
)
from tactix._apply_outcome__unclear_discovered_attack import (
    _apply_outcome__unclear_discovered_attack,
)
from tactix._apply_outcome__unclear_discovered_check import _apply_outcome__unclear_discovered_check
from tactix._apply_outcome__unclear_fork import _apply_outcome__unclear_fork
from tactix._apply_outcome__unclear_hanging_piece import _apply_outcome__unclear_hanging_piece
from tactix._apply_outcome__unclear_pin import _apply_outcome__unclear_pin
from tactix._apply_outcome__unclear_skewer import _apply_outcome__unclear_skewer
from tactix._compute_eval__discovered_attack_unclear_threshold import (
    _compute_eval__discovered_attack_unclear_threshold,
)
from tactix._compute_eval__discovered_check_unclear_threshold import (
    _compute_eval__discovered_check_unclear_threshold,
)
from tactix._compute_eval__failed_attempt_threshold import (
    _compute_eval__failed_attempt_threshold,
)
from tactix._compute_eval__fork_unclear_threshold import _compute_eval__fork_unclear_threshold
from tactix._compute_eval__hanging_piece_unclear_threshold import (
    _compute_eval__hanging_piece_unclear_threshold,
)
from tactix._compute_eval__pin_unclear_threshold import _compute_eval__pin_unclear_threshold
from tactix._compute_eval__skewer_unclear_threshold import _compute_eval__skewer_unclear_threshold
from tactix.mate_outcome import MateOutcome
from tactix.outcome_context import OutcomeOverridesContext


def _apply_outcome_overrides(
    ctx: OutcomeOverridesContext,
) -> tuple[str, str, int | None]:
    """Apply override rules and return updated result and motif."""
    outcome = ctx.outcome
    result, motif = _apply_outcome__failed_attempt_line_tactics(
        outcome.result,
        outcome.motif,
        ctx.best_motif,
        outcome.swing,
        ctx.settings,
    )
    outcome = replace(outcome, result=result, motif=motif)
    result = _apply_outcome__unclear_fork(
        outcome,
        _compute_eval__fork_unclear_threshold(ctx.settings),
    )
    outcome = replace(outcome, result=result)
    result = _apply_outcome__unclear_skewer(
        outcome,
        _compute_eval__skewer_unclear_threshold(ctx.settings),
    )
    outcome = replace(outcome, result=result)
    result = _apply_outcome__unclear_discovered_attack(
        outcome,
        _compute_eval__discovered_attack_unclear_threshold(ctx.settings),
    )
    outcome = replace(outcome, result=result)
    result = _apply_outcome__unclear_discovered_check(
        outcome,
        _compute_eval__discovered_check_unclear_threshold(ctx.settings),
    )
    outcome = replace(outcome, result=result)
    result = _apply_outcome__unclear_hanging_piece(
        outcome,
        _compute_eval__hanging_piece_unclear_threshold(ctx.settings),
    )
    result, motif = _apply_outcome__failed_attempt_hanging_piece(
        result,
        outcome.motif,
        ctx.best_motif,
        outcome.swing,
        _compute_eval__failed_attempt_threshold("hanging_piece", ctx.settings),
    )
    outcome = replace(outcome, result=result, motif=motif)
    result = _apply_outcome__unclear_pin(
        outcome,
        _compute_eval__pin_unclear_threshold(ctx.settings),
    )
    outcome = replace(outcome, result=result)
    mate_context = MateOutcome(
        outcome=outcome,
        after_cp=ctx.after_cp,
        mate_in_one=ctx.mate_in_one,
        mate_in_two=ctx.mate_in_two,
    )
    return _apply_mate_overrides(mate_context)
</file>

<file path="_best_san_from_fen.py">
"""Convert best UCI move to SAN from a FEN position."""

import chess


def _best_san_from_fen(fen: str | None, best_uci: str) -> str | None:
    """Return SAN for the best UCI move when possible."""
    if not fen:
        return None
    try:
        board = chess.Board(str(fen))
        move = chess.Move.from_uci(best_uci)
        return board.san(move) if move in board.legal_moves else None
    except (ValueError, AssertionError):
        return None
</file>

<file path="_build_pgn_context.py">
"""Build PgnContext objects from user inputs."""

from tactix.pgn_context_kwargs import build_pgn_context_kwargs_from_values
from tactix.PgnContext import PgnContext


def _build_pgn_context(
    pgn: str | PgnContext,
    user: str | None = None,
    source: str | None = None,
    game_id: str | None = None,
    side_to_move_filter: str | None = None,
) -> PgnContext:
    """Return a PgnContext from user inputs."""
    if isinstance(pgn, PgnContext):
        return pgn
    if user is None or source is None:
        raise ValueError("user and source are required when pgn is a string")

    kwargs = build_pgn_context_kwargs_from_values(
        pgn,
        user,
        source,
        game_id,
        side_to_move_filter,
    )
    return PgnContext(**kwargs)
</file>

<file path="_build_pgn_upsert_plan.py">
"""Build PGN upsert plans from raw rows."""

from collections.abc import Callable, Mapping
from datetime import datetime
from typing import cast

from tactix.define_base_db_store__db_store import BaseDbStore
from tactix.PgnUpsertInputs import PgnUpsertHashing, PgnUpsertInputs, PgnUpsertTimestamps
from tactix.PgnUpsertPlan import PgnUpsertPlan


def _resolve_hash_pgn__pgn_upsert_plan() -> Callable[[str], str]:
    return BaseDbStore.hash_pgn


def _build_pgn_upsert_hashing__pgn_upsert_plan(
    hash_pgn: Callable[[str], str],
) -> PgnUpsertHashing:
    return PgnUpsertHashing(normalize_pgn=None, hash_pgn=hash_pgn)


def _build_pgn_upsert_timestamps__pgn_upsert_plan(
    row: Mapping[str, object],
) -> PgnUpsertTimestamps:
    # Justification: mapping raw row timestamps into a dedicated DTO needs multiple fields.
    return PgnUpsertTimestamps(
        fetched_at=cast(datetime | None, row.get("fetched_at")),
        ingested_at=cast(datetime | None, row.get("ingested_at")),
        last_timestamp_ms=cast(int, row.get("last_timestamp_ms", 0)),
    )


def _build_pgn_upsert_inputs__pgn_upsert_plan(
    row: Mapping[str, object],
    latest_hash: str | None,
    latest_version: int,
    hash_pgn: Callable[[str], str],
) -> PgnUpsertInputs:
    # Justification: assembling the input DTO requires multiple fields from the raw row.
    return PgnUpsertInputs(
        pgn_text=str(row.get("pgn") or ""),
        user=str(row.get("user") or ""),
        latest_hash=latest_hash,
        latest_version=latest_version,
        hashing=_build_pgn_upsert_hashing__pgn_upsert_plan(hash_pgn),
        timestamps=_build_pgn_upsert_timestamps__pgn_upsert_plan(row),
        cursor=row.get("cursor"),
    )


def _build_pgn_upsert_plan(
    row: Mapping[str, object],
    latest_hash: str | None,
    latest_version: int,
) -> PgnUpsertPlan | None:
    """Return a PGN upsert plan for the given raw row."""
    hash_pgn = _resolve_hash_pgn__pgn_upsert_plan()
    inputs = _build_pgn_upsert_inputs__pgn_upsert_plan(
        row,
        latest_hash,
        latest_version,
        hash_pgn,
    )
    return BaseDbStore.build_pgn_upsert_plan(inputs)
</file>

<file path="_build_raw_pgn_summary.py">
"""Build raw PGN summary payloads."""

from collections.abc import Mapping
from typing import Any

from tactix.db.raw_pgn_summary import build_raw_pgn_summary_payload


def _build_raw_pgn_summary(
    sources: list[Mapping[str, Any]], totals: Mapping[str, Any]
) -> dict[str, Any]:
    """Return a normalized raw PGN summary payload."""
    return build_raw_pgn_summary_payload(sources=sources, totals=totals)
</file>

<file path="_build_schema_label.py">
from tactix.config import Settings
from tactix.define_db_schemas__const import ANALYSIS_SCHEMA, PGN_SCHEMA


def _build_schema_label(settings: Settings) -> str:
    schema_label = "tactix_ops"
    if settings.postgres_analysis_enabled:
        schema_label = f"{schema_label},{ANALYSIS_SCHEMA}"
    if settings.postgres_pgns_enabled:
        schema_label = f"{schema_label},{PGN_SCHEMA}"
    return schema_label
</file>

<file path="_build_tactic_rows.py">
"""Build tactic and outcome row dicts from analysis inputs."""

from __future__ import annotations

from dataclasses import asdict

from tactix.OutcomeRow import OutcomeRow
from tactix.TacticRow import TacticRow
from tactix.TacticRowInput import TacticRowInput


def _build_tactic_rows(inputs: TacticRowInput) -> tuple[dict[str, object], dict[str, object]]:
    """Return tactic and outcome row mappings for storage."""
    tactic_row = TacticRow.from_inputs(inputs).to_row()
    outcome_row = asdict(OutcomeRow.from_inputs(inputs))
    return tactic_row, outcome_row
</file>

<file path="_clock_from_comment.py">
"""Parse clock values from PGN comments."""

from tactix._clock_to_seconds import _clock_to_seconds
from tactix._clock_token import _clock_token
from tactix._normalize_clock_parts import _normalize_clock_parts


def _clock_from_comment(comment: str) -> float | None:
    """Return clock seconds parsed from a comment, if present."""
    token = _clock_token(comment)
    if token is None:
        return None
    clock_parts = _normalize_clock_parts(token)
    if clock_parts is None:
        return None
    return _clock_to_seconds(*clock_parts)
</file>

<file path="_clock_to_seconds.py">
"""Convert clock components to total seconds."""


def _clock_to_seconds(hours: str, minutes: str, seconds: str) -> float | None:
    try:
        return float(hours) * 3600 + float(minutes) * 60 + float(seconds)
    except ValueError:
        return None
</file>

<file path="_clock_token.py">
"""Extract clock tokens from PGN comments."""

from tactix.CLK_PATTERN import CLK_PATTERN


def _clock_token(comment: str) -> str | None:
    """Return the clock token from a comment, if any."""
    match = CLK_PATTERN.search(comment or "")
    if not match:
        return None
    return match.group(1)
</file>

<file path="_coerce_fixture_rows.py">
"""Coerce fixture game rows for testing."""

from collections.abc import Callable, Iterable


def _coerce_fixture_rows(
    games: list[dict[str, object]],
    coerce_rows: Callable[[Iterable[dict[str, object]]], list[dict]] | None,
) -> list[dict[str, object]]:
    """Return coerced rows when a helper is provided."""
    if coerce_rows is None:
        return games
    return coerce_rows(games)
</file>

<file path="_collect_tables.py">
"""Collect tables for Postgres housekeeping."""

from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix._schema_tables import _schema_tables
from tactix.config import Settings
from tactix.define_db_schemas__const import ANALYSIS_SCHEMA, PGN_SCHEMA
from tactix.init_analysis_schema import init_analysis_schema
from tactix.init_pgn_schema import init_pgn_schema


def _collect_tables(conn: PgConnection, settings: Settings) -> list[str]:
    """Return table names for schemas used by the service."""
    tables: list[str] = []
    tables.extend(_schema_tables(conn, "tactix_ops", "tactix_ops"))
    if settings.postgres_analysis_enabled:
        init_analysis_schema(conn)
        tables.extend(_schema_tables(conn, ANALYSIS_SCHEMA, ANALYSIS_SCHEMA))
    if settings.postgres_pgns_enabled:
        init_pgn_schema(conn)
        tables.extend(_schema_tables(conn, PGN_SCHEMA, PGN_SCHEMA))
    return tables
</file>

<file path="_compare_move__best_line.py">
from dataclasses import dataclass
from typing import cast

import chess

from tactix._score_best_line__after_move import _score_best_line__after_move
from tactix.StockfishEngine import StockfishEngine
from tactix.utils.logger import funclogger


@dataclass(frozen=True)
class BestLineContext:
    board: chess.Board
    best_move: chess.Move | None
    user_move_uci: str
    after_cp: int
    engine: StockfishEngine
    mover_color: bool


def _resolve_best_line_context(
    context: BestLineContext | chess.Board,
    args: tuple[object, ...],
    kwargs: dict[str, object],
) -> BestLineContext:
    if isinstance(context, BestLineContext):
        return context
    values = _init_best_line_values(kwargs)
    _apply_best_line_args(values, args)
    _ensure_best_line_required(values)
    return BestLineContext(
        board=context,
        best_move=cast(chess.Move | None, values["best_move"]),
        user_move_uci=cast(str, values["user_move_uci"]),
        after_cp=cast(int, values["after_cp"]),
        engine=cast(StockfishEngine, values["engine"]),
        mover_color=cast(bool, values["mover_color"]),
    )


def _init_best_line_values(kwargs: dict[str, object]) -> dict[str, object]:
    return {
        "best_move": kwargs.get("best_move"),
        "user_move_uci": kwargs.get("user_move_uci"),
        "after_cp": kwargs.get("after_cp"),
        "engine": kwargs.get("engine"),
        "mover_color": kwargs.get("mover_color"),
    }


def _apply_best_line_args(values: dict[str, object], args: tuple[object, ...]) -> None:
    ordered_keys = ("best_move", "user_move_uci", "after_cp", "engine", "mover_color")
    values.update(dict(zip(ordered_keys, args, strict=False)))


def _ensure_best_line_required(values: dict[str, object]) -> None:
    required = ("user_move_uci", "after_cp", "engine", "mover_color")
    if any(values[key] is None for key in required):
        raise TypeError("Missing required best-line context values")


@funclogger
def _compare_move__best_line(
    context: BestLineContext | chess.Board,
    *args: object,
    **kwargs: object,
) -> int | None:
    resolved = _resolve_best_line_context(context, args, kwargs)
    if resolved.best_move is None or resolved.user_move_uci == resolved.best_move.uci():
        return None
    best_after_cp = _score_best_line__after_move(
        resolved.board,
        resolved.best_move,
        resolved.engine,
        resolved.mover_color,
    )
    if best_after_cp is None:
        return None
    return resolved.after_cp - best_after_cp
</file>

<file path="_compute_eval__discovered_attack_unclear_threshold.py">
from tactix.analyze_tactics__positions import _DISCOVERED_ATTACK_UNCLEAR_SWING_THRESHOLD
from tactix.config import Settings


def _compute_eval__discovered_attack_unclear_threshold(settings: Settings | None) -> int | None:
    del settings
    return _DISCOVERED_ATTACK_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_compute_eval__discovered_check_unclear_threshold.py">
from tactix.analyze_tactics__positions import _DISCOVERED_CHECK_UNCLEAR_SWING_THRESHOLD
from tactix.config import Settings


def _compute_eval__discovered_check_unclear_threshold(settings: Settings | None) -> int | None:
    del settings
    return _DISCOVERED_CHECK_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_compute_eval__failed_attempt_threshold.py">
"""Compute thresholds for failed attempt eval swings."""

from tactix.analyze_tactics__positions import (
    _DISCOVERED_ATTACK_FAILED_ATTEMPT_SWING_THRESHOLD,
    _DISCOVERED_CHECK_FAILED_ATTEMPT_SWING_THRESHOLD,
    _HANGING_PIECE_FAILED_ATTEMPT_SWING_THRESHOLD,
    _PIN_FAILED_ATTEMPT_SWING_THRESHOLD,
    _SKEWER_FAILED_ATTEMPT_SWING_THRESHOLD,
)
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _compute_eval__failed_attempt_threshold(
    motif: str,
    settings: Settings | None,
) -> int | None:
    """Return the failed-attempt threshold for the given motif."""
    del settings
    thresholds = {
        "pin": _PIN_FAILED_ATTEMPT_SWING_THRESHOLD,
        "skewer": _SKEWER_FAILED_ATTEMPT_SWING_THRESHOLD,
        "discovered_attack": _DISCOVERED_ATTACK_FAILED_ATTEMPT_SWING_THRESHOLD,
        "discovered_check": _DISCOVERED_CHECK_FAILED_ATTEMPT_SWING_THRESHOLD,
        "hanging_piece": _HANGING_PIECE_FAILED_ATTEMPT_SWING_THRESHOLD,
    }
    return thresholds.get(motif)
</file>

<file path="_compute_eval__fork_unclear_threshold.py">
"""Resolve the fork unclear threshold for evaluation swings."""

from tactix.analyze_tactics__positions import _FORK_UNCLEAR_SWING_THRESHOLD
from tactix.config import Settings


def _compute_eval__fork_unclear_threshold(settings: Settings | None) -> int | None:
    """Return the configured fork unclear threshold."""
    del settings
    return _FORK_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_compute_eval__hanging_piece_unclear_threshold.py">
"""Compute hanging piece unclear threshold values."""

from tactix.analyze_tactics__positions import _HANGING_PIECE_UNCLEAR_SWING_THRESHOLD
from tactix.config import Settings


def _compute_eval__hanging_piece_unclear_threshold(settings: Settings | None) -> int | None:
    """Return the eval swing threshold for hanging piece unclear results."""
    del settings
    return _HANGING_PIECE_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_compute_eval__pin_unclear_threshold.py">
"""Compute eval threshold for unclear pin outcomes."""

from tactix.analyze_tactics__positions import _PIN_UNCLEAR_SWING_THRESHOLD
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _compute_eval__pin_unclear_threshold(settings: Settings | None) -> int | None:
    """Return the eval swing threshold for unclear pins."""
    del settings
    return _PIN_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_compute_eval__skewer_unclear_threshold.py">
from tactix.analyze_tactics__positions import _SKEWER_UNCLEAR_SWING_THRESHOLD
from tactix.config import Settings


def _compute_eval__skewer_unclear_threshold(settings: Settings | None) -> int | None:
    del settings
    return _SKEWER_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_compute_severity__tactic.py">
"""Compute tactic severity scores."""

from dataclasses import dataclass
from typing import cast

from tactix._apply_fork_severity_floor import _apply_fork_severity_floor
from tactix._severity_for_result import _severity_for_result
from tactix.analyze_tactics__positions import _SEVERITY_MAX
from tactix.config import Settings
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.utils.logger import funclogger

_SEVERITY_KEYS = ("base_cp", "delta", "motif", "mate_in", "result", "settings")


@dataclass(frozen=True)
class SeverityContext:
    base_cp: int
    delta: int
    motif: str
    mate_in: int | None
    result: str
    settings: Settings | None


@dataclass(frozen=True)
class SeverityInputs:
    """Explicit inputs for building severity contexts."""

    base_cp: int
    delta: int
    motif: str
    mate_in: int | None
    result: str
    settings: Settings | None


@funclogger
def _init_severity_values(params: SeverityContext | None) -> dict[str, object]:
    values = init_legacy_values(_SEVERITY_KEYS)
    if params is None:
        return values
    values.update(
        {
            "base_cp": params.base_cp,
            "delta": params.delta,
            "motif": params.motif,
            "mate_in": params.mate_in,
            "result": params.result,
            "settings": params.settings,
        }
    )
    return values


@funclogger
def _apply_severity_base_cp(values: dict[str, object], context: int | None) -> None:
    if context is None:
        return
    if values["base_cp"] is not None:
        raise TypeError("base_cp provided multiple times")
    values["base_cp"] = context


@funclogger
def build_severity_context(inputs: SeverityInputs) -> SeverityContext:
    """Build a severity context from explicit inputs."""
    return SeverityContext(
        base_cp=inputs.base_cp,
        delta=inputs.delta,
        motif=inputs.motif,
        mate_in=inputs.mate_in,
        result=inputs.result,
        settings=inputs.settings,
    )


@funclogger
def _build_severity_context(values: dict[str, object]) -> SeverityContext:
    if values["base_cp"] is None:
        raise TypeError("base_cp, delta, motif, and result are required")
    if values["delta"] is None or values["motif"] is None or values["result"] is None:
        raise TypeError("base_cp, delta, motif, and result are required")
    base_cp = cast(int, values["base_cp"])
    delta = cast(int, values["delta"])
    motif = cast(str, values["motif"])
    mate_in = cast(int | None, values["mate_in"])
    result = cast(str, values["result"])
    settings = cast(Settings | None, values["settings"])
    inputs = SeverityInputs(
        **dict(
            zip(
                _SEVERITY_KEYS,
                (base_cp, delta, motif, mate_in, result, settings),
                strict=True,
            )
        )
    )
    return build_severity_context(inputs)


@funclogger
def _resolve_severity_context(
    context: SeverityContext | int | None,
    args: tuple[object, ...],
    params: SeverityContext | None,
    legacy: dict[str, object],
) -> SeverityContext:
    if isinstance(context, SeverityContext):
        return context
    values = _init_severity_values(params)
    _apply_severity_base_cp(values, context)
    apply_legacy_kwargs(values, _SEVERITY_KEYS, legacy)
    apply_legacy_args(values, _SEVERITY_KEYS[1:], args)
    return _build_severity_context(values)


@funclogger
def _compute_severity__tactic(
    *args: object,
    params: SeverityContext | None = None,
    **legacy: object,
) -> float:
    context = None
    remaining_args = args
    if args:
        context = cast(SeverityContext | int | None, args[0])
        remaining_args = args[1:]
    if "context" in legacy and context is None:
        context = cast(SeverityContext | int | None, legacy.pop("context"))
    resolved = _resolve_severity_context(context, remaining_args, params, legacy)
    severity = _severity_for_result(
        resolved.base_cp,
        resolved.delta,
        resolved.motif,
        resolved.mate_in,
        resolved.result,
    )
    severity = min(severity, _SEVERITY_MAX)
    return _apply_fork_severity_floor(severity, resolved.motif, resolved.settings)
</file>

<file path="_configure_engine_options.py">
from typing import Any

import chess.engine

from tactix.utils.logger import Logger

logger = Logger(__name__)


def _configure_engine_options(
    engine: chess.engine.SimpleEngine,
    applied_options: dict[str, Any],
) -> dict[str, Any]:
    if not applied_options:
        return {}
    try:
        engine.configure(applied_options)
    except chess.engine.EngineError as exc:  # pragma: no cover - engine-specific
        logger.warning("Stockfish option configuration failed: %s", exc)
    return applied_options
</file>

<file path="_connection_kwargs.py">
"""Build Postgres connection keyword arguments."""

from typing import Any

from tactix.config import Settings


def _connection_kwargs(settings: Settings) -> dict[str, Any] | None:
    if settings.postgres_dsn:
        return {"dsn": settings.postgres_dsn}
    if not settings.postgres_host or not settings.postgres_db:
        return None
    return {
        "host": settings.postgres_host,
        "port": settings.postgres_port,
        "dbname": settings.postgres_db,
        "user": settings.postgres_user,
        "password": settings.postgres_password,
        "sslmode": settings.postgres_sslmode,
        "connect_timeout": settings.postgres_connect_timeout_s,
    }
</file>

<file path="_core.pyi">
def hello_from_bin() -> str: ...
</file>

<file path="_delete_existing_analysis.py">
"""Delete existing analysis rows for a position."""

from tactix.define_db_schemas__const import ANALYSIS_SCHEMA


def _delete_existing_analysis(cur, position_id: int) -> None:
    """Delete existing tactics and outcomes for a position id."""
    cur.execute(
        f"""
        DELETE FROM {ANALYSIS_SCHEMA}.tactic_outcomes
        WHERE tactic_id IN (
            SELECT tactic_id FROM {ANALYSIS_SCHEMA}.tactics WHERE position_id = %s
        )
        """,
        (position_id,),
    )
    cur.execute(
        f"DELETE FROM {ANALYSIS_SCHEMA}.tactics WHERE position_id = %s",
        (position_id,),
    )
</file>

<file path="_disabled_raw_pgn_summary.py">
"""Helpers for disabled raw PGN summaries."""

from typing import Any


def _disabled_raw_pgn_summary() -> dict[str, Any]:
    """Return the default disabled raw PGN summary payload."""
    return {
        "status": "disabled",
        "total_rows": 0,
        "distinct_games": 0,
        "latest_ingested_at": None,
        "sources": [],
    }
</file>

<file path="_empty_pgn_metadata.py">
"""Provide an empty PGN metadata payload."""


def _empty_pgn_metadata() -> dict[str, object]:
    """Return default metadata values for missing PGN headers."""
    return {
        "user_rating": None,
        "time_control": None,
        "white_player": None,
        "black_player": None,
        "white_elo": None,
        "black_elo": None,
        "result": None,
        "event": None,
        "site": None,
        "utc_date": None,
        "utc_time": None,
        "termination": None,
        "start_timestamp_ms": None,
    }
</file>

<file path="_engine_command_available.py">
import shutil
from pathlib import Path


def _engine_command_available(command: str) -> bool:
    return bool(Path(command).exists() or shutil.which(command))
</file>

<file path="_ensure_chesscom_site_url.py">
"""Ensure chess.com PGNs use a canonical Site URL."""

from io import StringIO

import chess.pgn

from tactix.extract_game_id import extract_game_id


def _ensure_chesscom_site_url(pgn: str) -> str:
    """Return PGN text with normalized chess.com Site header."""
    game = chess.pgn.read_game(StringIO(pgn))
    if not game:
        return pgn
    site = (game.headers.get("Site") or "").lower()
    if "chess.com" in site and "chess.com/game/live/" not in site:
        game_id = extract_game_id(pgn)
        game.headers["Site"] = f"https://chess.com/game/live/{game_id}"
        exporter = chess.pgn.StringExporter(
            headers=True,
            variations=True,
            comments=True,
            columns=80,
        )
        return game.accept(exporter).strip()
    return pgn
</file>

<file path="_evaluate_engine_position.py">
"""Evaluate a position using the engine and derive metadata."""

import chess

from tactix.analyze_tactics__positions import MATE_IN_TWO
from tactix.detect_tactics__motifs import BaseTacticDetector
from tactix.StockfishEngine import StockfishEngine
from tactix.utils.logger import funclogger


@funclogger
def _evaluate_engine_position(
    board: chess.Board,
    engine: StockfishEngine,
    mover_color: bool,
    motif_board: chess.Board,
) -> tuple[chess.Move | None, str | None, int, bool, bool]:
    """Return engine analysis results and mate flags for a position."""
    engine_result = engine.analyse(board)
    best_move_obj = engine_result.best_move
    best_move = best_move_obj.uci() if best_move_obj else None
    base_cp = BaseTacticDetector.score_from_pov(engine_result.score_cp, mover_color, board.turn)
    mate_in_one = False
    mate_in_two = False
    if best_move_obj is not None:
        mate_board = motif_board.copy()
        mate_board.push(best_move_obj)
        mate_in_one = mate_board.is_checkmate()
    if engine_result.mate_in is not None and engine_result.mate_in == MATE_IN_TWO:
        mate_in_two = True
    return best_move_obj, best_move, base_cp, mate_in_one, mate_in_two
</file>

<file path="_extract_metadata_from_headers.py">
"""Extract metadata fields from PGN headers."""

from collections.abc import Mapping

from tactix._normalize_header_value import _normalize_header_value
from tactix._parse_elo import _parse_elo
from tactix._parse_utc_start_ms import _parse_utc_start_ms
from tactix._resolve_user_rating import _resolve_user_rating


def _extract_metadata_from_headers(
    headers: Mapping[str, str],
    user: str,
) -> dict[str, object]:
    time_control = _normalize_header_value(headers.get("TimeControl"))
    white = _normalize_header_value(headers.get("White", ""))
    black = _normalize_header_value(headers.get("Black", ""))
    white_elo = _parse_elo(headers.get("WhiteElo"))
    black_elo = _parse_elo(headers.get("BlackElo"))
    rating = _resolve_user_rating(user, white, black, white_elo, black_elo)
    utc_date = _normalize_header_value(headers.get("UTCDate"))
    utc_time = _normalize_header_value(headers.get("UTCTime"))
    return {
        "user_rating": rating,
        "time_control": time_control,
        "white_player": white,
        "black_player": black,
        "white_elo": white_elo,
        "black_elo": black_elo,
        "result": _normalize_header_value(headers.get("Result")),
        "event": _normalize_header_value(headers.get("Event")),
        "site": _normalize_header_value(headers.get("Site")),
        "utc_date": utc_date,
        "utc_time": utc_time,
        "termination": _normalize_header_value(headers.get("Termination")),
        "start_timestamp_ms": _parse_utc_start_ms(utc_date, utc_time),
    }
</file>

<file path="_extract_positions_fallback.py">
"""Python fallback for position extraction."""

import importlib

from tactix._extract_positions_python import _extract_positions_python
from tactix.extractor_context import ExtractorRequest


def _resolve_position_extractor() -> object | None:
    try:
        return importlib.import_module("tactix.extract_positions__pgn")
    except ImportError:  # pragma: no cover - optional dependency in some environments
        return None


def _extract_positions_fallback(
    request: ExtractorRequest,
) -> list[dict[str, object]]:
    """Extract positions using the pure-Python implementation."""
    extractor = _resolve_position_extractor()
    if extractor is None:
        extractor_fn = _extract_positions_python
    else:
        extractor_fn = getattr(extractor, "_extract_positions_python", _extract_positions_python)
    return extractor_fn(
        request.pgn,
        request.user,
        request.source,
        request.game_id,
        side_to_move_filter=request.side_to_move_filter,
    )
</file>

<file path="_extract_positions_python.py">
"""Pure-Python PGN position extractor."""

from __future__ import annotations

from tactix._build_pgn_context import _build_pgn_context
from tactix._iter_position_contexts import _iter_position_contexts
from tactix._normalize_side_filter import _normalize_side_filter
from tactix._resolve_game_context import _resolve_game_context


def _extract_positions_python(
    pgn: str,
    user: str,
    source: str,
    game_id: str | None = None,
    side_to_move_filter: str | None = None,
) -> list[dict[str, object]]:
    """Extract positions from PGN text using the Python path."""
    ctx = _build_pgn_context(pgn, user, source, game_id, side_to_move_filter)
    game_context = _resolve_game_context(ctx)
    if game_context is None:
        return []
    game, board, user_color = game_context
    side_filter = _normalize_side_filter(ctx.side_to_move_filter)
    return _iter_position_contexts(ctx, game, board, user_color, side_filter)
</file>

<file path="_extract_site_id.py">
import chess.pgn

from tactix._match_site_id import _match_site_id


def _extract_site_id(game: chess.pgn.Game | None) -> str | None:
    if not game:
        return None
    site = game.headers.get("Site", "")
    return _match_site_id(site)
</file>

<file path="_fallback_kwargs.py">
"""Build fallback dependencies for position extraction."""

import os

from tactix._extract_positions_fallback import _extract_positions_fallback
from tactix._FallbackKwargs import _FallbackKwargs
from tactix.extract_positions__pgn import _call_rust_extractor, _load_rust_extractor
from tactix.extractor_context import ExtractorDependencies


def _fallback_kwargs() -> ExtractorDependencies:
    """Return extraction dependencies used for Rust fallback."""
    kwargs: _FallbackKwargs = {
        "getenv": os.getenv,
        "load_rust_extractor": _load_rust_extractor,
        "call_rust_extractor": _call_rust_extractor,
        "extract_positions_fallback": _extract_positions_fallback,
    }
    return ExtractorDependencies(**kwargs)
</file>

<file path="_FallbackKwargs.py">
"""Legacy fallback kwargs typing for extractor dependencies."""

# pylint: disable=invalid-name

from collections.abc import Callable
from typing import TypedDict

from tactix.extractor_context import ExtractorRequest


class _FallbackKwargs(TypedDict):
    getenv: Callable[[str], str | None]
    load_rust_extractor: Callable[[], object | None]
    call_rust_extractor: Callable[[object, ExtractorRequest], list[dict[str, object]]]
    extract_positions_fallback: Callable[[ExtractorRequest], list[dict[str, object]]]
</file>

<file path="_fetch_latest_pgn_metadata.py">
"""Fetch the latest PGN metadata for a game."""

from tactix.define_db_schemas__const import PGN_SCHEMA


def _fetch_latest_pgn_metadata(
    cur,
    game_id: str,
    source: str,
) -> tuple[str | None, int]:
    """Return latest PGN hash/version for a game and source."""
    cur.execute(
        f"""
        SELECT pgn_hash, pgn_version
        FROM {PGN_SCHEMA}.raw_pgns
        WHERE game_id = %s AND source = %s
        ORDER BY pgn_version DESC
        LIMIT 1
        """,
        (game_id, source),
    )
    existing = cur.fetchone()
    if existing:
        return existing[0], int(existing[1] or 0)
    return None, 0
</file>

<file path="_fetch_raw_pgn_summary.py">
"""Fetch raw PGN summary metrics from the database."""

from collections.abc import Mapping
from typing import Any

from tactix.define_db_schemas__const import PGN_SCHEMA


def _fetch_raw_pgn_summary(cur) -> tuple[list[Mapping[str, Any]], Mapping[str, Any]]:
    """Return per-source and total raw PGN summary data."""
    cur.execute(
        f"""
        SELECT
            source,
            COUNT(*) AS total_rows,
            COUNT(DISTINCT game_id) AS distinct_games,
            MAX(ingested_at) AS latest_ingested_at
        FROM {PGN_SCHEMA}.raw_pgns
        GROUP BY source
        ORDER BY source
        """
    )
    sources = cur.fetchall()
    cur.execute(
        f"""
        SELECT
            COUNT(*) AS total_rows,
            COUNT(DISTINCT game_id) AS distinct_games,
            MAX(ingested_at) AS latest_ingested_at
        FROM {PGN_SCHEMA}.raw_pgns
        """
    )
    totals = cur.fetchone() or {}
    return sources, totals if isinstance(totals, Mapping) else {}
</file>

<file path="_filter_fixture_games.py">
"""Filter fixture PGNs by timestamp window."""

from tactix._fixture_payload import _fixture_payload
from tactix._should_include_fixture import _should_include_fixture
from tactix.extract_last_timestamp_ms import extract_last_timestamp_ms


def _filter_fixture_games(
    chunks: list[str],
    user: str,
    source: str,
    since_ms: int,
    until_ms: int | None,
) -> list[dict[str, object]]:
    games: list[dict[str, object]] = []
    for raw in chunks:
        last_ts = extract_last_timestamp_ms(raw)
        if not _should_include_fixture(last_ts, since_ms, until_ms):
            continue
        games.append(_fixture_payload(raw, user, source, last_ts))
    return games
</file>

<file path="_filter_supported_options.py">
"""Filter Stockfish options to supported keys."""

from typing import Any

import chess.engine


def _filter_supported_options(
    engine: chess.engine.SimpleEngine,
    options: dict[str, Any],
) -> dict[str, Any]:
    supported = getattr(engine, "options", {}) or {}
    return {name: value for name, value in options.items() if name in supported}
</file>

<file path="_fixture_payload.py">
"""Build fixture payload rows for PGN inputs."""

from datetime import UTC, datetime

from tactix._ensure_chesscom_site_url import _ensure_chesscom_site_url
from tactix.extract_game_id import extract_game_id


def _fixture_payload(
    pgn: str,
    user: str,
    source: str,
    last_ts: int,
) -> dict[str, object]:
    if source == "chesscom":
        pgn = _ensure_chesscom_site_url(pgn)
    return {
        "game_id": extract_game_id(pgn),
        "user": user,
        "source": source,
        "fetched_at": datetime.now(UTC),
        "pgn": pgn,
        "last_timestamp_ms": last_ts,
    }
</file>

<file path="_fork_floor_for_settings.py">
"""Resolve fork severity floor from settings."""

from tactix._is_profile_in import _is_profile_in
from tactix.analyze_tactics__positions import _SEVERITY_MAX
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _fork_floor_for_settings(settings: Settings | None) -> float | None:
    """Resolve the fork severity floor if configured or implied by profile."""
    if settings is None:
        return None
    if settings.fork_severity_floor is not None:
        return settings.fork_severity_floor
    if _is_profile_in(settings, {"bullet"}):
        return _SEVERITY_MAX
    return None
</file>

<file path="_forks_meet_threshold.py">
"""Check fork targets against thresholds."""

from __future__ import annotations

import chess

MIN_FORK_TARGETS = 2
MIN_FORK_CHECK_TARGETS = 1


def _forks_meet_threshold(forks: int, board: chess.Board) -> bool:
    """Return True when forks meet the target threshold."""
    threshold = MIN_FORK_CHECK_TARGETS if board.is_check() else MIN_FORK_TARGETS
    return forks >= threshold
</file>

<file path="_get_game_result_for_user_from_pgn_headers.py">
"""Resolve PGN game results for a specific user."""

from __future__ import annotations

from typing import TYPE_CHECKING, cast

import chess
import chess.pgn

from tactix._get_user_color_from_pgn_headers import _get_user_color_from_pgn_headers
from tactix.chess_game_result import ChessGameResult
from tactix.chess_player_color import ChessPlayerColor
from tactix.utils.logger import funclogger

if TYPE_CHECKING:
    from tactix.pgn_headers import PgnHeaders


@funclogger
def _get_game_result_for_user_from_pgn_headers(
    headers: chess.pgn.Headers | PgnHeaders,
    user: str,
) -> ChessGameResult:
    if _is_pgn_headers(headers):
        pgn_headers = cast("PgnHeaders", headers)
        return pgn_headers.result or ChessGameResult.INCOMPLETE
    chess_headers = cast(chess.pgn.Headers, headers)
    result = _resolve_result_str(chess_headers)
    if result is None:
        return ChessGameResult.INCOMPLETE
    color = _get_user_color_from_pgn_headers(chess_headers, user)
    return _resolve_game_result(result, color)


def _is_pgn_headers(headers: object) -> bool:
    return hasattr(headers, "result") and hasattr(headers, "white_player")


def _resolve_result_str(headers: chess.pgn.Headers) -> str | None:
    result = headers.get("Result")
    if result not in {"1-0", "0-1", "1/2-1/2"}:
        return None
    return result


def _resolve_game_result(result: str, color: ChessPlayerColor) -> ChessGameResult:
    try:
        return ChessGameResult.from_str(result, color)
    except ValueError:
        return ChessGameResult.INCOMPLETE
</file>

<file path="_get_user_color_from_pgn_headers.py">
"""Resolve player colors from PGN headers."""

from __future__ import annotations

from functools import singledispatch
from typing import TYPE_CHECKING

import chess
import chess.pgn

from tactix.chess_player_color import ChessPlayerColor
from tactix.utils.logger import funclogger

if TYPE_CHECKING:
    from tactix.pgn_headers import PgnHeaders


@funclogger
def _get_user_color_from_pgn_headers(
    headers: chess.pgn.Headers | PgnHeaders, user: str
) -> ChessPlayerColor:
    """Return the user's color from PGN headers."""
    white, black = _resolve_player_names(headers)
    return _resolve_user_color(white, black, user)


@singledispatch
def _resolve_player_names(headers: object) -> tuple[str, str]:
    """Resolve player names from a headers object."""
    white = getattr(headers, "white_player", None) or ""
    black = getattr(headers, "black_player", None) or ""
    return white.lower(), black.lower()


@_resolve_player_names.register
def _resolve_player_names_from_headers(headers: chess.pgn.Headers) -> tuple[str, str]:
    """Resolve player names from standard PGN headers."""
    white = headers.get("White") or ""
    black = headers.get("Black") or ""
    return white.lower(), black.lower()


def _resolve_user_color(white: str, black: str, user: str) -> ChessPlayerColor:
    """Return the player's color based on header names."""
    user_lower = user.lower()
    is_white = white == user_lower
    is_black = black == user_lower
    if not is_white and not is_black:
        raise ValueError(f"User '{user}' not found in PGN headers.")
    return ChessPlayerColor.WHITE if is_white else ChessPlayerColor.BLACK


_VULTURE_USED = (_resolve_player_names_from_headers,)
</file>

<file path="_has_discovered_attack.py">
"""Detect discovered attack motifs."""

from tactix._has_discovered_line import (
    _has_discovered_line,
    build_discovered_line_context,
)
from tactix._has_new_target import _has_new_target
from tactix.DiscoveredAttackContext import DiscoveredAttackContext
from tactix.utils.logger import funclogger


@funclogger
def _has_discovered_attack(context: DiscoveredAttackContext) -> bool:
    return _has_discovered_line(
        build_discovered_line_context(context),
        predicate=lambda square: _has_new_target(
            context.detector,
            context.board_before,
            context.board_after,
            square,
            context.opponent,
        ),
    )
</file>

<file path="_has_discovered_check.py">
"""Detect discovered check motifs."""

from tactix._has_discovered_line import (
    _has_discovered_line,
    build_discovered_line_context,
)
from tactix._is_discovered_check_slider import _is_discovered_check_slider
from tactix.DiscoveredCheckContext import DiscoveredCheckContext
from tactix.utils.logger import funclogger


@funclogger
def _has_discovered_check(context: DiscoveredCheckContext) -> bool:
    return _has_discovered_line(
        build_discovered_line_context(context),
        predicate=lambda square: _is_discovered_check_slider(
            context.board_after,
            square,
            context.king_square,
        ),
    )
</file>

<file path="_has_discovered_line.py">
"""Shared helper for discovered line evaluations."""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass

import chess

from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.utils.logger import funclogger


@dataclass(frozen=True)
class DiscoveredLineContext:
    """Inputs for detected discovered line analysis."""

    detector: BaseTacticDetector
    board_before: chess.Board
    board_after: chess.Board
    mover_color: bool
    exclude_square: chess.Square


def build_discovered_line_context(source: object) -> DiscoveredLineContext:
    """Build a DiscoveredLineContext from a richer context object."""
    return DiscoveredLineContext(
        source.detector,
        source.board_before,
        source.board_after,
        source.mover_color,
        source.exclude_square,
    )


@funclogger
def _has_discovered_line(
    context: DiscoveredLineContext,
    predicate: Callable[[chess.Square], bool],
) -> bool:
    for square, _piece in context.detector.iter_unchanged_sliders(
        context.board_before,
        context.board_after,
        context.mover_color,
        exclude_square=context.exclude_square,
    ):
        if predicate(square):
            return True
    return False
</file>

<file path="_has_game_source.py">
"""Check if a game row includes a source value."""


def _has_game_source(game_id: str, source: str) -> bool:
    return bool(game_id and source)
</file>

<file path="_has_new_target.py">
"""Check if a discovered line reveals a new target."""

from __future__ import annotations

import chess

from tactix.BaseTacticDetector import HIGH_VALUE_PIECES, BaseTacticDetector


def _has_new_target(
    _detector: BaseTacticDetector,
    board_before: chess.Board,
    board_after: chess.Board,
    square: chess.Square,
    opponent: bool,
) -> bool:
    """Return True when a new high-value target appears after the move."""
    non_king_targets = HIGH_VALUE_PIECES - {chess.KING}

    def has_non_king_target(board: chess.Board) -> bool:
        piece = board.piece_at(square)
        if piece is None or piece.color == opponent:
            return False
        for target in board.attacks(square):
            target_piece = board.piece_at(target)
            if (
                target_piece
                and target_piece.color == opponent
                and target_piece.piece_type in non_king_targets
            ):
                return True
        return False

    return has_non_king_target(board_after) and not has_non_king_target(board_before)
</file>

<file path="_has_pin_in_steps.py">
"""Detect pins along a set of steps."""

from __future__ import annotations

import chess

from tactix._is_line_tactic import _is_line_tactic
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.LineTacticContext import LineTacticContext


def _has_pin_in_steps(  # pragma: no cover
    detector: BaseTacticDetector,
    board: chess.Board,
    start: chess.Square,
    steps: tuple[int, ...],
    opponent: bool,
) -> bool:
    """Return True when any step produces a pin pattern."""
    for step in steps:
        if _is_line_tactic(LineTacticContext(detector, board, start, step, opponent, False)):
            return True
    return False


_VULTURE_USED = (_has_pin_in_steps,)
</file>

<file path="_has_skewer_in_steps.py">
"""Detect skewers along a set of steps."""

from __future__ import annotations

import chess

from tactix._is_line_tactic import _is_line_tactic
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.LineTacticContext import LineTacticContext


def _has_skewer_in_steps(
    detector: BaseTacticDetector,
    board: chess.Board,
    start: chess.Square,
    steps: tuple[int, ...],
    opponent: bool,
) -> bool:
    """Return True when any step produces a skewer pattern."""
    for step in steps:
        if _is_line_tactic(LineTacticContext(detector, board, start, step, opponent, True)):
            return True
    return False
</file>

<file path="_ignore_progress.py">
"""No-op progress callback."""


def _ignore_progress(_payload: dict[str, object]) -> None:
    """Ignore progress payloads."""
    return
</file>

<file path="_infer_hanging_or_detected_motif.py">
from collections.abc import Iterable

import chess

from tactix._is_new_hanging_piece import _is_new_hanging_piece
from tactix.analyze_tactics__positions import MOTIF_DETECTORS
from tactix.detect_tactics__motifs import BaseTacticDetector
from tactix.utils.logger import funclogger


def _capture_square_for_move(
    board: chess.Board,
    move: chess.Move,
    mover_color: bool,
) -> chess.Square:
    if board.is_en_passant(move):
        return move.to_square + (-8 if mover_color == chess.WHITE else 8)
    return move.to_square


def _resolve_checkmate_capture_motif(
    motif_board: chess.Board,
    move: chess.Move,
    mover_color: bool,
) -> str:
    capture_square = _capture_square_for_move(motif_board, move, mover_color)
    captured_piece = motif_board.piece_at(capture_square)
    if captured_piece is None:
        return "mate"
    is_high_value = BaseTacticDetector.piece_value(
        captured_piece.piece_type
    ) >= BaseTacticDetector.piece_value(chess.ROOK)
    is_undefended = not motif_board.is_attacked_by(not mover_color, capture_square)
    return "hanging_piece" if is_high_value and is_undefended else "mate"


def _resolve_capture_motif(
    motif_board: chess.Board,
    user_board: chess.Board,
    move: chess.Move,
    mover_color: bool,
) -> str | None:
    if not motif_board.is_capture(move):
        return None
    if user_board.is_checkmate():
        return _resolve_checkmate_capture_motif(motif_board, move, mover_color)
    if BaseTacticDetector.is_hanging_capture(motif_board, user_board, move, mover_color):
        return "hanging_piece"
    return None


def _resolve_initiative_hanging_piece(
    motif: str,
    motif_board: chess.Board,
    user_board: chess.Board,
    move: chess.Move,
    mover_color: bool,
) -> str | None:
    if motif != "initiative" or not _is_new_hanging_piece(motif_board, user_board, mover_color):
        return None
    attacks = user_board.attacks(move.to_square)
    has_hanging_attack = any(
        _is_hanging_attack(user_board, mover_color, square, attacks)
        for square in _iter_opponent_squares(user_board, mover_color)
    )
    return "hanging_piece" if has_hanging_attack else None


def _is_hanging_attack(
    user_board: chess.Board,
    mover_color: bool,
    square: chess.Square,
    attacks: chess.SquareSet,
) -> bool:
    if square not in attacks:
        return False
    return user_board.is_attacked_by(mover_color, square) and not user_board.is_attacked_by(
        not mover_color, square
    )


def _iter_opponent_squares(
    board: chess.Board,
    mover_color: bool,
) -> Iterable[chess.Square]:
    for square, piece in board.piece_map().items():
        if piece.color != mover_color:
            yield square


@funclogger
def _infer_hanging_or_detected_motif(
    motif_board: chess.Board,
    move: chess.Move,
    mover_color: bool,
) -> str:
    user_board = motif_board.copy()
    user_board.push(move)
    result = _resolve_capture_motif(motif_board, user_board, move, mover_color)
    if result is not None:
        return result
    if user_board.is_checkmate():
        return "mate"
    motif = MOTIF_DETECTORS.infer_motif(motif_board, move)
    return (
        _resolve_initiative_hanging_piece(
            motif,
            motif_board,
            user_board,
            move,
            mover_color,
        )
        or motif
    )
</file>

<file path="_initialize_engine.py">
"""Initialize the Stockfish engine process."""

from pathlib import Path

import chess.engine

from tactix.config import Settings
from tactix.utils.logger import Logger
from tactix.verify_stockfish_checksum import verify_stockfish_checksum

logger = Logger(__name__)


def _initialize_engine(command: str, settings: Settings) -> chess.engine.SimpleEngine | None:
    """Return a configured Stockfish engine instance or None on failure."""
    try:
        verify_stockfish_checksum(
            Path(command),
            settings.stockfish_checksum,
            mode=settings.stockfish_checksum_mode,
        )
        return chess.engine.SimpleEngine.popen_uci(command)
    except (OSError, ValueError, RuntimeError, chess.engine.EngineError) as exc:  # pragma: no cover
        logger.warning("Stockfish failed to start: %s", exc)
        return None
</file>

<file path="_insert_analysis_outcome.py">
"""Insert analysis outcome rows into Postgres."""

from tactix.define_db_schemas__const import ANALYSIS_SCHEMA
from tactix.define_outcome_insert_plan__db_store import OutcomeInsertPlan


def _insert_analysis_outcome(
    cur,
    tactic_id: int,
    outcome_plan: OutcomeInsertPlan,
) -> None:
    """Insert an analysis outcome row for a tactic."""
    cur.execute(
        f"""
        INSERT INTO {ANALYSIS_SCHEMA}.tactic_outcomes (
            tactic_id,
            result,
            user_uci,
            eval_delta
        )
        VALUES (%s, %s, %s, %s)
        """,
        (
            tactic_id,
            outcome_plan.result,
            outcome_plan.user_uci,
            outcome_plan.eval_delta,
        ),
    )
</file>

<file path="_insert_analysis_tactic.py">
"""Insert analysis tactic rows into Postgres."""

from tactix.define_db_schemas__const import ANALYSIS_SCHEMA
from tactix.define_tactic_insert_plan__db_store import TacticInsertPlan


def _insert_analysis_tactic(cur, tactic_plan: TacticInsertPlan) -> int:
    """Insert a tactic row and return its id."""
    cur.execute(
        f"""
        INSERT INTO {ANALYSIS_SCHEMA}.tactics (
            game_id,
            position_id,
            motif,
            severity,
            best_uci,
            best_san,
            explanation,
            eval_cp
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        RETURNING tactic_id
        """,
        (
            tactic_plan.game_id,
            tactic_plan.position_id,
            tactic_plan.motif,
            tactic_plan.severity,
            tactic_plan.best_uci,
            tactic_plan.best_san,
            tactic_plan.explanation,
            tactic_plan.eval_cp,
        ),
    )
    tactic_id_row = cur.fetchone()
    return int(tactic_id_row[0]) if tactic_id_row else 0
</file>

<file path="_insert_raw_pgn_row.py">
from collections.abc import Mapping

from tactix.define_db_schemas__const import PGN_SCHEMA
from tactix.PgnUpsertPlan import PgnUpsertPlan


def _insert_raw_pgn_row(
    cur,
    game_id: str,
    source: str,
    row: Mapping[str, object],
    plan: PgnUpsertPlan,
) -> None:
    cur.execute(
        f"""
        INSERT INTO {PGN_SCHEMA}.raw_pgns (
            game_id,
            source,
            player_username,
            fetched_at,
            pgn_raw,
            pgn_normalized,
            pgn_hash,
            pgn_version,
            user_rating,
            time_control,
            ingested_at,
            last_timestamp_ms,
            cursor,
            white_player,
            black_player,
            white_elo,
            black_elo,
            result,
            event,
            site,
            utc_date,
            utc_time,
            termination,
            start_timestamp_ms
        )
        VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
        )
        ON CONFLICT DO NOTHING
        """,
        (
            game_id,
            source,
            row.get("user"),
            plan.fetched_at,
            plan.pgn_text,
            plan.normalized_pgn,
            plan.pgn_hash,
            plan.pgn_version,
            plan.metadata.get("user_rating"),
            plan.metadata.get("time_control"),
            plan.ingested_at,
            plan.last_timestamp_ms,
            plan.cursor,
            plan.metadata.get("white_player"),
            plan.metadata.get("black_player"),
            plan.metadata.get("white_elo"),
            plan.metadata.get("black_elo"),
            plan.metadata.get("result"),
            plan.metadata.get("event"),
            plan.metadata.get("site"),
            plan.metadata.get("utc_date"),
            plan.metadata.get("utc_time"),
            plan.metadata.get("termination"),
            plan.metadata.get("start_timestamp_ms"),
        ),
    )
</file>

<file path="_is_discovered_check_slider.py">
"""Check whether a slider delivers a discovered check."""

from __future__ import annotations

import chess


def _is_discovered_check_slider(
    board_after: chess.Board,
    square: chess.Square,
    king_square: chess.Square,
) -> bool:
    """Return True when a slider attacks the king after a move."""
    piece = board_after.piece_at(square)
    if piece is None or piece.piece_type not in {chess.ROOK, chess.BISHOP, chess.QUEEN}:
        return False
    return board_after.is_attacked_by(piece.color, king_square)
</file>

<file path="_is_fork_piece.py">
"""Determine if a piece can deliver a fork."""

from __future__ import annotations

import chess


def _is_fork_piece(piece: chess.Piece | None) -> bool:
    """Return True when the piece type can realistically fork."""
    if piece is None:
        return False
    return piece.piece_type in {
        chess.KNIGHT,
        chess.QUEEN,
        chess.ROOK,
        chess.BISHOP,
    }
</file>

<file path="_is_line_tactic.py">
"""Detect line tactics from a board position."""

from __future__ import annotations

from collections.abc import Iterable

import chess

from tactix.LineTacticContext import LineTacticContext

BOARD_SQUARES = 64
_STEP_RULES = {
    1: (1, 0),
    -1: (1, 0),
    8: (0, 1),
    -8: (0, 1),
    7: (1, 1),
    -7: (1, 1),
    9: (1, 1),
    -9: (1, 1),
}
_LINE_PIECES_TARGET = 2


def _is_line_tactic(context: LineTacticContext) -> bool:
    """Return True when two opponent pieces satisfy a line tactic."""
    first_piece, second_piece = _find_two_opponent_pieces(
        context.board,
        context.start,
        context.step,
        context.opponent,
    )

    if first_piece is None or second_piece is None:
        return False
    first_value = context.detector.piece_value(first_piece.piece_type)
    second_value = context.detector.piece_value(second_piece.piece_type)
    if context.target_stronger:
        return first_value > second_value
    return second_value > first_value


def _step_square(square: chess.Square, step: int) -> chess.Square | None:
    next_square = square + step
    if not _is_square_in_bounds(next_square):
        return None
    if not _matches_step_rule(square, next_square, step):
        return None
    return chess.Square(next_square)


def _is_square_in_bounds(square: int) -> bool:
    return 0 <= square < BOARD_SQUARES


def _matches_step_rule(
    square: chess.Square,
    next_square: int,
    step: int,
) -> bool:
    expected = _STEP_RULES.get(step)
    if expected is None:
        return False
    expected_file, expected_rank = expected
    file_diff = abs(chess.square_file(next_square) - chess.square_file(square))
    rank_diff = abs(chess.square_rank(next_square) - chess.square_rank(square))
    return file_diff == expected_file and rank_diff == expected_rank


def _iter_line_pieces(
    board: chess.Board,
    start: chess.Square,
    step: int,
) -> Iterable[chess.Piece]:
    current = start
    while True:
        next_square = _step_square(current, step)
        if next_square is None:
            return
        current = next_square
        piece = board.piece_at(current)
        if piece is not None:
            yield piece


def _find_two_opponent_pieces(
    board: chess.Board,
    start: chess.Square,
    step: int,
    opponent: bool,
) -> tuple[chess.Piece | None, chess.Piece | None]:
    found: list[chess.Piece] = []
    for piece in _iter_line_pieces(board, start, step):
        if piece.color != opponent:
            return None, None
        found.append(piece)
        if len(found) == _LINE_PIECES_TARGET:
            return found[0], found[1]
    return None, None
</file>

<file path="_is_missed_mate.py">
"""Check whether a missed mate threshold is met."""


def _is_missed_mate(result: str, after_cp: int, threshold: int) -> bool:
    return result == "missed" and after_cp >= threshold
</file>

<file path="_is_new_hanging_piece.py">
"""Detect whether a move creates a new hanging piece."""

import chess

from tactix.utils.logger import funclogger


@funclogger
def _is_new_hanging_piece(
    board_before: chess.Board,
    board_after: chess.Board,
    mover_color: bool,
) -> bool:
    """Return True if a hanging piece appears after the move."""
    opponent = not mover_color

    def opponent_has_hanging_piece(board: chess.Board) -> bool:
        for square, piece in board.piece_map().items():
            if piece.color != opponent:
                continue
            if board.is_attacked_by(mover_color, square) and not board.is_attacked_by(
                opponent, square
            ):
                return True
        return False

    if opponent_has_hanging_piece(board_before):
        return False
    return opponent_has_hanging_piece(board_after)
</file>

<file path="_is_profile_in.py">
"""Check whether the current profile is in a set."""

from tactix._normalize_profile__settings import _normalize_profile__settings
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _is_profile_in(settings: Settings, profiles: set[str]) -> bool:
    normalized = _normalize_profile__settings(settings)
    if not normalized:
        return False
    return normalized in {entry.strip().lower() for entry in profiles}
</file>

<file path="_is_skewer_in_step.py">
"""Detect a skewer along a single step."""

import chess

from tactix._is_line_tactic import _is_line_tactic
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.line_tactic_helpers import LineTacticInputs, build_line_tactic_context
from tactix.utils.logger import funclogger


@funclogger
def _is_skewer_in_step(  # pragma: no cover
    detector: BaseTacticDetector,
    board: chess.Board,
    start: chess.Square,
    step: int,
    opponent: bool,
) -> bool:
    return _is_line_tactic(
        build_line_tactic_context(LineTacticInputs(detector, board, start, step, opponent, True))
    )


_VULTURE_USED = (_is_skewer_in_step,)
</file>

<file path="_is_swing_at_least.py">
def _is_swing_at_least(swing: int | None, threshold: int) -> bool:
    if swing is None:
        return False
    return swing >= threshold
</file>

<file path="_is_unclear_discovered_attack_candidate.py">
"""Identify discovered attack candidates for unclear outcomes."""


def _is_unclear_discovered_attack_candidate(
    motif: str,
    best_move: str,
    user_move_uci: str,
    result: str,
) -> bool:
    if motif != "discovered_attack":
        return False
    if user_move_uci == best_move:
        return False
    return result in {"missed", "unclear"}
</file>

<file path="_is_unclear_discovered_check_candidate.py">
"""Identify discovered check candidates for unclear outcomes."""


def _is_unclear_discovered_check_candidate(
    motif: str,
    best_move: str,
    user_move_uci: str,
    result: str,
) -> bool:
    if motif != "discovered_check":
        return False
    if user_move_uci == best_move:
        return False
    return result in {"missed", "unclear"}
</file>

<file path="_is_unclear_fork_candidate.py">
"""Determine if a fork is an unclear candidate."""


def _is_unclear_fork_candidate(
    motif: str,
    best_move: str,
    user_move_uci: str,
    result: str,
) -> bool:
    """Return True when a fork outcome is unclear."""
    if motif != "fork":
        return False
    if user_move_uci == best_move:
        return False
    return result in {"missed", "failed_attempt", "unclear"}
</file>

<file path="_is_unclear_hanging_piece_candidate.py">
"""Identify unclear hanging piece outcomes."""


def _is_unclear_hanging_piece_candidate(
    motif: str,
    best_move: str,
    user_move_uci: str,
    result: str,
) -> bool:
    """Return True when the hanging piece outcome can be unclear."""
    if motif != "hanging_piece":
        return False
    if user_move_uci == best_move:
        return False
    return result in {"failed_attempt", "unclear"}
</file>

<file path="_is_unclear_pin_candidate.py">
"""Identify pin candidates for unclear outcomes."""


def _is_unclear_pin_candidate(
    motif: str,
    best_move: str,
    user_move_uci: str,
    result: str,
) -> bool:
    if motif != "pin":
        return False
    if user_move_uci == best_move:
        return False
    return result in {"missed", "unclear"}
</file>

<file path="_is_unclear_skewer_candidate.py">
"""Identify skewer candidates for unclear outcomes."""


def _is_unclear_skewer_candidate(
    motif: str,
    best_move: str,
    user_move_uci: str,
    result: str,
) -> bool:
    """Return True when the skewer outcome is unclear."""
    if motif != "skewer":
        return False
    if user_move_uci == best_move:
        return False
    return result in {"missed", "failed_attempt", "unclear"}
</file>

<file path="_is_unclear_two_move_mate.py">
from tactix.analyze_tactics__positions import _MATE_IN_TWO_UNCLEAR_SWING_THRESHOLD


def _is_unclear_two_move_mate(
    result: str,
    best_move: str | None,
    user_move_uci: str,
    swing: int | None,
) -> bool:
    if result != "unclear" or best_move is None:
        return False
    if user_move_uci == best_move:
        return False
    if swing is None:
        return False
    return swing <= _MATE_IN_TWO_UNCLEAR_SWING_THRESHOLD
</file>

<file path="_iter_position_contexts.py">
"""Iterate position contexts from a parsed PGN."""

import chess
import chess.pgn

from tactix._position_from_node import PositionNodeInputs, _position_from_node
from tactix.PgnContext import PgnContext


def _iter_position_contexts(
    ctx: PgnContext,
    game: chess.pgn.Game,
    board: chess.Board,
    user_color: bool,
    side_filter: str | None,
) -> list[dict[str, object]]:
    positions: list[dict[str, object]] = []
    for node in game.mainline():
        position = _position_from_node(
            PositionNodeInputs(
                ctx=ctx,
                game=game,
                board=board,
                user_color=user_color,
                side_filter=side_filter,
                node=node,
            )
        )
        if position is not None:
            positions.append(position)
    return positions
</file>

<file path="_latest_pgn_metadata.py">
from tactix._fetch_latest_pgn_metadata import _fetch_latest_pgn_metadata


def _latest_pgn_metadata(
    cur,
    key: tuple[str, str],
    latest_cache: dict[tuple[str, str], tuple[str | None, int]],
) -> tuple[str | None, int]:
    cached = latest_cache.get(key)
    if cached is not None:
        return cached
    latest_hash, latest_version = _fetch_latest_pgn_metadata(cur, key[0], key[1])
    latest_cache[key] = (latest_hash, latest_version)
    return latest_hash, latest_version
</file>

<file path="_list_tables.py">
"""List tables from a Postgres schema."""

from psycopg2.extensions import connection as PgConnection  # noqa: N812


def _list_tables(conn: PgConnection, schema: str) -> list[str]:
    """Return table names for the provided schema."""
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = %s
            ORDER BY table_name
            """,
            (schema,),
        )
        return [row[0] for row in cur.fetchall()]
</file>

<file path="_match_site_id.py">
"""Helpers for extracting site ids from PGN headers."""

import re

SITE_PATTERNS = [
    re.compile(r"https?://lichess\.org/([A-Za-z0-9]+)"),
    re.compile(r"https?://(?:www\.)?chess\.com/game/(?:live|daily|archive|analysis)/([0-9]+)"),
    re.compile(r"https?://(?:www\.)?chess\.com/game/([0-9]+)"),
]


def _match_site_id(site: str) -> str | None:
    """Return a matched site id if available."""
    match = _match_site_patterns(site)
    if match is not None:
        return match
    return _fallback_site_id(site)


def _match_site_patterns(site: str) -> str | None:
    """Match site id using known patterns."""
    for pattern in SITE_PATTERNS:
        match = pattern.search(site)
        if match:
            return match.group(1)
    return None


def _fallback_site_id(site: str) -> str | None:
    """Fallback to a sanitized id from the site string."""
    if not site:
        return None
    sanitized = re.sub(r"[^A-Za-z0-9]+", "", site)
    if not sanitized or not _has_digits(sanitized):
        return None
    return sanitized[-16:]


def _has_digits(value: str) -> bool:
    """Return True when the value contains digits."""
    return re.search(r"\d", value) is not None
</file>

<file path="_maybe_upsert_raw_pgn_row.py">
"""Conditionally upsert a raw PGN row."""

from collections.abc import Mapping
from dataclasses import dataclass

from tactix._build_pgn_upsert_plan import _build_pgn_upsert_plan
from tactix._has_game_source import _has_game_source
from tactix._insert_raw_pgn_row import _insert_raw_pgn_row
from tactix._latest_pgn_metadata import _latest_pgn_metadata
from tactix._parse_game_source import _parse_game_source
from tactix._record_upsert_result import (
    _record_upsert_result,
)
from tactix.PgnUpsertPlan import PgnUpsertPlan


@dataclass(frozen=True)
class _RawPgnUpsertContext:
    cur: object
    key: tuple[str, str]
    row: Mapping[str, object]
    latest_meta: tuple[str | None, int]
    latest_cache: dict[tuple[str, str], tuple[str | None, int]]


def _maybe_upsert_raw_pgn_row(
    cur,
    row: Mapping[str, object],
    latest_cache: dict[tuple[str, str], tuple[str | None, int]],
) -> int:
    """Upsert a row if newer metadata is available."""
    game_id, source = _parse_game_source(row)
    if not _has_game_source(game_id, source):
        return 0
    key = (game_id, source)
    latest_hash, latest_version = _latest_pgn_metadata(cur, key, latest_cache)
    plan = _build_pgn_upsert_plan(row, latest_hash, latest_version)
    context = _RawPgnUpsertContext(
        cur=cur,
        key=key,
        row=row,
        latest_meta=(latest_hash, latest_version),
        latest_cache=latest_cache,
    )
    return _apply_pgn_upsert_plan__raw_pgn_row(context, plan)


def _apply_pgn_upsert_plan__raw_pgn_row(
    context: _RawPgnUpsertContext,
    plan: PgnUpsertPlan | None,
) -> int:
    """Return the upsert outcome for a plan."""
    # Justification: the upsert path has to handle both skip and write branches.
    if plan is None:
        context.latest_cache[context.key] = context.latest_meta
        return 0
    game_id, source = context.key
    _insert_raw_pgn_row(
        context.cur,
        game_id,
        source,
        context.row,
        plan,
    )
    return _record_upsert_result(
        context.cur,
        context.key,
        plan,
        context.latest_cache,
    )
</file>

<file path="_normalize_clock_parts.py">
"""Normalize clock tokens into full time components."""

CLOCK_PARTS_FULL = 3
CLOCK_PARTS_SHORT = 2


def _normalize_clock_parts(token: str) -> tuple[str, str, str] | None:
    parts = token.split(":")
    if len(parts) == CLOCK_PARTS_FULL:
        return parts[0], parts[1], parts[2]
    if len(parts) == CLOCK_PARTS_SHORT:
        return "0", parts[0], parts[1]
    return None
</file>

<file path="_normalize_header_value.py">
"""Normalize optional PGN header values."""


def _normalize_header_value(value: str | None) -> str | None:
    if not value:
        return None
    if value.strip() == "?":
        return None
    return value
</file>

<file path="_normalize_profile__settings.py">
"""Normalize profile values in settings."""

from tactix._resolve_profile_value__settings import _resolve_profile_value__settings
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _normalize_profile__settings(settings: Settings) -> str:
    profile_value = _resolve_profile_value__settings(settings)
    return profile_value.strip().lower()
</file>

<file path="_normalize_side_filter.py">
"""Normalize side-to-move filters."""

from tactix._position_context_helpers import logger


def _normalize_side_filter(side_to_move_filter: str | None) -> str | None:
    """Normalize a side filter value or return None."""
    if not side_to_move_filter:
        return None
    normalized = side_to_move_filter.strip().lower()
    if normalized in {"white", "black"}:
        return normalized
    logger.warning("Unknown side_to_move_filter: %s", side_to_move_filter)
    return None
</file>

<file path="_opponent_king_square.py">
"""Locate the opponent king square."""

from __future__ import annotations

import chess


def _opponent_king_square(board: chess.Board, mover_color: bool) -> chess.Square | None:
    """Return the opponent king square if present."""
    return board.king(not mover_color)
</file>

<file path="_override_mate_motif.py">
from tactix.analyze_tactics__positions import MATE_IN_ONE, MATE_IN_TWO


def _override_mate_motif(motif: str, mate_in: int | None) -> str:
    if mate_in == MATE_IN_TWO:
        return "mate"
    if mate_in == MATE_IN_ONE and motif != "hanging_piece":
        return "mate"
    return motif
</file>

<file path="_override_motif_for_missed.py">
"""Override motifs for missed tactics when appropriate."""

from tactix._should_override_motif import _should_override_motif
from tactix.utils.logger import funclogger


@funclogger
def _override_motif_for_missed(
    user_motif: str,
    best_motif: str | None,
    result: str,
) -> str:
    """Return the overridden motif for missed tactics."""
    if not _should_override_motif(user_motif, best_motif, result):
        return user_motif
    return best_motif or user_motif
</file>

<file path="_parse_elo.py">
"""Parse ELO values from PGN headers."""


def _parse_elo(raw: str | None) -> int | None:
    """Return the parsed ELO rating or None."""
    if not raw:
        return None
    try:
        return int(raw)
    except ValueError:
        return None
</file>

<file path="_parse_game_source.py">
"""Parse game source identifiers from row mappings."""

from collections.abc import Mapping


def _parse_game_source(row: Mapping[str, object]) -> tuple[str, str]:
    game_id = str(row.get("game_id") or "")
    source = str(row.get("source") or "")
    return game_id, source
</file>

<file path="_parse_optional_int.py">
"""Parse an optional integer from a string."""


def _parse_optional_int(value: str | None) -> int | None:
    if value is None:
        return None
    try:
        return int(value)
    except (TypeError, ValueError):
        return None
</file>

<file path="_parse_user_move.py">
"""Parse user move UCI strings into chess moves."""

import chess

from tactix.analyze_tactics__positions import logger
from tactix.utils.logger import funclogger


@funclogger
def _parse_user_move(board: chess.Board, user_move_uci: str, fen: str) -> chess.Move | None:
    try:
        user_move = chess.Move.from_uci(user_move_uci)
    except ValueError:
        logger.warning("Invalid UCI move %s; skipping position", user_move_uci)
        return None
    if user_move not in board.legal_moves:
        logger.warning("Illegal move %s for FEN %s", user_move_uci, fen)
        return None
    return user_move
</file>

<file path="_parse_utc_start_ms.py">
"""Parse UTC start timestamps from PGN metadata."""

from datetime import UTC, datetime


def _parse_utc_start_ms(utc_date: str | None, utc_time: str | None) -> int | None:
    """Return a UTC start timestamp in milliseconds."""
    if not utc_date or not utc_time:
        return None
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y.%m.%d %H:%M:%S"):
        try:
            dt = datetime.strptime(f"{utc_date} {utc_time}", fmt).replace(tzinfo=UTC)
            return int(dt.timestamp() * 1000)
        except ValueError:
            continue
    return None
</file>

<file path="_position_context_helpers.py">
from __future__ import annotations

from dataclasses import asdict, dataclass

import chess
import chess.pgn

from tactix._clock_from_comment import _clock_from_comment
from tactix.PgnContext import PgnContext
from tactix.PositionContext import PositionContext
from tactix.utils.logger import Logger

logger = Logger(__name__)


@dataclass(frozen=True)
class PositionContextInputs:
    ctx: PgnContext
    game: chess.pgn.Game
    board: chess.Board
    node: chess.pgn.ChildNode
    move: chess.Move
    side_to_move: str


def _get_user_color(white: str, user: str) -> bool:
    return chess.WHITE if user.lower() == white.lower() else chess.BLACK


def _should_skip_for_turn(board: chess.Board, user_color: bool) -> bool:
    return board.turn != user_color


def _should_skip_for_side(side_to_move: str, side_filter: str | None) -> bool:
    return bool(side_filter and side_to_move != side_filter)


def _is_illegal_move(board: chess.Board, move: chess.Move) -> bool:
    return move not in board.legal_moves


def _side_from_turn(turn: bool) -> str:
    return "white" if turn == chess.WHITE else "black"


def _build_position_context(
    inputs: PositionContextInputs,
) -> dict[str, object]:
    return asdict(
        PositionContext(
            game_id=inputs.ctx.game_id or inputs.game.headers.get("Site", ""),
            user=inputs.ctx.user,
            source=inputs.ctx.source,
            fen=inputs.board.fen(),
            ply=inputs.board.ply(),
            move_number=inputs.board.fullmove_number,
            side_to_move=inputs.side_to_move,
            user_to_move=True,
            uci=inputs.move.uci(),
            san=inputs.board.san(inputs.move),
            clock_seconds=_clock_from_comment(inputs.node.comment or ""),
            is_legal=True,
        )
    )
</file>

<file path="_position_from_node.py">
"""Build position dictionaries from PGN nodes."""

from dataclasses import dataclass

import chess
import chess.pgn

from tactix._position_context_helpers import (
    PositionContextInputs,
    _build_position_context,
    _is_illegal_move,
    _should_skip_for_side,
    _should_skip_for_turn,
    _side_from_turn,
    logger,
)
from tactix._push_and_none import _push_and_none
from tactix.PgnContext import PgnContext


@dataclass(frozen=True)
class PositionNodeInputs:
    """Inputs required to build a position from a node."""

    ctx: PgnContext
    game: chess.pgn.Game
    board: chess.Board
    user_color: bool
    side_filter: str | None
    node: chess.pgn.ChildNode


def _position_from_node(
    inputs: PositionNodeInputs,
) -> dict[str, object] | None:
    """Return a position dict for the given PGN node."""
    move = inputs.node.move
    if move is None:
        return None
    if _should_skip_for_turn(inputs.board, inputs.user_color):
        _push_and_none(inputs.board, move)
        return None
    side_to_move = _side_from_turn(inputs.board.turn)
    if _should_skip_for_side(side_to_move, inputs.side_filter):
        _push_and_none(inputs.board, move)
        return None
    if _is_illegal_move(inputs.board, move):
        logger.warning("Illegal move %s for FEN %s", move.uci(), inputs.board.fen())
        _push_and_none(inputs.board, move)
        return None
    position = _build_position_context(
        PositionContextInputs(
            ctx=inputs.ctx,
            game=inputs.game,
            board=inputs.board,
            node=inputs.node,
            move=move,
            side_to_move=side_to_move,
        )
    )
    inputs.board.push(move)
    return position
</file>

<file path="_prepare_position_inputs.py">
"""Prepare analysis inputs from a position row."""

import chess

from tactix.utils.logger import funclogger


@funclogger
def _prepare_position_inputs(
    position: dict[str, object],
) -> tuple[str, str, chess.Board, chess.Board, bool]:
    """Return normalized analysis inputs from a position mapping."""
    fen = str(position["fen"])
    user_move_uci = str(position["uci"])
    board = chess.Board(fen)
    motif_board = board.copy()
    mover_color = board.turn
    return fen, user_move_uci, board, motif_board, mover_color
</file>

<file path="_push_and_none.py">
"""Push a move and return None for helper flows."""

import chess


def _push_and_none(board: chess.Board, move: chess.Move) -> None:
    """Push the move on the board."""
    board.push(move)
</file>

<file path="_reclassify_failed_attempt.py">
from tactix._reclassify_motif import _reclassify_motif
from tactix._should_reclassify import _should_reclassify
from tactix.utils.logger import funclogger


@funclogger
def _reclassify_failed_attempt(result: str, delta: int, motif: str, user_motif: str) -> str:
    reclassify_motif = _reclassify_motif(motif, user_motif)
    if not _should_reclassify(result, delta, reclassify_motif):
        return result
    return "failed_attempt"
</file>

<file path="_reclassify_motif.py">
"""Reclassify motif names based on settings rules."""

from tactix.analyze_tactics__positions import _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS
from tactix.utils.logger import funclogger


@funclogger
def _reclassify_motif(motif: str, user_motif: str) -> str | None:
    if user_motif in _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS:
        return user_motif
    if motif in _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS and motif != "discovered_attack":
        return motif
    return None
</file>

<file path="_record_upsert_result.py">
"""Record upsert results into the cache."""

from tactix.PgnUpsertPlan import PgnUpsertPlan


def _record_upsert_result(
    cur,
    key: tuple[str, str],
    plan: PgnUpsertPlan,
    latest_cache: dict[tuple[str, str], tuple[str | None, int]],
) -> int:
    """Update the latest cache and return rows affected."""
    if not cur.rowcount:
        return 0
    latest_cache[key] = (plan.pgn_hash, plan.pgn_version)
    return 1
</file>

<file path="_resolve_chesscom_profile_value.py">
"""Resolve Chess.com profile values for settings."""

from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _resolve_chesscom_profile_value(settings: Settings) -> str:
    """Return the effective Chess.com profile value."""
    profile_value = settings.chesscom_profile or settings.chesscom.time_class
    if profile_value == "daily":
        return "correspondence"
    return profile_value
</file>

<file path="_resolve_dashboard_filters.py">
"""Resolve dashboard filter defaults and normalization."""

from datetime import datetime

from tactix.coerce_date_to_datetime__datetime import _coerce_date_to_datetime
from tactix.config import Settings, get_settings
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.normalize_source__source import _normalize_source


def _resolve_dashboard_filters(
    filters: DashboardQueryFilters,
) -> tuple[datetime | None, datetime | None, str | None, Settings]:
    start_datetime = _coerce_date_to_datetime(filters.start_date)
    end_datetime = _coerce_date_to_datetime(filters.end_date, end_of_day=True)
    normalized_source = _normalize_source(filters.source)
    settings = get_settings(source=normalized_source)
    return start_datetime, end_datetime, normalized_source, settings
</file>

<file path="_resolve_fixture_message.py">
"""Resolve fixture messages with defaults."""


def _resolve_fixture_message(message: str | None, default: str) -> str:
    """Return the message or a default value."""
    if message is None:
        return default
    return message
</file>

<file path="_resolve_game_context.py">
"""Resolve a parsed PGN game context for extraction."""

import chess
import chess.pgn

from tactix._position_context_helpers import _get_user_color, logger
from tactix.PgnContext import PgnContext


def _resolve_game_context(ctx: PgnContext) -> tuple[chess.pgn.Game, chess.Board, bool] | None:
    game = ctx.game
    if not game:
        logger.warning("Unable to parse PGN")
        return None
    if ctx.user not in {ctx.white, ctx.black}:
        logger.info("User %s not present in game headers; skipping", ctx.user)
        return None
    board = ctx.board
    if board is None:
        logger.warning("Unable to build board for PGN")
        return None
    user_color = _get_user_color(ctx.white, ctx.user)
    return game, board, user_color
</file>

<file path="_resolve_mate_in.py">
from tactix.analyze_tactics__positions import MATE_IN_ONE, MATE_IN_TWO


def _resolve_mate_in(mate_in_one: bool, mate_in_two: bool) -> int | None:
    if mate_in_two:
        return MATE_IN_TWO
    if mate_in_one:
        return MATE_IN_ONE
    return None


_VULTURE_USED = (_resolve_mate_in,)
</file>

<file path="_resolve_profile_value__settings.py">
"""Resolve the profile value for a settings object."""

from tactix._resolve_chesscom_profile_value import _resolve_chesscom_profile_value
from tactix.config import Settings
from tactix.utils.logger import funclogger


@funclogger
def _resolve_profile_value__settings(settings: Settings) -> str:
    """Return the resolved profile name for the given settings."""
    if settings.source == "chesscom":
        return _resolve_chesscom_profile_value(settings)
    return settings.lichess_profile or settings.rapid_perf
</file>

<file path="_resolve_user_rating.py">
"""Resolve which rating belongs to the user."""


def _resolve_user_rating(
    user: str,
    white: str | None,
    black: str | None,
    white_elo: int | None,
    black_elo: int | None,
) -> int | None:
    """Return the user's rating when present in the headers."""
    user_lower = user.lower()
    white_lower = (white or "").lower()
    black_lower = (black or "").lower()
    if white_lower == user_lower:
        return white_elo
    if black_lower == user_lower:
        return black_elo
    return None
</file>

<file path="_schema_tables.py">
"""List schema-qualified table names."""

from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix._list_tables import _list_tables


def _schema_tables(conn: PgConnection, schema: str, label: str) -> list[str]:
    """Return tables in the schema prefixed by a label."""
    return [f"{label}.{name}" for name in _list_tables(conn, schema)]
</file>

<file path="_score_after_move.py">
"""Score positions after a move with the engine."""

import chess

from tactix.detect_tactics__motifs import BaseTacticDetector
from tactix.StockfishEngine import StockfishEngine
from tactix.utils.logger import funclogger


@funclogger
def _score_after_move(board: chess.Board, engine: StockfishEngine, mover_color: bool) -> int:
    return BaseTacticDetector.score_from_pov(
        engine.analyse(board).score_cp, mover_color, board.turn
    )
</file>

<file path="_score_best_line__after_move.py">
"""Score the best line after a move is applied."""

import chess

from tactix._score_after_move import _score_after_move
from tactix.StockfishEngine import StockfishEngine
from tactix.utils.logger import funclogger


@funclogger
def _score_best_line__after_move(
    board: chess.Board,
    best_move: chess.Move | None,
    engine: StockfishEngine,
    mover_color: bool,
) -> int | None:
    """Return the evaluation score after applying the best move."""
    if best_move is None:
        return None
    best_board = board.copy()
    best_board.push(best_move)
    return _score_after_move(best_board, engine, mover_color)
</file>

<file path="_select_motif__discovered_attack_target.py">
"""Select the motif target for discovered attack overrides."""


def _select_motif__discovered_attack_target(motif: str, best_motif: str | None) -> str:
    """Return the target motif for discovered attack overrides."""
    if best_motif == "discovered_attack":
        return "discovered_attack"
    if motif == "discovered_attack":
        return "discovered_attack"
    return ""
</file>

<file path="_select_motif__discovered_check_target.py">
"""Select the motif target for discovered check overrides."""


def _select_motif__discovered_check_target(motif: str, best_motif: str | None) -> str:
    """Return the target motif for discovered check overrides."""
    if best_motif == "discovered_check":
        return "discovered_check"
    if motif == "discovered_check":
        return "discovered_check"
    return ""
</file>

<file path="_select_motif__hanging_piece_target.py">
"""Select hanging piece motifs for overrides."""


def _select_motif__hanging_piece_target(motif: str, best_motif: str | None) -> str:
    if best_motif == "hanging_piece":
        return "hanging_piece"
    if motif == "hanging_piece":
        return "hanging_piece"
    return ""
</file>

<file path="_select_motif__pin_target.py">
"""Select the motif target for pin overrides."""


def _select_motif__pin_target(motif: str, best_motif: str | None) -> str:
    """Return the target motif for pin overrides."""
    if best_motif == "pin":
        return "pin"
    if motif == "pin":
        return "pin"
    return ""
</file>

<file path="_select_motif__skewer_target.py">
"""Select the skewer target motif label."""


def _select_motif__skewer_target(motif: str, best_motif: str | None) -> str:
    if best_motif == "skewer":
        return "skewer"
    if motif == "skewer":
        return "skewer"
    return ""
</file>

<file path="_severity_for_found_tactic.py">
"""Compute severity for found tactics."""

from tactix.analyze_tactics__positions import _SEVERITY_MAX, MATE_IN_ONE, MATE_IN_TWO
from tactix.utils.logger import funclogger


@funclogger
def _severity_for_found_tactic(
    base_cp: int,
    delta: int,
    motif: str,
    mate_in: int | None,
) -> float:
    """Return a severity score for a found tactic."""
    if mate_in in {MATE_IN_ONE, MATE_IN_TWO} or motif == "mate":
        return _SEVERITY_MAX
    if motif == "pin":
        return abs(base_cp) / 100.0
    return max(abs(delta) / 100.0, 0.01)
</file>

<file path="_severity_for_nonfound_tactic.py">
from tactix.utils.logger import funclogger


@funclogger
def _severity_for_nonfound_tactic(base_cp: int, delta: int, motif: str) -> float:
    if motif == "discovered_check":
        return abs(delta) / 100.0
    return max(abs(base_cp), abs(delta)) / 100.0
</file>

<file path="_severity_for_result.py">
"""Compute severity for a tactic outcome."""

from tactix._severity_for_found_tactic import _severity_for_found_tactic
from tactix._severity_for_nonfound_tactic import _severity_for_nonfound_tactic
from tactix.utils.logger import funclogger


@funclogger
def _severity_for_result(
    base_cp: int,
    delta: int,
    motif: str,
    mate_in: int | None,
    result: str,
) -> float:
    if result == "found":
        return _severity_for_found_tactic(base_cp, delta, motif, mate_in)
    return _severity_for_nonfound_tactic(base_cp, delta, motif)
</file>

<file path="_should_include_fixture.py">
"""Determine whether a fixture game should be included."""


def _should_include_fixture(
    last_ts: int,
    since_ms: int,
    until_ms: int | None,
) -> bool:
    if since_ms and last_ts <= since_ms:
        return False
    return not (until_ms is not None and last_ts >= until_ms)
</file>

<file path="_should_mark_unclear_candidate.py">
"""Shared helper for unclear outcome detection."""

from __future__ import annotations

from collections.abc import Callable

from tactix._is_swing_at_least import _is_swing_at_least
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_candidate(
    context: BaseOutcomeContext,
    threshold: int | None,
    predicate: Callable[[str, str, str, str], bool],
) -> bool:
    """Return True when an outcome should be marked unclear."""
    if context.swing is None or threshold is None or context.best_move is None:
        return False
    if not predicate(
        context.motif,
        context.best_move,
        context.user_move_uci,
        context.result,
    ):
        return False
    return _is_swing_at_least(context.swing, threshold)
</file>

<file path="_should_mark_unclear_discovered_attack.py">
"""Determine if discovered attack should be marked unclear."""

from tactix._is_swing_at_least import (
    _is_swing_at_least,
)
from tactix._is_unclear_discovered_attack_candidate import _is_unclear_discovered_attack_candidate
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_discovered_attack(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    """Return True when a discovered attack result is unclear."""
    if context.swing is None or threshold is None or context.best_move is None:
        return False
    if not _is_unclear_discovered_attack_candidate(
        context.motif,
        context.best_move,
        context.user_move_uci,
        context.result,
    ):
        return False
    return _is_swing_at_least(context.swing, threshold)
</file>

<file path="_should_mark_unclear_discovered_check.py">
"""Determine if a discovered-check outcome should be unclear."""

from tactix._is_swing_at_least import (
    _is_swing_at_least,
)
from tactix._is_unclear_discovered_check_candidate import _is_unclear_discovered_check_candidate
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_discovered_check(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    if context.swing is None or threshold is None or context.best_move is None:
        return False
    if not _is_unclear_discovered_check_candidate(
        context.motif,
        context.best_move,
        context.user_move_uci,
        context.result,
    ):
        return False
    return _is_swing_at_least(context.swing, threshold)
</file>

<file path="_should_mark_unclear_fork.py">
from tactix._is_swing_at_least import _is_swing_at_least
from tactix._is_unclear_fork_candidate import _is_unclear_fork_candidate
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_fork(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    if context.swing is None or threshold is None or context.best_move is None:
        return False
    if not _is_unclear_fork_candidate(
        context.motif,
        context.best_move,
        context.user_move_uci,
        context.result,
    ):
        return False
    return _is_swing_at_least(context.swing, threshold)
</file>

<file path="_should_mark_unclear_hanging_piece.py">
"""Determine if a hanging piece outcome should be unclear."""

from typing import cast

from tactix._compute_eval__failed_attempt_threshold import (
    _compute_eval__failed_attempt_threshold,
)
from tactix._is_swing_at_least import _is_swing_at_least
from tactix._is_unclear_hanging_piece_candidate import _is_unclear_hanging_piece_candidate
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_hanging_piece(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    """Return True when a hanging piece result is unclear."""
    if not _has_unclear_hanging_piece_inputs(context, threshold):
        return False
    best_move = cast(str, context.best_move)
    swing = cast(int, context.swing)
    threshold_value = cast(int, threshold)
    if not _is_unclear_hanging_piece_candidate(
        context.motif,
        best_move,
        context.user_move_uci,
        context.result,
    ):
        return False
    if _fails_hanging_piece_failed_attempt(context):
        return False
    return _is_swing_at_least(swing, threshold_value)


def _has_unclear_hanging_piece_inputs(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    return all(
        (
            context.swing is not None,
            threshold is not None,
            context.best_move is not None,
        )
    )


def _fails_hanging_piece_failed_attempt(context: BaseOutcomeContext) -> bool:
    if context.result != "failed_attempt" or context.swing is None:
        return False
    failed_attempt_threshold = _compute_eval__failed_attempt_threshold(
        "hanging_piece",
        None,
    )
    return failed_attempt_threshold is not None and context.swing <= failed_attempt_threshold
</file>

<file path="_should_mark_unclear_mate_in_one.py">
from tactix.analyze_tactics__positions import _MATE_IN_ONE_UNCLEAR_SCORE_THRESHOLD, MATE_IN_ONE
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_mate_in_one(
    context: BaseOutcomeContext,
    mate_in: int | None,
) -> bool:
    after_cp = context.after_cp
    if after_cp is None:
        return False
    return all(
        (
            mate_in == MATE_IN_ONE,
            context.best_move is not None,
            context.user_move_uci != context.best_move,
            context.result in {"missed", "failed_attempt"},
            after_cp >= _MATE_IN_ONE_UNCLEAR_SCORE_THRESHOLD,
        )
    )
</file>

<file path="_should_mark_unclear_mate_in_two.py">
from tactix._is_swing_at_least import _is_swing_at_least
from tactix.analyze_tactics__positions import _MATE_IN_TWO_UNCLEAR_SWING_THRESHOLD, MATE_IN_TWO
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_mate_in_two(
    context: BaseOutcomeContext,
    mate_in: int | None,
) -> bool:
    return all(
        (
            mate_in == MATE_IN_TWO,
            context.best_move is not None,
            context.user_move_uci != context.best_move,
            context.result in {"missed", "failed_attempt", "unclear"},
            _is_swing_at_least(context.swing, _MATE_IN_TWO_UNCLEAR_SWING_THRESHOLD),
        )
    )
</file>

<file path="_should_mark_unclear_pin.py">
"""Determine if a pin outcome should be marked unclear."""

from tactix._is_swing_at_least import _is_swing_at_least
from tactix._is_unclear_pin_candidate import _is_unclear_pin_candidate
from tactix.outcome_context import BaseOutcomeContext


def _should_mark_unclear_pin(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    """Return True when a pin result is unclear."""
    if context.swing is None or threshold is None or context.best_move is None:
        return False
    if not _is_unclear_pin_candidate(
        context.motif,
        context.best_move,
        context.user_move_uci,
        context.result,
    ):
        return False
    return _is_swing_at_least(context.swing, threshold)
</file>

<file path="_should_mark_unclear_skewer.py">
"""Determine if a skewer outcome should be marked unclear."""

from tactix._is_unclear_skewer_candidate import _is_unclear_skewer_candidate
from tactix._should_mark_unclear_candidate import _should_mark_unclear_candidate
from tactix.outcome_context import BaseOutcomeContext
from tactix.utils.logger import funclogger


@funclogger
def _should_mark_unclear_skewer(
    context: BaseOutcomeContext,
    threshold: int | None,
) -> bool:
    """Return True when a skewer result is unclear."""
    return _should_mark_unclear_candidate(
        context,
        threshold,
        _is_unclear_skewer_candidate,
    )
</file>

<file path="_should_override__discovered_attack_failed_attempt.py">
"""Override checks for discovered attack failed attempts."""

from tactix.should_override_failed_attempt__tactics import _should_override_failed_attempt


def _should_override__discovered_attack_failed_attempt(
    result: str,
    swing: int | None,
    threshold: int | None,
    target_motif: str,
) -> bool:
    """Return True if a discovered attack failed attempt should override."""
    return _should_override_failed_attempt(result, swing, threshold, target_motif)
</file>

<file path="_should_override__discovered_check_failed_attempt.py">
"""Check whether to override a discovered-check failed attempt."""

from tactix.should_override_failed_attempt__tactics import _should_override_failed_attempt


def _should_override__discovered_check_failed_attempt(
    result: str,
    swing: int | None,
    threshold: int | None,
    target_motif: str,
) -> bool:
    return _should_override_failed_attempt(result, swing, threshold, target_motif)
</file>

<file path="_should_override__hanging_piece_failed_attempt.py">
from tactix.should_override_failed_attempt__tactics import _should_override_failed_attempt


def _should_override__hanging_piece_failed_attempt(
    result: str,
    swing: int | None,
    threshold: int | None,
    target_motif: str,
) -> bool:
    return _should_override_failed_attempt(result, swing, threshold, target_motif)
</file>

<file path="_should_override__pin_failed_attempt.py">
"""Check whether to override a pin failed attempt."""

from tactix.should_override_failed_attempt__tactics import _should_override_failed_attempt


def _should_override__pin_failed_attempt(
    result: str,
    swing: int | None,
    threshold: int | None,
    target_motif: str,
) -> bool:
    return _should_override_failed_attempt(result, swing, threshold, target_motif)
</file>

<file path="_should_override__skewer_failed_attempt.py">
"""Skewer-specific failed-attempt override helper."""

from tactix.should_override_failed_attempt__tactics import _should_override_failed_attempt


def _should_override__skewer_failed_attempt(
    result: str,
    swing: int | None,
    threshold: int | None,
    target_motif: str,
) -> bool:
    return _should_override_failed_attempt(result, swing, threshold, target_motif)
</file>

<file path="_should_override_motif.py">
from tactix.analyze_tactics__positions import (
    _FAILED_ATTEMPT_OVERRIDE_TARGETS,
    _MISSED_OVERRIDE_TARGETS,
    _OVERRIDEABLE_USER_MOTIFS,
)
from tactix.utils.logger import funclogger


@funclogger
def _should_override_motif(user_motif: str, best_motif: str | None, result: str) -> bool:
    if user_motif not in _OVERRIDEABLE_USER_MOTIFS:
        return False
    if result == "missed":
        return bool(best_motif in _MISSED_OVERRIDE_TARGETS)
    if result == "failed_attempt":
        return bool(best_motif in _FAILED_ATTEMPT_OVERRIDE_TARGETS)
    return False
</file>

<file path="_should_reclassify.py">
"""Determine whether a tactic result should be reclassified."""

from tactix.analyze_tactics__positions import _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS
from tactix.utils.logger import funclogger


@funclogger
def _should_reclassify(result: str, delta: int, reclassify_motif: str | None) -> bool:
    if result != "missed" or reclassify_motif is None:
        return False
    return delta > _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS[reclassify_motif]
</file>

<file path="_should_upgrade_mate_result.py">
"""Decide when mate results should be upgraded."""

from tactix._is_missed_mate import _is_missed_mate
from tactix._is_unclear_two_move_mate import _is_unclear_two_move_mate
from tactix.analyze_tactics__positions import (
    _MATE_MISSED_SCORE_MULTIPLIER,
    MATE_IN_ONE,
    MATE_IN_TWO,
)
from tactix.outcome_context import BaseOutcomeContext


def _should_upgrade_mate_result(
    ctx: BaseOutcomeContext,
    mate_in: int | None,
) -> bool:
    """Return True when a mate result should be upgraded.

    This happens when the mate was missed or unclear.
    """
    if mate_in not in {MATE_IN_ONE, MATE_IN_TWO}:
        return False
    missed_threshold = _MATE_MISSED_SCORE_MULTIPLIER * mate_in
    if ctx.after_cp is None:
        return False
    if _is_missed_mate(ctx.result, ctx.after_cp, missed_threshold):
        return True
    if mate_in == MATE_IN_TWO:
        return _is_unclear_two_move_mate(
            ctx.result,
            ctx.best_move,
            ctx.user_move_uci,
            ctx.swing,
        )
    return False
</file>

<file path="_skewer_sources.py">
"""Identify candidate skewer source squares."""

from __future__ import annotations

import chess

ORTHOGONAL_STEPS = (1, -1, 8, -8)
DIAGONAL_STEPS = (7, -7, 9, -9)
SKEWER_PIECE_STEPS = {
    chess.ROOK: ORTHOGONAL_STEPS,
    chess.BISHOP: DIAGONAL_STEPS,
    chess.QUEEN: ORTHOGONAL_STEPS + DIAGONAL_STEPS,
}


def _skewer_sources(
    board: chess.Board,
    mover_color: bool,
) -> list[tuple[chess.Square, tuple[int, ...]]]:
    """Return slider squares and steps for skewer detection."""
    sources: list[tuple[chess.Square, tuple[int, ...]]] = []
    for square, piece in board.piece_map().items():
        if piece.color != mover_color:
            continue
        steps = SKEWER_PIECE_STEPS.get(piece.piece_type)
        if steps is not None:
            sources.append((square, steps))
    return sources
</file>

<file path="_upsert_postgres_raw_pgn_rows.py">
"""Upsert raw PGN rows into Postgres."""

from collections.abc import Mapping

from tactix._maybe_upsert_raw_pgn_row import _maybe_upsert_raw_pgn_row


def _upsert_postgres_raw_pgn_rows(
    cur,
    rows: list[Mapping[str, object]],
) -> int:
    """Upsert rows and return the number inserted."""
    latest_cache: dict[tuple[str, str], tuple[str | None, int]] = {}
    inserted = 0
    for row in rows:
        inserted += _maybe_upsert_raw_pgn_row(cur, row, latest_cache)
    return inserted
</file>

<file path="airflow_daily_sync_context.py">
"""Context for triggering Airflow daily sync."""

from __future__ import annotations

from dataclasses import dataclass

from tactix.config import Settings


@dataclass(frozen=True)
class AirflowDailySyncTriggerContext:
    """Context for triggering Airflow daily sync."""

    settings: Settings
    source: str | None
    profile: str | None
    backfill_start_ms: int | None = None
    backfill_end_ms: int | None = None
    triggered_at_ms: int | None = None
</file>

<file path="airflow_settings.py">
import os
from dataclasses import dataclass


@dataclass(slots=True)
class AirflowSettings:
    """Airflow-specific configuration."""

    base_url: str = os.getenv("TACTIX_AIRFLOW_URL", "").strip()
    username: str = os.getenv("TACTIX_AIRFLOW_USERNAME", "admin").strip()
    password: str = os.getenv("TACTIX_AIRFLOW_PASSWORD", "admin").strip()
    enabled: bool = os.getenv("TACTIX_AIRFLOW_ENABLED", "0") == "1"
    api_timeout_s: int = int(os.getenv("TACTIX_AIRFLOW_TIMEOUT_S", "15"))
    poll_interval_s: int = int(os.getenv("TACTIX_AIRFLOW_POLL_INTERVAL_S", "5"))
    poll_timeout_s: int = int(os.getenv("TACTIX_AIRFLOW_POLL_TIMEOUT_S", "600"))
</file>

<file path="analyse_with_retries__pipeline.py">
"""Retry wrapper for Stockfish analysis."""

from __future__ import annotations

import time
from importlib import import_module

import chess.engine

from tactix.analyze_position import analyze_position
from tactix.config import Settings
from tactix.utils import Logger, funclogger

logger = Logger(__name__)


@funclogger
def _analyse_with_retries(
    engine,
    position: dict[str, object],
    settings: Settings,
) -> tuple[dict[str, object], dict[str, object]] | None:
    max_retries = max(settings.stockfish_max_retries, 0)
    backoff_seconds = max(settings.stockfish_retry_backoff_ms, 0) / 1000.0
    for attempt in range(max_retries + 1):
        try:
            return _analyse_position__pipeline(position, engine, settings)
        except (
            chess.engine.EngineTerminatedError,
            chess.engine.EngineError,
            OSError,
        ) as exc:
            if attempt >= max_retries:
                raise
            _handle_stockfish_retry(engine, attempt + 1, max_retries, exc, backoff_seconds)
    return None


def analyse_with_retries(
    engine,
    position: dict[str, object],
    settings: Settings,
) -> tuple[dict[str, object], dict[str, object]] | None:
    """Analyze a position with Stockfish retry logic."""
    return _analyse_with_retries(engine, position, settings)


@funclogger
def _analyse_position__pipeline(
    position: dict[str, object],
    engine,
    settings: Settings,
) -> tuple[dict[str, object], dict[str, object]] | None:
    """
    Analyzes a chess position using the provided engine and settings,
    with support for pipeline integration.

    Args:
        position (dict[str, object]): The chess position to analyze, represented as a dictionary.
        engine: The chess engine instance to use for analysis.
        settings (Settings): Configuration settings for the analysis.

    Returns:
        tuple[dict[str, object], dict[str, object]] | None:
            A tuple containing the analysis results and additional information as dictionaries,
            or None if the analysis could not be performed.
    """
    pipeline_module = import_module("tactix.pipeline")
    analyze_fn = getattr(pipeline_module, "analyze_position", analyze_position)
    return analyze_fn(position, engine, settings=settings)


@funclogger
def _handle_stockfish_retry(
    engine,
    attempt: int,
    max_retries: int,
    exc: Exception,
    backoff_seconds: float,
) -> None:
    logger.warning(
        "Stockfish error on attempt %s/%s: %s; restarting engine",
        attempt,
        max_retries,
        exc,
    )
    if hasattr(engine, "restart"):
        engine.restart()
    _sleep_with_backoff(backoff_seconds, attempt)


@funclogger
def _sleep_with_backoff(backoff_seconds: float, attempt: int) -> None:
    if backoff_seconds:
        time.sleep(backoff_seconds * (2 ** (attempt - 1)))
</file>

<file path="analysis_context.py">
"""Dataclass containers for analysis pipeline contexts."""

from __future__ import annotations

from dataclasses import dataclass

import duckdb

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.StockfishEngine import StockfishEngine


@dataclass(frozen=True)
class AnalysisRunInputs:
    """Grouped inputs for analysis runs."""

    positions: list[dict[str, object]]
    resume_index: int
    analysis_signature: str
    progress: ProgressCallback | None


class _AnalysisRunAccessors:
    run: AnalysisRunInputs

    @property
    def positions(self) -> list[dict[str, object]]:
        """Return the positions to analyze."""
        return self.run.positions

    @property
    def resume_index(self) -> int:
        """Return the resume index."""
        return self.run.resume_index

    @property
    def analysis_signature(self) -> str:
        """Return the analysis signature."""
        return self.run.analysis_signature

    @property
    def progress(self) -> ProgressCallback | None:
        """Return the progress callback."""
        return self.run.progress


@dataclass(frozen=True)
class AnalysisRunContext(_AnalysisRunAccessors):
    """Inputs for analysis runs that operate on stored positions."""

    conn: duckdb.DuckDBPyConnection
    settings: Settings
    run: AnalysisRunInputs


@dataclass(frozen=True)
class AnalysisAndMetricsContext(_AnalysisRunAccessors):
    """Inputs for analysis runs that also refresh metrics."""

    conn: duckdb.DuckDBPyConnection
    settings: Settings
    run: AnalysisRunInputs
    profile: str | None


@dataclass(frozen=True)
class AnalysisPositionMeta:
    """Position-specific metadata for analysis processing."""

    pos: dict[str, object]
    idx: int
    resume_index: int
    total_positions: int
    progress_every: int


@dataclass(frozen=True)
class AnalysisPositionPersistence:
    """Persistence-related dependencies for analysis processing."""

    analysis_checkpoint_path: object
    analysis_signature: str
    pg_conn: object
    analysis_pg_enabled: bool


@dataclass(frozen=True)
class AnalysisPositionContext:
    """Combined context for processing a single analysis position."""

    conn: duckdb.DuckDBPyConnection
    settings: Settings
    engine: StockfishEngine
    meta: AnalysisPositionMeta
    persistence: AnalysisPositionPersistence
    progress: ProgressCallback | None

    @property
    def pos(self) -> dict[str, object]:
        """Return the position payload."""
        return self.meta.pos

    @property
    def idx(self) -> int:
        """Return the position index."""
        return self.meta.idx

    @property
    def resume_index(self) -> int:
        """Return the resume index for analysis."""
        return self.meta.resume_index

    @property
    def total_positions(self) -> int:
        """Return the total positions count."""
        return self.meta.total_positions

    @property
    def progress_every(self) -> int:
        """Return the progress interval for analysis updates."""
        return self.meta.progress_every

    @property
    def analysis_checkpoint_path(self) -> object:
        """Return the analysis checkpoint path."""
        return self.persistence.analysis_checkpoint_path

    @property
    def analysis_signature(self) -> str:
        """Return the analysis signature."""
        return self.persistence.analysis_signature

    @property
    def pg_conn(self) -> object:
        """Return the Postgres connection for analysis writes."""
        return self.persistence.pg_conn

    @property
    def analysis_pg_enabled(self) -> bool:
        """Return whether Postgres analysis writes are enabled."""
        return self.persistence.analysis_pg_enabled
</file>

<file path="analysis_progress_interval__pipeline.py">
"""Compute analysis progress reporting intervals."""

from __future__ import annotations

from tactix.define_pipeline_state__pipeline import ANALYSIS_PROGRESS_BUCKETS, INDEX_OFFSET
from tactix.utils.logger import funclogger


@funclogger
def _analysis_progress_interval(total_positions: int) -> int:
    if total_positions:
        return max(1, total_positions // ANALYSIS_PROGRESS_BUCKETS)
    return INDEX_OFFSET
</file>

<file path="analysis_signature__pipeline.py">
"""Build analysis signature hashes."""

from __future__ import annotations

import json

from tactix.utils import funclogger
from tactix.utils import hash as hash_value


@funclogger
def _analysis_signature(game_ids: list[str], positions_count: int, source: str) -> str:
    """
    Generates a unique SHA-256 hash signature for a given set of game analysis parameters.

    Parameters
    ----------
    game_ids : list[str]
        A list of game IDs being analyzed.
    positions_count : int
        The total number of positions analyzed across the games.
    source : str
        The source of the analysis request.

    Returns
    -------
    str
        A SHA-256 hash signature representing the analysis parameters.
    """
    payload = {
        "game_ids": game_ids,
        "positions_count": positions_count,
        "source": source,
    }
    serialized = json.dumps(payload, sort_keys=True)
    return hash_value(serialized)
</file>

<file path="AnalysisLoopContext.py">
"""Context for running the analysis loop."""

# pylint: disable=invalid-name

from dataclasses import dataclass

import duckdb

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback


@dataclass(frozen=True)
class AnalysisLoopContext:  # pylint: disable=too-many-instance-attributes
    """Inputs required to process an analysis loop."""

    conn: duckdb.DuckDBPyConnection
    settings: Settings
    positions: list[dict[str, object]]
    resume_index: int
    analysis_checkpoint_path: object
    analysis_signature: str
    progress: ProgressCallback | None
    pg_conn: object
    analysis_pg_enabled: bool
</file>

<file path="AnalysisPrepResult.py">
from dataclasses import dataclass


@dataclass(slots=True)
class AnalysisPrepResult:
    """Outputs from preparing analysis inputs."""

    positions: list[dict[str, object]]
    resume_index: int
    analysis_signature: str
    raw_pgns_inserted: int
    raw_pgns_hashed: int
    raw_pgns_matched: int
    postgres_raw_pgns_inserted: int
</file>

<file path="analyze_position.py">
from dataclasses import dataclass

import chess

from tactix._apply_outcome_overrides import _apply_outcome_overrides
from tactix._build_tactic_rows import (
    _build_tactic_rows,
)
from tactix._compare_move__best_line import BestLineContext, _compare_move__best_line
from tactix._compute_eval__failed_attempt_threshold import (
    _compute_eval__failed_attempt_threshold,
)
from tactix._compute_eval__hanging_piece_unclear_threshold import (
    _compute_eval__hanging_piece_unclear_threshold,
)
from tactix._compute_severity__tactic import (
    SeverityContext,
    SeverityInputs,
    _compute_severity__tactic,
    build_severity_context,
)
from tactix._evaluate_engine_position import _evaluate_engine_position
from tactix._infer_hanging_or_detected_motif import _infer_hanging_or_detected_motif
from tactix._override_motif_for_missed import _override_motif_for_missed
from tactix._parse_user_move import _parse_user_move
from tactix._prepare_position_inputs import _prepare_position_inputs
from tactix._reclassify_failed_attempt import _reclassify_failed_attempt
from tactix._score_after_move import _score_after_move
from tactix.analyze_tactics__positions import _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.config import Settings
from tactix.format_tactics__explanation import format_tactic_explanation
from tactix.outcome_context import BaseOutcomeContext, OutcomeOverridesContext
from tactix.OutcomeDetails import OutcomeDetails
from tactix.StockfishEngine import StockfishEngine
from tactix.TacticDetails import TacticDetails
from tactix.TacticRowInput import TacticRowInput
from tactix.utils.logger import funclogger


@dataclass(frozen=True)
class MotifResolutionContext:
    result: str
    delta: int
    user_motif: str
    best_move_obj: object | None
    motif_board: object
    mover_color: bool


@dataclass(frozen=True)
class InitialResultContext:
    best_move: str | None
    best_move_obj: chess.Move | None
    user_move: chess.Move
    user_move_uci: str
    base_cp: int
    after_cp: int


@dataclass(frozen=True)
class PositionMotifContext:
    result: str
    delta: int
    motif_board: chess.Board
    user_move: chess.Move
    best_move_obj: chess.Move | None
    mover_color: bool


@dataclass(frozen=True)
class OutcomeOverridePositionContext:
    outcome: BaseOutcomeContext
    after_cp: int
    best_motif: str | None
    mate_in_one: bool
    mate_in_two: bool
    settings: Settings | None


def _should_mark_missed_for_initiative(
    context: MotifResolutionContext,
    result: str,
    user_motif: str,
) -> bool:
    return result == "initiative" and context.best_move_obj is None and user_motif != "unknown"


def _should_infer_best_motif(result: str, best_move_obj: object | None) -> bool:
    return result in {"missed", "failed_attempt", "unclear"} and best_move_obj is not None


def _resolve_best_motif_adjustment(
    context: MotifResolutionContext,
    result: str,
    user_motif: str,
) -> tuple[str, str, str | None]:
    best_motif = _infer_hanging_or_detected_motif(
        context.motif_board,
        context.best_move_obj,
        context.mover_color,
    )
    if (
        best_motif == "initiative"
        and result in {"missed", "failed_attempt"}
        and user_motif not in _FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS
    ):
        return "unclear", user_motif, best_motif
    return result, _override_motif_for_missed(user_motif, best_motif, result), best_motif


def _classify_hanging_piece_from_base(
    base_cp: int | None,
    delta: int,
    unclear_threshold: int,
) -> str | None:
    if base_cp is None:
        return None
    if base_cp < abs(unclear_threshold):
        return None
    after_cp = base_cp + delta
    if delta <= unclear_threshold < after_cp:
        return "failed_attempt"
    return None


@funclogger
def _resolve_motif_and_result(
    context: MotifResolutionContext,
) -> tuple[str, str, str | None]:
    result = context.result
    user_motif = context.user_motif
    motif = user_motif
    best_motif: str | None = None
    if _should_mark_missed_for_initiative(context, result, user_motif):
        result = "missed"
    if _should_infer_best_motif(result, context.best_move_obj):
        result, motif, best_motif = _resolve_best_motif_adjustment(
            context,
            result,
            user_motif,
        )
    result = _reclassify_failed_attempt(result, context.delta, motif, user_motif)
    return result, motif, best_motif


@funclogger
def _adjust_hanging_piece_result(
    result: str,
    motif: str,
    delta: int | None,
    base_cp: int | None,
    settings: Settings | None,
) -> str:
    if not _should_adjust_hanging_piece(result, motif, delta):
        return result
    adjusted = _resolve_hanging_piece_adjustment(result, delta, base_cp, settings)
    return result if adjusted is None else adjusted


@funclogger
def _should_adjust_hanging_piece(
    result: str,
    motif: str,
    delta: int | None,
) -> bool:
    return all(
        (
            motif == "hanging_piece",
            delta is not None,
            result != "found",
        )
    )


@funclogger
def _resolve_hanging_piece_adjustment(
    result: str,
    delta: int | None,
    base_cp: int | None,
    settings: Settings | None,
) -> str | None:
    if delta is None:
        return None
    failed_attempt_threshold = _compute_eval__failed_attempt_threshold(
        "hanging_piece",
        settings,
    )
    unclear_threshold = _compute_eval__hanging_piece_unclear_threshold(settings)
    if failed_attempt_threshold is None or unclear_threshold is None:
        return None
    return _classify_hanging_piece_result(
        result,
        delta,
        unclear_threshold,
        failed_attempt_threshold,
        base_cp,
    )


@funclogger
def _classify_hanging_piece_result(
    result: str,
    delta: int,
    unclear_threshold: int,
    failed_attempt_threshold: int,
    base_cp: int | None,
) -> str:
    if result == "unclear":
        return "unclear"
    classification = _classify_hanging_piece_from_base(base_cp, delta, unclear_threshold)
    if classification is not None:
        return classification
    if _is_missed_due_to_delta(base_cp, delta, unclear_threshold, failed_attempt_threshold):
        return "missed"
    return _classify_non_missed_delta(result, delta, failed_attempt_threshold)


def _is_missed_due_to_delta(
    base_cp: int | None,
    delta: int,
    unclear_threshold: int,
    failed_attempt_threshold: int,
) -> bool:
    if base_cp is not None and base_cp <= unclear_threshold and delta <= failed_attempt_threshold:
        return True
    return delta <= unclear_threshold


def _classify_non_missed_delta(
    result: str,
    delta: int,
    failed_attempt_threshold: int,
) -> str:
    if delta <= failed_attempt_threshold:
        return "failed_attempt"
    if delta < 0:
        return "failed_attempt" if result == "failed_attempt" else "unclear"
    return "unclear"


@funclogger
def analyze_position(
    position: dict[str, object],
    engine: StockfishEngine,
    settings: Settings | None = None,
) -> tuple[dict[str, object], dict[str, object]] | None:
    fen, user_move_uci, board, motif_board, mover_color = _prepare_position_inputs(position)

    best_move_obj, best_move, base_cp, mate_in_one, mate_in_two = _evaluate_engine_position(
        board, engine, mover_color, motif_board
    )

    user_move = _parse_user_move(board, user_move_uci, fen)
    if user_move is None:
        return None

    board.push(user_move)
    after_cp = _score_after_move(board, engine, mover_color)

    result, delta = _resolve_initial_result(
        InitialResultContext(
            best_move=best_move,
            best_move_obj=best_move_obj,
            user_move=user_move,
            user_move_uci=user_move_uci,
            base_cp=base_cp,
            after_cp=after_cp,
        )
    )
    result, motif, best_motif = _resolve_motif_for_position(
        PositionMotifContext(
            result=result,
            delta=delta,
            motif_board=motif_board,
            user_move=user_move,
            best_move_obj=best_move_obj,
            mover_color=mover_color,
        )
    )
    swing = _resolve_best_line_swing(
        BestLineContext(
            board=motif_board,
            best_move=best_move_obj,
            user_move_uci=user_move_uci,
            after_cp=after_cp,
            engine=engine,
            mover_color=mover_color,
        )
    )
    result, motif, mate_in = _apply_outcome_overrides_for_position(
        OutcomeOverridePositionContext(
            outcome=BaseOutcomeContext(
                result=result,
                motif=motif,
                best_move=best_move,
                user_move_uci=user_move_uci,
                swing=swing,
                after_cp=after_cp,
            ),
            after_cp=after_cp,
            best_motif=best_motif,
            mate_in_one=mate_in_one,
            mate_in_two=mate_in_two,
            settings=settings,
        )
    )
    result = _finalize_hanging_piece_result(result, motif, delta, base_cp, settings)
    severity = _compute_severity_for_position(
        build_severity_context(
            SeverityInputs(
                base_cp=base_cp,
                delta=delta,
                motif=motif,
                mate_in=mate_in,
                result=result,
                settings=settings,
            )
        )
    )
    best_san, explanation = format_tactic_explanation(fen, best_move or "", motif)
    return _build_tactic_rows(
        TacticRowInput(
            position=position,
            details=TacticDetails(
                motif=motif,
                severity=severity,
                best_move=best_move,
                base_cp=base_cp,
                mate_in=mate_in,
                best_san=best_san,
                explanation=explanation,
            ),
            outcome=OutcomeDetails(
                result=result,
                user_move_uci=user_move_uci,
                delta=delta,
            ),
        )
    )


@funclogger
def _resolve_initial_result(
    context: InitialResultContext,
) -> tuple[str, int]:
    result, delta = BaseTacticDetector.classify_result(
        context.best_move,
        context.user_move_uci,
        context.base_cp,
        context.after_cp,
    )
    if context.best_move_obj is not None and context.user_move == context.best_move_obj:
        result = "found"
    return result, delta


@funclogger
def _resolve_motif_for_position(
    context: PositionMotifContext,
) -> tuple[str, str, str | None]:
    user_motif = _infer_hanging_or_detected_motif(
        context.motif_board,
        context.user_move,
        context.mover_color,
    )
    return _resolve_motif_and_result(
        MotifResolutionContext(
            result=context.result,
            delta=context.delta,
            user_motif=user_motif,
            best_move_obj=context.best_move_obj,
            motif_board=context.motif_board,
            mover_color=context.mover_color,
        )
    )


@funclogger
def _resolve_best_line_swing(context: BestLineContext) -> int | None:
    return _compare_move__best_line(context)


@funclogger
def _apply_outcome_overrides_for_position(
    context: OutcomeOverridePositionContext,
) -> tuple[str, str, int | None]:
    return _apply_outcome_overrides(
        OutcomeOverridesContext(
            outcome=context.outcome,
            best_motif=context.best_motif,
            after_cp=context.after_cp,
            mate_in_one=context.mate_in_one,
            mate_in_two=context.mate_in_two,
            settings=context.settings,
        )
    )


@funclogger
def _finalize_hanging_piece_result(
    result: str,
    motif: str,
    delta: int | None,
    base_cp: int,
    settings: Settings | None,
) -> str:
    return _adjust_hanging_piece_result(result, motif, delta, base_cp, settings)


@funclogger
def _compute_severity_for_position(context: SeverityContext) -> int:
    return _compute_severity__tactic(context)
</file>

<file path="analyze_positions__pipeline.py">
from __future__ import annotations

from tactix.AnalysisLoopContext import AnalysisLoopContext
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import INDEX_OFFSET, RESUME_INDEX_START
from tactix.init_analysis_schema_if_needed__pipeline import _init_analysis_schema_if_needed
from tactix.maybe_sync_analysis_results__pipeline import _maybe_sync_analysis_results
from tactix.ops_event import OpsEvent
from tactix.postgres_analysis_enabled import postgres_analysis_enabled
from tactix.postgres_connection import postgres_connection
from tactix.record_ops_event import record_ops_event
from tactix.run_analysis_loop__pipeline import _run_analysis_loop
from tactix.utils.logger import funclogger


@funclogger
def _analyze_positions(
    conn,
    settings: Settings,
    positions: list[dict[str, object]],
) -> tuple[int, int]:
    if not positions:
        return 0, 0
    analysis_pg_enabled = postgres_analysis_enabled(settings)
    with postgres_connection(settings) as pg_conn:
        _init_analysis_schema_if_needed(pg_conn, analysis_pg_enabled)
        tactics_detected, postgres_written = _run_analysis_loop(
            AnalysisLoopContext(
                conn=conn,
                settings=settings,
                positions=positions,
                resume_index=RESUME_INDEX_START - INDEX_OFFSET,
                analysis_checkpoint_path=None,
                analysis_signature="",
                progress=None,
                pg_conn=pg_conn,
                analysis_pg_enabled=analysis_pg_enabled,
            )
        )
        postgres_synced, postgres_written = _maybe_sync_analysis_results(
            conn,
            settings,
            pg_conn,
            analysis_pg_enabled,
            postgres_written,
        )
    positions_analyzed = len(positions)
    record_ops_event(
        OpsEvent(
            settings=settings,
            component="analysis",
            event_type="analysis_complete",
            source=settings.source,
            profile=settings.lichess_profile or settings.chesscom.profile,
            metadata={
                "positions_analyzed": positions_analyzed,
                "tactics_detected": tactics_detected,
                "postgres_tactics_written": postgres_written,
                "postgres_tactics_synced": postgres_synced,
            },
        )
    )
    return positions_analyzed, tactics_detected
</file>

<file path="analyze_positions_with_progress__pipeline.py">
"""Run analysis loop with progress reporting and Postgres sync."""

from __future__ import annotations

from tactix.analysis_context import AnalysisRunContext
from tactix.AnalysisLoopContext import AnalysisLoopContext
from tactix.app.use_cases.pipeline_support import _maybe_clear_analysis_checkpoint
from tactix.init_analysis_schema_if_needed__pipeline import _init_analysis_schema_if_needed
from tactix.maybe_sync_analysis_results__pipeline import _maybe_sync_analysis_results
from tactix.postgres_analysis_enabled import postgres_analysis_enabled
from tactix.postgres_connection import postgres_connection
from tactix.run_analysis_loop__pipeline import _run_analysis_loop
from tactix.utils.logger import funclogger


@funclogger
def _analyze_positions_with_progress(
    ctx: AnalysisRunContext,
) -> tuple[int, int, int]:
    analysis_pg_enabled = postgres_analysis_enabled(ctx.settings)
    with postgres_connection(ctx.settings) as pg_conn:
        _init_analysis_schema_if_needed(pg_conn, analysis_pg_enabled)
        tactics_count, postgres_written = _run_analysis_loop(
            AnalysisLoopContext(
                conn=ctx.conn,
                settings=ctx.settings,
                positions=ctx.positions,
                resume_index=ctx.resume_index,
                analysis_checkpoint_path=ctx.settings.analysis_checkpoint_path,
                analysis_signature=ctx.analysis_signature,
                progress=ctx.progress,
                pg_conn=pg_conn,
                analysis_pg_enabled=analysis_pg_enabled,
            )
        )
        postgres_synced, postgres_written = _maybe_sync_analysis_results(
            ctx.conn,
            ctx.settings,
            pg_conn,
            analysis_pg_enabled,
            postgres_written,
        )
    _maybe_clear_analysis_checkpoint(ctx.settings.analysis_checkpoint_path)
    return tactics_count, postgres_written, postgres_synced
</file>

<file path="analyze_positions.py">
"""Batch analysis helpers for chess positions."""

from collections.abc import Iterable

from tactix import analyze_tactics__positions as tactics_impl
from tactix.analyze_position import analyze_position
from tactix.config import Settings


def analyze_positions(
    positions: Iterable[dict[str, object]],
    settings: Settings,
) -> tuple[list[dict[str, object]], list[dict[str, object]]]:
    """Analyze a batch of positions and return tactics/outcomes rows."""
    tactics_rows: list[dict[str, object]] = []
    outcomes_rows: list[dict[str, object]] = []

    with tactics_impl.StockfishEngine(settings) as engine:
        for pos in positions:
            result = analyze_position(pos, engine, settings=settings)
            if result is None:
                continue
            tactic_row, outcome_row = result
            tactics_rows.append(tactic_row)
            outcomes_rows.append(outcome_row)
    return tactics_rows, outcomes_rows
</file>

<file path="analyze_tactics__positions.py">
from __future__ import annotations

from tactix.config import Settings
from tactix.detect_tactics__motifs import (
    MotifDetectorSuite,
    build_default_motif_detector_suite,
)
from tactix.StockfishEngine import StockfishEngine
from tactix.utils import Logger

logger = Logger(__name__)


MOTIF_DETECTORS: MotifDetectorSuite = build_default_motif_detector_suite()
_FAILED_ATTEMPT_RECLASSIFY_THRESHOLDS = {
    "discovered_attack": -1200,
    "discovered_check": -950,
    "fork": -500,
    "skewer": -700,
}
_PIN_FAILED_ATTEMPT_SWING_THRESHOLD = -50
_SKEWER_FAILED_ATTEMPT_SWING_THRESHOLD = -50
_DISCOVERED_ATTACK_FAILED_ATTEMPT_SWING_THRESHOLD = -50
_DISCOVERED_CHECK_FAILED_ATTEMPT_SWING_THRESHOLD = -50
_HANGING_PIECE_FAILED_ATTEMPT_SWING_THRESHOLD = -50
_PIN_UNCLEAR_SWING_THRESHOLD = -300
_FORK_UNCLEAR_SWING_THRESHOLD = -300
_SKEWER_UNCLEAR_SWING_THRESHOLD = -300
_DISCOVERED_ATTACK_UNCLEAR_SWING_THRESHOLD = -300
_DISCOVERED_CHECK_UNCLEAR_SWING_THRESHOLD = -300
_HANGING_PIECE_UNCLEAR_SWING_THRESHOLD = -300
_MATE_MISSED_SCORE_MULTIPLIER = 200
_SEVERITY_MIN = 1.0
_SEVERITY_MAX = 1.5
_OVERRIDEABLE_USER_MOTIFS = {"initiative", "capture", "check", "escape"}
_MISSED_OVERRIDE_TARGETS = {
    "mate",
    "fork",
    "pin",
    "skewer",
    "discovered_attack",
    "discovered_check",
    "hanging_piece",
}
_FAILED_ATTEMPT_OVERRIDE_TARGETS = {
    "mate",
    "fork",
    "pin",
    "skewer",
    "discovered_attack",
    "discovered_check",
    "hanging_piece",
}
MATE_IN_ONE = 1
MATE_IN_TWO = 2
_MATE_IN_ONE_UNCLEAR_SCORE_THRESHOLD = _MATE_MISSED_SCORE_MULTIPLIER * MATE_IN_ONE + 50
_MATE_IN_TWO_UNCLEAR_SWING_THRESHOLD = -50


def _compute_eval__swing_threshold(
    motif: str,
    settings: Settings | None,
) -> int | None:
    del settings
    thresholds = {
        "pin": _PIN_FAILED_ATTEMPT_SWING_THRESHOLD,
        "skewer": _SKEWER_FAILED_ATTEMPT_SWING_THRESHOLD,
        "discovered_attack": _DISCOVERED_ATTACK_FAILED_ATTEMPT_SWING_THRESHOLD,
        "discovered_check": _DISCOVERED_CHECK_FAILED_ATTEMPT_SWING_THRESHOLD,
        "hanging_piece": _HANGING_PIECE_FAILED_ATTEMPT_SWING_THRESHOLD,
    }
    return thresholds.get(motif)


__all__ = [
    "MATE_IN_ONE",
    "MATE_IN_TWO",
    "MOTIF_DETECTORS",
    "StockfishEngine",
    "_compute_eval__swing_threshold",
]
</file>

<file path="analyzer.py">
"""Simple tactics analyzer wrapper."""

from __future__ import annotations

from dataclasses import dataclass

from tactix.StockfishEngine import StockfishEngine


@dataclass
class TacticsAnalyzer:
    """Analyzer for Tactics data."""

    engine: StockfishEngine

    def analyze(self, data):
        """Analyze the given data."""
</file>

<file path="api.py">
"""FastAPI application wiring and route registration."""

from __future__ import annotations

from typing import Any, cast

from fastapi import Depends, FastAPI
from starlette.middleware import Middleware
from starlette.middleware.cors import CORSMiddleware

from tactix.apply_airflow_optional_conf__airflow_jobs import _apply_airflow_optional_conf
from tactix.build_airflow_conf__airflow_jobs import _airflow_conf
from tactix.build_dashboard_cache_key__api_cache import _dashboard_cache_key
from tactix.check_airflow_enabled__airflow_settings import _airflow_enabled
from tactix.clear_dashboard_cache__api_cache import _clear_dashboard_cache
from tactix.coerce_backfill_end_ms__airflow_jobs import _coerce_backfill_end_ms
from tactix.coerce_cache_value__api_cache import _cache_value
from tactix.coerce_date_cache_value__api_cache import _date_cache_value
from tactix.coerce_date_to_datetime__datetime import _coerce_date_to_datetime
from tactix.dashboard_cache_state__api_cache import (
    _DASHBOARD_CACHE,
    _DASHBOARD_CACHE_MAX_ENTRIES,
    _DASHBOARD_CACHE_TTL_S,
)
from tactix.ensure_airflow_success__airflow_jobs import _ensure_airflow_success
from tactix.extract_api_token__request_auth import _extract_api_token
from tactix.format_sse__api_streaming import _format_sse
from tactix.get_airflow_run_id__airflow_response import _airflow_run_id
from tactix.get_airflow_state__airflow_jobs import _airflow_state
from tactix.get_auth_token__api import auth_token
from tactix.get_cached_dashboard_payload__api_cache import _get_cached_dashboard_payload
from tactix.get_dashboard__api import get_dashboard as dashboard
from tactix.get_dashboard_summary__api import dashboard_summary
from tactix.get_game_detail__api import game_detail
from tactix.get_health__api import health
from tactix.get_job_status__api_jobs import get_job_status
from tactix.get_postgres_analysis__api import postgres_analysis
from tactix.get_postgres_raw_pgns__api import postgres_raw_pgns
from tactix.get_postgres_status__api import postgres_status
from tactix.get_practice_next__api import practice_next
from tactix.get_practice_queue__api import practice_queue
from tactix.get_raw_pgns_summary__api import raw_pgns_summary
from tactix.get_tactics_search__api import tactics_search
from tactix.job_stream import (
    _event_stream,
    _queue_backfill_window,
    _queue_progress,
    _run_airflow_daily_sync_job,
    _run_stream_job,
    _stream_job_worker,
    _wait_for_airflow_run,
    stream_job_by_id,
    stream_jobs,
    stream_metrics,
)
from tactix.list_sources_for_cache_refresh__api_cache import _sources_for_cache_refresh
from tactix.manage_lifespan__fastapi import lifespan
from tactix.models import PracticeAttemptRequest
from tactix.motif_stats import motif_stats
from tactix.normalize_source__source import _normalize_source
from tactix.post_practice_attempt__api import practice_attempt
from tactix.prime_dashboard_cache__api_cache import _prime_dashboard_cache
from tactix.raise_unsupported_job__api_jobs import _raise_unsupported_job
from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async
from tactix.require_api_token__request_auth import require_api_token
from tactix.resolve_backfill_end_ms__airflow_jobs import _resolve_backfill_end_ms
from tactix.run_pipeline__api import run_pipeline
from tactix.set_dashboard_cache__api_cache import _set_dashboard_cache
from tactix.trend_stats import trend_stats
from tactix.trigger_airflow_daily_sync__airflow_jobs import _trigger_airflow_daily_sync
from tactix.trigger_daily_sync__api_jobs import trigger_daily_sync
from tactix.trigger_job__api_jobs import trigger_job
from tactix.trigger_migrations__api_jobs import trigger_migrations
from tactix.trigger_refresh_metrics__api_jobs import trigger_refresh_metrics
from tactix.validate_backfill_window__airflow_jobs import _validate_backfill_window

app = FastAPI(
    title="TACTIX",
    version="0.1.0",
    dependencies=[Depends(require_api_token)],
    lifespan=lifespan,
    middleware=[
        Middleware(
            cast(Any, CORSMiddleware),
            allow_origins=["*"],
            allow_methods=["*"],
            allow_headers=["*"],
        )
    ],
)

app.get("/api/health")(health)
app.get("/api/auth/token")(auth_token)
app.get("/api/postgres/status")(postgres_status)
app.get("/api/postgres/analysis")(postgres_analysis)
app.get("/api/postgres/raw_pgns")(postgres_raw_pgns)
app.post("/api/jobs/daily_game_sync")(trigger_daily_sync)
app.post("/api/jobs/refresh_metrics")(trigger_refresh_metrics)
app.post("/api/jobs/migrations")(trigger_migrations)
app.post("/api/jobs/trigger")(trigger_job)
app.post("/api/pipeline/run")(run_pipeline)
app.get("/api/dashboard")(dashboard)
app.get("/api/dashboard/summary")(dashboard_summary)
app.get("/api/practice/queue")(practice_queue)
app.get("/api/practice/next")(practice_next)
app.get("/api/raw_pgns/summary")(raw_pgns_summary)
app.get("/api/tactics/search")(tactics_search)
app.get("/api/games/{game_id}")(game_detail)
app.post("/api/practice/attempt")(practice_attempt)
app.get("/api/jobs/stream")(stream_jobs)
app.get("/api/jobs/{job_id}/stream")(stream_job_by_id)
app.get("/api/jobs/{job_id}")(get_job_status)
app.get("/api/metrics/stream")(stream_metrics)
app.get("/api/stats/motifs")(motif_stats)
app.get("/api/stats/trends")(trend_stats)

__all__ = [
    "_DASHBOARD_CACHE",
    "_DASHBOARD_CACHE_MAX_ENTRIES",
    "_DASHBOARD_CACHE_TTL_S",
    "PracticeAttemptRequest",
    "_airflow_conf",
    "_airflow_enabled",
    "_airflow_run_id",
    "_airflow_state",
    "_apply_airflow_optional_conf",
    "_cache_value",
    "_clear_dashboard_cache",
    "_coerce_backfill_end_ms",
    "_coerce_date_to_datetime",
    "_dashboard_cache_key",
    "_date_cache_value",
    "_ensure_airflow_success",
    "_event_stream",
    "_extract_api_token",
    "_format_sse",
    "_get_cached_dashboard_payload",
    "_normalize_source",
    "_prime_dashboard_cache",
    "_queue_backfill_window",
    "_queue_progress",
    "_raise_unsupported_job",
    "_refresh_dashboard_cache_async",
    "_resolve_backfill_end_ms",
    "_run_airflow_daily_sync_job",
    "_run_stream_job",
    "_set_dashboard_cache",
    "_sources_for_cache_refresh",
    "_stream_job_worker",
    "_trigger_airflow_daily_sync",
    "_validate_backfill_window",
    "_wait_for_airflow_run",
    "app",
    "auth_token",
    "dashboard",
    "dashboard_summary",
    "game_detail",
    "get_job_status",
    "health",
    "lifespan",
    "motif_stats",
    "postgres_analysis",
    "postgres_raw_pgns",
    "postgres_status",
    "practice_attempt",
    "practice_next",
    "practice_queue",
    "raw_pgns_summary",
    "require_api_token",
    "run_pipeline",
    "stream_job_by_id",
    "stream_jobs",
    "stream_metrics",
    "tactics_search",
    "trend_stats",
    "trigger_daily_sync",
    "trigger_job",
    "trigger_migrations",
    "trigger_refresh_metrics",
]
</file>

<file path="apply_airflow_optional_conf__airflow_jobs.py">
from __future__ import annotations


def _apply_airflow_optional_conf(conf: dict[str, object], key: str, value: int | None) -> None:
    if value is not None:
        conf[key] = value
</file>

<file path="apply_backfill_filter__pipeline.py">
from __future__ import annotations

from tactix.filter_backfill_games__pipeline import _filter_backfill_games
from tactix.GameRow import GameRow


def _apply_backfill_filter(
    conn,
    games: list[GameRow],
    backfill_mode: bool,
    source: str,
) -> tuple[list[GameRow], list[GameRow]]:
    if not backfill_mode:
        return games, []
    return _filter_backfill_games(conn, games, source)
</file>

<file path="apply_env_user_overrides__config.py">
from __future__ import annotations

import os

from tactix.define_settings__config import Settings


def _apply_env_user_overrides(settings: Settings) -> None:
    lichess_username = os.getenv("LICHESS_USERNAME") or os.getenv("LICHESS_USER")
    if lichess_username:
        settings.lichess_user = lichess_username
        if not os.getenv("TACTIX_USER"):
            settings.user = lichess_username
    chesscom_username = os.getenv("CHESSCOM_USERNAME")
    if chesscom_username:
        settings.chesscom_user = chesscom_username
</file>

<file path="apply_settings_aliases__config.py">
"""Apply settings aliases in configuration helpers."""

from __future__ import annotations

from typing import TYPE_CHECKING

from tactix.define_config_defaults__config import _MISSING, _SETTINGS_ALIAS_FIELDS

if TYPE_CHECKING:
    from tactix.define_settings__config import Settings


def _apply_settings_aliases(settings: Settings, kwargs: dict[str, object]) -> None:
    for alias in _SETTINGS_ALIAS_FIELDS:
        value = kwargs.pop(alias, _MISSING)
        if value is not _MISSING:
            setattr(settings, alias, value)
</file>

<file path="attach_position_ids__pipeline.py">
"""Attach position ids to extracted positions."""

from __future__ import annotations


def _attach_position_ids(
    positions: list[dict[str, object]],
    position_ids: list[int],
) -> None:
    """Mutate positions to include their corresponding ids."""
    for pos, pos_id in zip(positions, position_ids, strict=False):
        pos["position_id"] = pos_id
</file>

<file path="BaseTacticDetector.py">
"""Base helpers for tactic detectors."""

from __future__ import annotations

from collections.abc import Iterable

import chess
from pydantic import BaseModel

PIECE_VALUES = {
    chess.KING: 10000,
    chess.QUEEN: 900,
    chess.ROOK: 500,
    chess.BISHOP: 300,
    chess.KNIGHT: 300,
    chess.PAWN: 100,
}

MISSED_DELTA_THRESHOLD = -300
FAILED_ATTEMPT_THRESHOLD = -100

HIGH_VALUE_PIECES = {
    chess.QUEEN,
    chess.ROOK,
    chess.BISHOP,
    chess.KNIGHT,
    chess.KING,
}

SLIDER_PIECES = {
    chess.ROOK,
    chess.BISHOP,
    chess.QUEEN,
}


def _capture_square_for_move(
    board: chess.Board,
    move: chess.Move,
    mover_color: bool,
) -> chess.Square:
    if board.is_en_passant(move):
        return move.to_square + (-8 if mover_color == chess.WHITE else 8)
    return move.to_square


def _classify_no_best_move(delta: int) -> str:
    if delta <= MISSED_DELTA_THRESHOLD:
        return "missed"
    if delta <= FAILED_ATTEMPT_THRESHOLD:
        return "failed_attempt"
    return "initiative"


def _classify_with_best_move(user_move_uci: str, best_move: str, delta: int) -> str:
    if user_move_uci == best_move:
        return "found"
    if delta <= MISSED_DELTA_THRESHOLD:
        return "missed"
    if delta <= FAILED_ATTEMPT_THRESHOLD:
        return "failed_attempt"
    return "unclear"


def _iter_opponent_targets(
    board: chess.Board,
    square: chess.Square,
    mover_color: bool,
) -> Iterable[chess.Piece]:
    for target in board.attacks(square):
        target_piece = board.piece_at(target)
        if target_piece and target_piece.color != mover_color:
            yield target_piece


def _is_unchanged_slider(
    board_before: chess.Board,
    square: chess.Square,
    piece: chess.Piece,
) -> bool:
    before_piece = board_before.piece_at(square)
    if before_piece is None:
        return False
    if before_piece.piece_type != piece.piece_type:
        return False
    return before_piece.color == piece.color


def _is_slider_candidate(
    piece: chess.Piece,
    mover_color: bool,
    square: chess.Square,
    exclude_square: chess.Square | None,
) -> bool:
    if piece.color != mover_color or piece.piece_type not in SLIDER_PIECES:
        return False
    if exclude_square is None:
        return True
    return square != exclude_square


def _iter_unchanged_sliders(
    board_before: chess.Board,
    board_after: chess.Board,
    mover_color: bool,
    exclude_square: chess.Square | None,
) -> Iterable[tuple[chess.Square, chess.Piece]]:
    for square, piece in board_after.piece_map().items():
        if not _is_slider_candidate(piece, mover_color, square, exclude_square):
            continue
        if _is_unchanged_slider(board_before, square, piece):
            yield square, piece


def _can_compare_capture(
    mover_piece: chess.Piece | None,
    board_after: chess.Board,
) -> bool:
    if mover_piece is None:
        return False
    return not board_after.is_checkmate()


def _is_favorable_trade(captured_piece: chess.Piece, mover_piece: chess.Piece) -> bool:
    return PIECE_VALUES.get(captured_piece.piece_type, 0) > PIECE_VALUES.get(
        mover_piece.piece_type, 0
    )


class PieceInfo(BaseModel):
    """Describe a piece on the board."""

    square: chess.Square
    piece: chess.Piece


class SliderInfo(PieceInfo):
    """Describe a stationary slider piece."""


class BaseTacticDetector:
    """Base class providing shared tactic helper logic."""

    motif = "unknown"

    def detect(self, _context: object) -> bool:
        """Return True when the tactic is detected."""
        raise NotImplementedError

    @staticmethod
    def piece_value(piece_type: int) -> int:
        """Return the material value for a piece type."""
        return PIECE_VALUES.get(piece_type, 0)

    @staticmethod
    def score_from_pov(score_cp: int, mover_color: bool, board_turn: bool) -> int:
        """Normalize a score to the mover's perspective."""
        return score_cp if mover_color == board_turn else -score_cp

    @classmethod
    def classify_result(
        cls,
        best_move: str | None,
        user_move_uci: str,
        base_cp: int,
        after_cp: int,
    ) -> tuple[str, int]:
        """Classify the tactic result based on evaluation swing."""
        delta = after_cp - base_cp
        if best_move is None:
            return _classify_no_best_move(delta), delta
        return _classify_with_best_move(user_move_uci, best_move, delta), delta

    @classmethod
    def has_hanging_piece(cls, board: chess.Board, mover_color: bool) -> bool:
        """Return True when the mover has a hanging piece."""
        opponent = not mover_color
        for square, piece in board.piece_map().items():
            if piece.color != mover_color:
                continue
            if board.is_attacked_by(opponent, square) and not board.is_attacked_by(
                mover_color, square
            ):
                return True
        return False

    @classmethod
    def is_hanging_capture(
        cls,
        board_before: chess.Board,
        board_after: chess.Board,
        move: chess.Move,
        mover_color: bool,
    ) -> bool:
        """Return True when a capture removes a hanging piece."""
        if not board_before.is_capture(move):
            return False
        capture_square = _capture_square_for_move(board_before, move, mover_color)
        captured_piece = board_before.piece_at(capture_square)
        if captured_piece is None:
            return False
        if not board_before.is_attacked_by(not mover_color, capture_square):
            return True
        mover_piece = board_before.piece_at(move.from_square)
        if not _can_compare_capture(mover_piece, board_after):
            return False
        return _is_favorable_trade(captured_piece, mover_piece)

    @classmethod
    def count_high_value_targets(
        cls,
        board: chess.Board,
        square: chess.Square,
        mover_color: bool,
    ) -> int:
        """Count high-value opponent targets attacked from a square."""
        piece = board.piece_at(square)
        if piece is None or piece.color != mover_color:
            return 0
        return sum(
            1
            for target_piece in _iter_opponent_targets(board, square, mover_color)
            if target_piece.piece_type in HIGH_VALUE_PIECES
        )

    @classmethod
    def has_high_value_target(
        cls,
        board: chess.Board,
        square: chess.Square,
        opponent: bool,
    ) -> bool:
        """Return True when a square attacks a high-value opponent piece."""
        piece = board.piece_at(square)
        if piece is None or piece.color == opponent:
            return False
        return any(
            target_piece.piece_type in HIGH_VALUE_PIECES
            for target_piece in _iter_opponent_targets(board, square, not opponent)
        )

    @classmethod
    def iter_unchanged_sliders(
        cls,
        board_before: chess.Board,
        board_after: chess.Board,
        mover_color: bool,
        *,
        exclude_square: chess.Square | None = None,
    ) -> Iterable[tuple[chess.Square, chess.Piece]]:
        """Yield slider pieces that did not move between boards."""
        yield from _iter_unchanged_sliders(
            board_before,
            board_after,
            mover_color,
            exclude_square,
        )


_VULTURE_USED = (
    SliderInfo,
    BaseTacticDetector.has_hanging_piece,
    BaseTacticDetector.has_high_value_target,
)
</file>

<file path="build_airflow_conf__airflow_jobs.py">
"""Build Airflow DAG run configuration payloads."""

from __future__ import annotations

from tactix.apply_airflow_optional_conf__airflow_jobs import _apply_airflow_optional_conf


def _airflow_conf(
    source: str | None,
    profile: str | None,
    backfill_start_ms: int | None = None,
    backfill_end_ms: int | None = None,
    triggered_at_ms: int | None = None,
) -> dict[str, object]:
    conf: dict[str, object] = {}
    if source:
        conf["source"] = source
    if profile:
        key = "chesscom_profile" if source == "chesscom" else "lichess_profile"
        conf[key] = profile
    _apply_airflow_optional_conf(conf, "backfill_start_ms", backfill_start_ms)
    _apply_airflow_optional_conf(conf, "backfill_end_ms", backfill_end_ms)
    _apply_airflow_optional_conf(conf, "triggered_at_ms", triggered_at_ms)
    return conf
</file>

<file path="build_chess_client__pipeline.py">
"""Build chess client instances for pipeline use."""

from __future__ import annotations

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import logger
from tactix.infra.clients.chesscom_client import ChesscomClient, ChesscomClientContext
from tactix.infra.clients.lichess_client import LichessClient, LichessClientContext
from tactix.ports.game_source_client import GameSourceClient


def _build_chess_client(
    settings: Settings,
    client: GameSourceClient | None,
) -> GameSourceClient:
    """Return an existing client or build one based on settings."""
    if client is not None:
        return client
    if settings.source == "chesscom":
        return ChesscomClient(ChesscomClientContext(settings=settings, logger=logger))
    return LichessClient(LichessClientContext(settings=settings, logger=logger))
</file>

<file path="build_chunk_row__pipeline.py">
"""Build chunk rows for raw PGN persistence."""

from __future__ import annotations

from typing import cast

from tactix.config import Settings
from tactix.extract_game_id import extract_game_id
from tactix.extract_last_timestamp_ms import extract_last_timestamp_ms
from tactix.GameRow import GameRow


def _build_chunk_row(row: GameRow, chunk: str, settings: Settings) -> GameRow:
    return cast(
        GameRow,
        {
            "game_id": extract_game_id(chunk),
            "user": row.get("user") or settings.user,
            "source": row.get("source") or settings.source,
            "fetched_at": row.get("fetched_at"),
            "pgn": chunk,
            "last_timestamp_ms": extract_last_timestamp_ms(chunk),
        },
    )
</file>

<file path="build_daily_sync_payload__pipeline.py">
"""Build the daily sync payload for API responses."""

from __future__ import annotations

from tactix.sync_contexts import DailySyncPayloadContext


def _build_daily_sync_payload(
    ctx: DailySyncPayloadContext,
) -> dict[str, object]:
    """Return a JSON payload summarizing daily sync results."""
    return {
        "source": ctx.settings.source,
        "user": ctx.settings.user,
        "fetched_games": len(ctx.games),
        "raw_pgns_inserted": ctx.raw_pgns_inserted,
        "raw_pgns_hashed": ctx.raw_pgns_hashed,
        "raw_pgns_matched": ctx.raw_pgns_matched,
        "postgres_raw_pgns_inserted": ctx.postgres_raw_pgns_inserted,
        "positions": ctx.positions_count,
        "tactics": ctx.tactics_count,
        "metrics_version": ctx.metrics_version,
        "checkpoint_ms": ctx.checkpoint_value,
        "cursor": ctx.fetch_context.cursor_before
        if ctx.backfill_mode
        else (ctx.fetch_context.next_cursor or ctx.fetch_context.cursor_value),
        "last_timestamp_ms": ctx.last_timestamp_value,
        "since_ms": ctx.fetch_context.since_ms,
    }
</file>

<file path="build_dashboard_cache_key__api_cache.py">
"""Build cache keys for dashboard payloads."""

from __future__ import annotations

from datetime import datetime
from typing import cast

from tactix.coerce_cache_value__api_cache import _cache_value
from tactix.coerce_date_cache_value__api_cache import _date_cache_value
from tactix.dashboard_query import DashboardQuery, resolve_dashboard_query
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values


def _dashboard_cache_key(
    settings,
    *args: object,
    query: DashboardQuery | str | None = None,
    filters: DashboardQuery | None = None,
    **legacy: object,
) -> tuple[object, ...]:
    ordered_keys = (
        "query",
        "source",
        "motif",
        "rating_bucket",
        "time_control",
        "start_date",
        "end_date",
    )
    values = init_legacy_values(
        ordered_keys,
        {"query": query} if query is not None else None,
    )
    apply_legacy_kwargs(values, ordered_keys[1:], legacy)
    apply_legacy_args(values, ordered_keys, args)
    resolved = resolve_dashboard_query(
        cast(DashboardQuery | str | None, values["query"]),
        filters=filters,
        source=cast(str | None, values["source"]),
        motif=cast(str | None, values["motif"]),
        rating_bucket=cast(str | None, values["rating_bucket"]),
        time_control=cast(str | None, values["time_control"]),
        start_date=cast(datetime | None, values["start_date"]),
        end_date=cast(datetime | None, values["end_date"]),
    )
    return (
        settings.user,
        str(settings.duckdb_path),
        _cache_value(resolved.source),
        _cache_value(resolved.motif),
        _cache_value(resolved.rating_bucket),
        _cache_value(resolved.time_control),
        _date_cache_value(resolved.start_date),
        _date_cache_value(resolved.end_date),
    )
</file>

<file path="build_dashboard_stats_payload__api.py">
"""Build dashboard stats payloads for API responses."""

from __future__ import annotations

from collections.abc import Callable

from tactix._resolve_dashboard_filters import _resolve_dashboard_filters
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.db.duckdb_store import fetch_version, get_connection, init_schema


def _build_dashboard_stats_payload(
    filters: DashboardQueryFilters,
    stats_fetcher: Callable[..., object],
    stats_key: str,
) -> dict[str, object]:
    start_datetime, end_datetime, normalized_source, settings = _resolve_dashboard_filters(
        filters,
    )
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    response_source = "all" if normalized_source is None else normalized_source
    return {
        "source": response_source,
        "metrics_version": fetch_version(conn),
        stats_key: stats_fetcher(
            conn,
            source=normalized_source,
            rating_bucket=filters.rating_bucket,
            time_control=filters.time_control,
            start_date=start_datetime,
            end_date=end_datetime,
        ),
    }
</file>

<file path="build_extractor_request.py">
"""Helpers for constructing extractor requests."""

from tactix.extractor_context import ExtractorRequest
from tactix.pgn_context_kwargs import PgnContextInputs, build_pgn_context_kwargs


def build_extractor_request(inputs: PgnContextInputs) -> ExtractorRequest:
    """Build an ExtractorRequest from PGN context inputs."""
    return ExtractorRequest(**build_pgn_context_kwargs(inputs=inputs))
</file>

<file path="build_pipeline_settings__pipeline.py">
"""Build pipeline settings with source/profile overrides."""

from __future__ import annotations

from tactix.config import Settings, get_settings


def _build_pipeline_settings(
    settings: Settings | None,
    source: str | None = None,
    profile: str | None = None,
) -> Settings:
    """Return settings for pipeline execution."""
    settings = settings or get_settings(source=source, profile=profile)
    if source:
        settings.source = source
    settings.apply_source_defaults()
    if profile is not None:
        settings.apply_lichess_profile(profile)
        settings.apply_chesscom_profile(profile)
    settings.ensure_dirs()
    return settings
</file>

<file path="CaptureDetector.py">
"""Detector for capture motifs."""

from __future__ import annotations

from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.TacticContext import TacticContext


class CaptureDetector(BaseTacticDetector):
    """Detect simple captures when no higher-priority motif applies."""

    motif = "capture"

    def detect(self, context: TacticContext) -> bool:
        """Return True when the move is a capture."""
        return context.board_before.is_capture(context.best_move)
</file>

<file path="check_airflow_enabled__airflow_settings.py">
"""Check whether Airflow is enabled."""

from __future__ import annotations


def _airflow_enabled(settings) -> bool:
    return settings.airflow_enabled and bool(settings.airflow_base_url)
</file>

<file path="chess_fen_char.py">
"""FEN character definitions for chess piece parsing."""

# pylint: disable=invalid-name

from enum import StrEnum


class ChessFenChar(StrEnum):
    """
    Enumeration representing the valid FEN (Forsyth-Edwards Notation) characters for chess pieces.

    Members:
        P: White pawn
        N: White knight
        B: White bishop
        R: White rook
        Q: White queen
        K: White king
        p: Black pawn
        n: Black knight
        b: Black bishop
        r: Black rook
        q: Black queen
        k: Black king

    Methods:
        is_valid(fen_char: str) -> bool:
            Checks if the given character is a valid FEN character for a chess piece.
    """

    P = "P"
    N = "N"
    B = "B"
    R = "R"
    Q = "Q"
    K = "K"
    p = "p"
    n = "n"
    b = "b"
    r = "r"
    q = "q"
    k = "k"

    @staticmethod
    def is_valid(fen_char: str) -> bool:
        """Return True when the character is a valid FEN piece letter."""
        return fen_char in ChessFenChar


_VULTURE_USED = (
    ChessFenChar.P,
    ChessFenChar.N,
    ChessFenChar.B,
    ChessFenChar.R,
    ChessFenChar.Q,
    ChessFenChar.K,
    ChessFenChar.p,
    ChessFenChar.n,
    ChessFenChar.b,
    ChessFenChar.r,
    ChessFenChar.q,
    ChessFenChar.k,
)
</file>

<file path="chess_game_result.py">
"""Chess game result enumeration helpers."""

from __future__ import annotations

from enum import StrEnum
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from tactix.chess_player_color import ChessPlayerColor


class ChessGameResult(StrEnum):
    """
    Enumeration representing possible outcomes of a chess game.

    Attributes:
        WIN: Indicates a win.
        LOSS: Indicates a loss.
        DRAW: Indicates a draw.
        INCOMPLETE: Indicates the game is incomplete.

    Methods:
        from_str(result_str: str, color: ChessPlayerColor) -> ChessGameResult:
            Converts a result string and player color to a ChessGameResult enum value.
            Raises ValueError if the result string is invalid.
    """

    WIN = "win"
    LOSS = "loss"
    DRAW = "draw"
    INCOMPLETE = "incomplete"

    @classmethod
    def from_str(cls, result_str: str, color: ChessPlayerColor) -> ChessGameResult:
        """Convert a result string and player color to a result enum."""
        # pylint: disable=import-outside-toplevel
        from tactix.chess_player_color import ChessPlayerColor  # noqa: PLC0415

        resolved = ChessPlayerColor.result_mapping(color).get(result_str.lower())
        if resolved is None:
            raise ValueError(f"Invalid game result string: {result_str}")
        return resolved
</file>

<file path="chess_game.py">
from tactix.define_chess_game__chess_game import ChessGame

__all__ = [
    "ChessGame",
]
</file>

<file path="chess_piece_type.py">
"""Piece type helpers for chess models."""

from __future__ import annotations

from enum import StrEnum

import chess


class ChessPieceType(StrEnum):
    """
    An enumeration representing the types of chess pieces.

    Attributes:
        PAWN (str): Represents a pawn piece.
        KNIGHT (str): Represents a knight piece.
        BISHOP (str): Represents a bishop piece.
        ROOK (str): Represents a rook piece.
        QUEEN (str): Represents a queen piece.
        KING (str): Represents a king piece.

    Methods:
        from_str(string: str) -> ChessPieceType:
            Converts a string representation of a chess piece
                (e.g., "p", "knight")
            to its corresponding ChessPieceType.
            Raises ValueError if the string does not correspond to a valid piece type.

        as_chess() -> chess.PieceType:
            Returns the corresponding `chess.PieceType` value for the ChessPieceType instance.
    """

    PAWN = "pawn"
    KNIGHT = "knight"
    BISHOP = "bishop"
    ROOK = "rook"
    QUEEN = "queen"
    KING = "king"

    @classmethod
    def from_str(cls, string: str) -> ChessPieceType:
        """Convert a string into a ChessPieceType."""
        mapping = {
            "p": cls.PAWN,
            "n": cls.KNIGHT,
            "b": cls.BISHOP,
            "r": cls.ROOK,
            "q": cls.QUEEN,
            "k": cls.KING,
            "pawn": cls.PAWN,
            "knight": cls.KNIGHT,
            "bishop": cls.BISHOP,
            "rook": cls.ROOK,
            "queen": cls.QUEEN,
            "king": cls.KING,
        }
        resolved = mapping.get(string.lower())
        if resolved is None:
            raise ValueError(f"Invalid piece string: {string}")
        return resolved

    def as_chess(self) -> chess.PieceType:
        """Return the python-chess piece type for this value."""
        mapping = {
            ChessPieceType.PAWN: chess.PAWN,
            ChessPieceType.KNIGHT: chess.KNIGHT,
            ChessPieceType.BISHOP: chess.BISHOP,
            ChessPieceType.ROOK: chess.ROOK,
            ChessPieceType.QUEEN: chess.QUEEN,
            ChessPieceType.KING: chess.KING,
        }
        return mapping[self]


_VULTURE_USED = (ChessPieceType.as_chess,)
</file>

<file path="chess_piece.py">
"""Chess piece data model."""

from __future__ import annotations

from dataclasses import dataclass

from tactix.chess_fen_char import ChessFenChar
from tactix.chess_piece_type import ChessPieceType
from tactix.chess_player_color import ChessPlayerColor


@dataclass
class ChessPiece:
    """
    Represents a chess piece with its color and type.

    Attributes:
        color (ChessPlayerColor): The color of the chess piece (e.g., white or black).
        piece (ChessPieceType): The type of the chess piece (e.g., pawn, knight, bishop, etc.).

    Methods:
        from_fen_char(fen_char: str) -> ChessPiece:
            Creates a ChessPiece instance from a FEN character.
            Raises ValueError if the FEN character is invalid.
    """

    color: ChessPlayerColor
    piece: ChessPieceType

    @classmethod
    def from_fen_char(cls, fen_char: str) -> ChessPiece:
        """Build a chess piece from a FEN character."""
        if not ChessFenChar.is_valid(fen_char):
            raise ValueError(f"Invalid FEN character: {fen_char}")
        color = ChessPlayerColor.from_fen_char(fen_char)
        piece_type = ChessPieceType.from_str(fen_char)
        return cls(color=color, piece=piece_type)
</file>

<file path="chess_player_color.py">
"""Chess player color helpers and result mappings."""

from __future__ import annotations

from enum import Enum

import chess

from tactix.chess_game_result import ChessGameResult


class ChessPlayerColor(Enum):
    """Enum representing the color of a chess player, with
    utility methods for conversion and result mapping.

    Attributes:
        WHITE: Represents the white player (corresponds to chess.WHITE).
        BLACK: Represents the black player (corresponds to chess.BLACK).

    Methods:
        from_str(color_str: str) -> ChessPlayerColor:
            Converts a string to the corresponding ChessPlayerColor enum value.
            Raises ValueError for input besides ("white", "w", "black", "b").

        from_fen_char(fen_char: str) -> ChessPlayerColor:
            Determines player color from a FEN character (uppercase for white, lowercase for black).

        is_white() -> bool:
            Returns True if the color is white.

        is_black() -> bool:
            Returns True if the color is black.

        result_mapping() -> dict[str, ChessGameResult]:
            Returns a mapping from result strings:
                ("win", "loss", "draw", "incomplete", "1-0", "0-1", "1/2-1/2")
            to ChessGameResult values, adjusted for the player's color.
    """

    WHITE = chess.WHITE
    BLACK = chess.BLACK

    @classmethod
    def from_str(cls, color_str: str) -> ChessPlayerColor:
        """Parse a color string into a `ChessPlayerColor`."""
        color_str = color_str.lower()
        if color_str in ["white", "w"]:
            return cls.WHITE
        if color_str in ["black", "b"]:
            return cls.BLACK
        raise ValueError(f"Invalid color string: {color_str}")

    @classmethod
    def from_fen_char(cls, fen_char: str) -> ChessPlayerColor:
        """Infer the color from a FEN piece character."""
        if fen_char.isupper():
            return cls.WHITE
        return cls.BLACK

    def is_white(self) -> bool:
        """Return True when this color is white."""
        return self == ChessPlayerColor.WHITE

    def is_black(self) -> bool:
        """Return True when this color is black."""
        return self == ChessPlayerColor.BLACK

    @property
    def _base_result_mapping(self) -> dict[str, ChessGameResult]:
        return {
            "win": ChessGameResult.WIN,
            "loss": ChessGameResult.LOSS,
            "draw": ChessGameResult.DRAW,
            "incomplete": ChessGameResult.INCOMPLETE,
        }

    @property
    def _result_mapping_if_white(self) -> dict[str, ChessGameResult]:
        return {
            **self._base_result_mapping,
            "1-0": ChessGameResult.WIN,
            "0-1": ChessGameResult.LOSS,
            "1/2-1/2": ChessGameResult.DRAW,
        }

    @property
    def _result_mapping_if_black(self) -> dict[str, ChessGameResult]:
        return {
            **self._base_result_mapping,
            "1-0": ChessGameResult.LOSS,
            "0-1": ChessGameResult.WIN,
            "1/2-1/2": ChessGameResult.DRAW,
        }

    def result_mapping(self) -> dict[str, ChessGameResult]:
        """Return the result mapping adjusted for the player's color."""
        if self.is_white():
            return self._result_mapping_if_white
        return self._result_mapping_if_black
</file>

<file path="chess_time_control.py">
"""Time control parsing helpers."""

from __future__ import annotations

from dataclasses import dataclass


@dataclass
class ChessTimeControl:
    """Represents a chess time control value."""

    initial: int  # initial time in seconds
    increment: int | None = None  # increment in seconds, if any

    @classmethod
    def from_pgn_string(cls, time_control_str: str) -> ChessTimeControl | None:
        """Parse a PGN time control string into a model."""
        if time_control_str == "-":
            return None
        if "+" in time_control_str:
            initial_str, increment_str = time_control_str.split("+", 1)
            return cls(initial=int(initial_str), increment=int(increment_str))
        return cls(initial=int(time_control_str), increment=None)

    def as_str(self) -> str:
        """Return the time control string representation."""
        if self.increment is not None:
            return f"{self.initial}+{self.increment}"
        return str(self.initial)

    def __str__(self) -> str:
        """Return the string representation of the time control."""
        return self.as_str()
</file>

<file path="chesscom_raw_games__pipeline.py">
"""Extract raw game rows from Chess.com fetch results."""

from __future__ import annotations

from collections.abc import Mapping
from typing import cast

from tactix.infra.clients.chesscom_client import ChesscomFetchResult


def _chesscom_raw_games(chesscom_result: ChesscomFetchResult) -> list[Mapping[str, object]]:
    """Return raw game row mappings from a fetch result."""
    return [cast(Mapping[str, object], row) for row in chesscom_result.games]
</file>

<file path="clear_dashboard_cache__api_cache.py">
"""Clear the dashboard cache."""

from __future__ import annotations

from tactix.dashboard_cache_state__api_cache import _DASHBOARD_CACHE, _DASHBOARD_CACHE_LOCK


def _clear_dashboard_cache() -> None:
    """Clear all cached dashboard entries."""
    with _DASHBOARD_CACHE_LOCK:
        _DASHBOARD_CACHE.clear()
</file>

<file path="CLK_PATTERN.py">
"""Regex for clock comment parsing."""

# pylint: disable=invalid-name

import re

CLK_PATTERN = re.compile(r"%clk\s+([0-9:.]+)")
</file>

<file path="coerce_backfill_end_ms__airflow_jobs.py">
"""Normalize backfill end timestamps."""

from __future__ import annotations


def _coerce_backfill_end_ms(backfill_end_ms: int | None, triggered_at_ms: int) -> int | None:
    """Return a safe backfill end timestamp."""
    if backfill_end_ms is None or backfill_end_ms > triggered_at_ms:
        return triggered_at_ms
    return backfill_end_ms
</file>

<file path="coerce_cache_value__api_cache.py">
from __future__ import annotations


def _cache_value(value: str | None, default: str = "all") -> str:
    return value or default
</file>

<file path="coerce_date_cache_value__api_cache.py">
from __future__ import annotations

from datetime import datetime


def _date_cache_value(value: datetime | None) -> str | None:
    return value.isoformat() if value else None
</file>

<file path="coerce_date_to_datetime__datetime.py">
"""Coerce date values into datetimes."""

from __future__ import annotations

from datetime import date, datetime, time


def _coerce_date_to_datetime(value: date | None, *, end_of_day: bool = False) -> datetime | None:
    """Return a datetime for the given date, optionally at end of day."""
    if value is None:
        return None
    if end_of_day:
        return datetime.combine(value, time.max)
    return datetime.combine(value, time.min)
</file>

<file path="collect_game_ids__pipeline.py">
from __future__ import annotations


def _collect_game_ids(rows: list[dict[str, object]]) -> list[str]:
    return [str(row.get("game_id", "")) for row in rows if row.get("game_id")]
</file>

<file path="collect_positions_for_monitor__pipeline.py">
from __future__ import annotations

from tactix.config import Settings
from tactix.db.fetch_unanalyzed_positions import fetch_unanalyzed_positions
from tactix.extract_positions_for_new_games__pipeline import _extract_positions_for_new_games


def _collect_positions_for_monitor(
    conn,
    settings: Settings,
    raw_pgns: list[dict[str, object]],
) -> tuple[int, list[str], list[dict[str, object]]]:
    positions_extracted = 0
    new_game_ids: list[str] = []
    if raw_pgns:
        extracted_positions, new_game_ids = _extract_positions_for_new_games(
            conn, settings, raw_pgns
        )
        positions_extracted = len(extracted_positions)

    positions_to_analyze: list[dict[str, object]] = []
    if new_game_ids:
        positions_to_analyze = fetch_unanalyzed_positions(
            conn, game_ids=new_game_ids, source=settings.source
        )

    return positions_extracted, new_game_ids, positions_to_analyze
</file>

<file path="compute_pgn_hashes__pipeline.py">
"""Compute PGN hashes for raw game rows."""

from __future__ import annotations

from tactix.db.raw_pgn_repository_provider import hash_pgn
from tactix.GameRow import GameRow


def _compute_pgn_hashes(rows: list[GameRow], source: str) -> dict[str, str]:
    hashes: dict[str, str] = {}
    for row in rows:
        game_id = row["game_id"]
        if game_id in hashes:
            raise ValueError(f"Duplicate game_id in raw PGN batch for source={source}: {game_id}")
        hashes[game_id] = hash_pgn(str(row["pgn"]))
    return hashes
</file>

<file path="config.py">
from __future__ import annotations

from dotenv import load_dotenv

from tactix.apply_env_user_overrides__config import _apply_env_user_overrides
from tactix.apply_settings_aliases__config import _apply_settings_aliases
from tactix.define_chesscom_settings__config import ChesscomSettings
from tactix.define_config_defaults__config import (
    _MISSING,
    _SETTINGS_ALIAS_FIELDS,
    _STOCKFISH_PROFILE_DEPTHS,
    DEFAULT_BLITZ_STOCKFISH_DEPTH,
    DEFAULT_BULLET_STOCKFISH_DEPTH,
    DEFAULT_CHESSCOM_ANALYSIS_CHECKPOINT,
    DEFAULT_CHESSCOM_CHECKPOINT,
    DEFAULT_CHESSCOM_FIXTURE,
    DEFAULT_CLASSICAL_STOCKFISH_DEPTH,
    DEFAULT_CORRESPONDENCE_STOCKFISH_DEPTH,
    DEFAULT_DATA_DIR,
    DEFAULT_LICHESS_ANALYSIS_CHECKPOINT,
    DEFAULT_LICHESS_CHECKPOINT,
    DEFAULT_LICHESS_FIXTURE,
    DEFAULT_RAPID_STOCKFISH_DEPTH,
)
from tactix.define_lichess_settings__config import LichessSettings
from tactix.define_settings__config import Settings
from tactix.define_stockfish_settings__config import StockfishSettings
from tactix.get_settings__config import get_settings
from tactix.raise_on_unexpected_kwargs__config import _raise_on_unexpected_kwargs
from tactix.read_fork_severity_floor__config import _read_fork_severity_floor
from tactix.resolve_field_value__config import _field_value

load_dotenv()

__all__ = [
    "DEFAULT_BLITZ_STOCKFISH_DEPTH",
    "DEFAULT_BULLET_STOCKFISH_DEPTH",
    "DEFAULT_CHESSCOM_ANALYSIS_CHECKPOINT",
    "DEFAULT_CHESSCOM_CHECKPOINT",
    "DEFAULT_CHESSCOM_FIXTURE",
    "DEFAULT_CLASSICAL_STOCKFISH_DEPTH",
    "DEFAULT_CORRESPONDENCE_STOCKFISH_DEPTH",
    "DEFAULT_DATA_DIR",
    "DEFAULT_LICHESS_ANALYSIS_CHECKPOINT",
    "DEFAULT_LICHESS_CHECKPOINT",
    "DEFAULT_LICHESS_FIXTURE",
    "DEFAULT_RAPID_STOCKFISH_DEPTH",
    "_MISSING",
    "_SETTINGS_ALIAS_FIELDS",
    "_STOCKFISH_PROFILE_DEPTHS",
    "ChesscomSettings",
    "LichessSettings",
    "Settings",
    "StockfishSettings",
    "_apply_env_user_overrides",
    "_apply_settings_aliases",
    "_field_value",
    "_raise_on_unexpected_kwargs",
    "_read_fork_severity_floor",
    "get_settings",
]
</file>

<file path="conversion_payload__pipeline.py">
"""Build conversion payloads for raw PGN ingestion."""

from __future__ import annotations

from tactix.config import Settings


def _build_conversion_payload(
    settings: Settings,
    *,
    games: int,
    inserted_games: int,
    positions: int,
) -> dict[str, object]:
    """Return a conversion payload summary."""
    return {
        "source": settings.source,
        "games": games,
        "inserted_games": inserted_games,
        "positions": positions,
    }


def _conversion_payload(
    settings: Settings,
    raw_pgns: list[dict[str, object]],
    to_process: list[dict[str, object]],
    positions: list[dict[str, object]],
) -> dict[str, object]:
    """Return a conversion payload using list lengths."""
    return _build_conversion_payload(
        settings,
        games=len(raw_pgns),
        inserted_games=len(to_process),
        positions=len(positions),
    )
</file>

<file path="convert_raw_pgns_to_positions__pipeline.py">
from __future__ import annotations

from tactix.attach_position_ids__pipeline import _attach_position_ids
from tactix.config import Settings
from tactix.conversion_payload__pipeline import _conversion_payload
from tactix.db.position_repository_provider import insert_positions
from tactix.empty_conversion_payload__pipeline import _empty_conversion_payload
from tactix.extract_positions_for_rows__pipeline import _extract_positions_for_rows
from tactix.filter_positions_to_process__pipeline import _filter_positions_to_process
from tactix.prepare_raw_pgn_context__pipeline import _prepare_raw_pgn_context


def convert_raw_pgns_to_positions(
    settings: Settings | None = None,
    source: str | None = None,
    profile: str | None = None,
    limit: int | None = None,
) -> dict[str, object]:
    settings, conn, raw_pgns = _prepare_raw_pgn_context(
        settings=settings,
        source=source,
        profile=profile,
        limit=limit,
    )
    if not raw_pgns:
        return _empty_conversion_payload(settings)

    to_process = _filter_positions_to_process(conn, raw_pgns, settings)
    positions = _extract_positions_for_rows(to_process, settings)
    position_ids = insert_positions(conn, positions)
    _attach_position_ids(positions, position_ids)

    return _conversion_payload(settings, raw_pgns, to_process, positions)
</file>

<file path="count_hash_matches__pipeline.py">
"""Count matching hashes between computed and stored maps."""

from __future__ import annotations

from collections.abc import Mapping


def _count_hash_matches(computed: Mapping[str, str], stored: Mapping[str, str]) -> int:
    return sum(1 for game_id, pgn_hash in computed.items() if stored.get(game_id) == pgn_hash)
</file>

<file path="DailyAnalysisResult.py">
from dataclasses import dataclass


@dataclass(slots=True)
class DailyAnalysisResult:
    """Summary of a daily analysis run."""

    total_positions: int
    tactics_count: int
    postgres_written: int
    postgres_synced: int
    metrics_version: int
</file>

<file path="dashboard_cache_state__api_cache.py">
"""In-memory dashboard cache state."""

from __future__ import annotations

from collections import OrderedDict
from threading import Lock

_DASHBOARD_CACHE_TTL_S = 300
_DASHBOARD_CACHE_MAX_ENTRIES = 32
_DASHBOARD_CACHE: OrderedDict[tuple[object, ...], tuple[float, dict[str, object]]] = OrderedDict()
_DASHBOARD_CACHE_LOCK = Lock()
</file>

<file path="dashboard_query_filters.py">
"""Dashboard query filter models."""

from __future__ import annotations

from datetime import date

from pydantic import BaseModel


class DashboardQueryFilters(BaseModel):
    """Filters accepted by the dashboard endpoints."""

    source: str | None = None
    rating_bucket: str | None = None
    time_control: str | None = None
    start_date: date | None = None
    end_date: date | None = None
</file>

<file path="dashboard_query.py">
"""Dashboard query models and helpers."""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from typing import Any, cast


@dataclass(frozen=True)
class DashboardQuery:
    """Filters used to query dashboard metrics and payloads."""

    source: str | None = None
    motif: str | None = None
    rating_bucket: str | None = None
    time_control: str | None = None
    start_date: datetime | None = None
    end_date: datetime | None = None


def resolve_dashboard_query(
    query: DashboardQuery | str | None = None,
    *,
    filters: DashboardQuery | None = None,
    **legacy: Any,
) -> DashboardQuery:
    """Resolve legacy inputs into a DashboardQuery instance."""
    if isinstance(query, DashboardQuery):
        return query
    if filters is None:
        filters = DashboardQuery(
            source=legacy.pop("source", None),
            motif=legacy.pop("motif", None),
            rating_bucket=legacy.pop("rating_bucket", None),
            time_control=legacy.pop("time_control", None),
            start_date=legacy.pop("start_date", None),
            end_date=legacy.pop("end_date", None),
        )
    if legacy:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(legacy))}")
    if query is None:
        return filters
    return DashboardQuery(
        source=query,
        motif=filters.motif,
        rating_bucket=filters.rating_bucket,
        time_control=filters.time_control,
        start_date=filters.start_date,
        end_date=filters.end_date,
    )


def clone_dashboard_query(
    query: DashboardQuery,
    **overrides: Any,
) -> DashboardQuery:
    """Clone a dashboard query with optional overrides."""
    return DashboardQuery(
        source=cast(str | None, overrides.get("source", query.source)),
        motif=cast(str | None, overrides.get("motif", query.motif)),
        rating_bucket=cast(str | None, overrides.get("rating_bucket", query.rating_bucket)),
        time_control=cast(str | None, overrides.get("time_control", query.time_control)),
        start_date=cast(datetime | None, overrides.get("start_date", query.start_date)),
        end_date=cast(datetime | None, overrides.get("end_date", query.end_date)),
    )
</file>

<file path="dedupe_games__pipeline.py">
from __future__ import annotations

from tactix.GameRow import GameRow


def _dedupe_games(rows: list[GameRow]) -> list[GameRow]:
    seen: set[tuple[str, str, int]] = set()
    deduped: list[GameRow] = []
    for game in rows:
        game_id = game["game_id"]
        source = game["source"]
        last_ts = game["last_timestamp_ms"]
        key = (game_id, source, last_ts)
        if key in seen:
            continue
        seen.add(key)
        deduped.append(game)
    return deduped
</file>

<file path="define_base_db_store__db_store.py">
"""Base database store interface and helpers."""

# pylint: disable=redefined-builtin

from __future__ import annotations

import logging
from collections.abc import Callable, Mapping
from datetime import UTC, datetime
from typing import cast

from tactix.config import Settings
from tactix.dashboard_query import DashboardQuery
from tactix.define_base_db_store_context__db_store import BaseDbStoreContext
from tactix.define_outcome_insert_plan__db_store import OutcomeInsertPlan
from tactix.define_tactic_insert_plan__db_store import TacticInsertPlan
from tactix.extract_pgn_metadata import extract_pgn_metadata
from tactix.is_latest_hash__db_store import _is_latest_hash
from tactix.PgnUpsertInputs import PgnUpsertHashing, PgnUpsertInputs, PgnUpsertTimestamps
from tactix.PgnUpsertPlan import PgnUpsertPlan
from tactix.resolve_pgn_hash__db_store import _resolve_pgn_hash
from tactix.resolve_timestamp__db_store import _resolve_timestamp
from tactix.utils import hash


class BaseDbStore:
    """Base class for database stores.

    Subclasses are expected to implement dashboard-specific behavior.
    """

    def __init__(self, context: BaseDbStoreContext) -> None:
        """Initialize the store with shared context.

        Args:
            context: Base context containing settings and logger.
        """

        self._context = context

    @property
    def settings(self) -> Settings:
        """Expose the settings from the context."""

        return self._context.settings

    @property
    def logger(self) -> logging.Logger:
        """Expose the logger from the context."""

        return self._context.logger

    @staticmethod
    def extract_pgn_metadata(pgn: str, user: str) -> Mapping[str, object]:
        """Extract PGN metadata using shared utilities."""

        return extract_pgn_metadata(pgn, user)

    @staticmethod
    def hash_pgn(pgn: str) -> str:
        """Return the normalized hash for PGN content."""

        return hash(pgn)

    @staticmethod
    def _resolve_pgn_upsert_inputs(
        inputs: PgnUpsertInputs | None,
        legacy: dict[str, object],
    ) -> PgnUpsertInputs:
        if inputs is not None:
            if legacy:
                raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(legacy))}")
            return inputs
        values = BaseDbStore._extract_pgn_upsert_values(legacy)
        return BaseDbStore._build_pgn_upsert_inputs(values)

    @staticmethod
    def _extract_pgn_upsert_values(legacy: dict[str, object]) -> dict[str, object]:
        defaults: dict[str, object] = {
            "pgn_text": None,
            "user": None,
            "latest_hash": None,
            "latest_version": None,
            "normalize_pgn": None,
            "hash_pgn": None,
            "fetched_at": None,
            "ingested_at": None,
            "last_timestamp_ms": 0,
            "cursor": None,
        }
        values = {key: legacy.pop(key, default) for key, default in defaults.items()}
        if legacy:
            raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(legacy))}")
        return values

    @staticmethod
    def _build_pgn_upsert_inputs(values: dict[str, object]) -> PgnUpsertInputs:
        if values["pgn_text"] is None or values["user"] is None or values["latest_version"] is None:
            raise TypeError("pgn_text, user, and latest_version are required")
        return PgnUpsertInputs(
            pgn_text=cast(str, values["pgn_text"]),
            user=cast(str, values["user"]),
            latest_hash=cast(str | None, values["latest_hash"]),
            latest_version=cast(int, values["latest_version"]),
            hashing=PgnUpsertHashing(
                normalize_pgn=cast(Callable[[str], str] | None, values["normalize_pgn"]),
                hash_pgn=cast(Callable[[str], str] | None, values["hash_pgn"]),
            ),
            timestamps=PgnUpsertTimestamps(
                fetched_at=cast(datetime | None, values["fetched_at"]),
                ingested_at=cast(datetime | None, values["ingested_at"]),
                last_timestamp_ms=cast(int, values["last_timestamp_ms"]),
            ),
            cursor=values["cursor"],
        )

    @staticmethod
    def build_pgn_upsert_plan(
        inputs: PgnUpsertInputs | None = None,
        **legacy: object,
    ) -> PgnUpsertPlan | None:
        """Build an upsert plan for PGN content when needed."""
        inputs = BaseDbStore._resolve_pgn_upsert_inputs(inputs, legacy)
        normalized, pgn_hash = _resolve_pgn_hash(
            inputs.pgn_text,
            inputs.normalize_pgn,
            inputs.hash_pgn,
        )
        if _is_latest_hash(inputs.latest_hash, pgn_hash):
            return None
        metadata = extract_pgn_metadata(inputs.pgn_text, inputs.user)
        return PgnUpsertPlan(
            pgn_text=inputs.pgn_text,
            pgn_hash=pgn_hash,
            pgn_version=inputs.latest_version + 1,
            normalized_pgn=normalized,
            metadata=metadata,
            fetched_at=_resolve_timestamp(inputs.fetched_at),
            ingested_at=_resolve_timestamp(inputs.ingested_at),
            last_timestamp_ms=inputs.last_timestamp_ms,
            cursor=inputs.cursor,
        )

    @staticmethod
    def require_position_id(
        tactic_row: Mapping[str, object],
        error_message: str,
    ) -> object:
        """Return the position id or raise a ValueError."""
        position_id = tactic_row.get("position_id")
        if position_id is None:
            raise ValueError(error_message)
        return position_id

    @staticmethod
    def build_tactic_insert_plan(
        *,
        game_id: object,
        position_id: object,
        tactic_row: Mapping[str, object],
    ) -> TacticInsertPlan:
        """Build a tactic insert plan from a row mapping."""
        return TacticInsertPlan(
            game_id=game_id,
            position_id=position_id,
            motif=cast(str, tactic_row.get("motif", "unknown")),
            severity=tactic_row.get("severity", 0.0),
            best_uci=tactic_row.get("best_uci", ""),
            best_san=tactic_row.get("best_san"),
            explanation=tactic_row.get("explanation"),
            eval_cp=tactic_row.get("eval_cp", 0),
        )

    @staticmethod
    def build_outcome_insert_plan(
        outcome_row: Mapping[str, object],
    ) -> OutcomeInsertPlan:
        """Build an outcome insert plan from a row mapping."""
        return OutcomeInsertPlan(
            result=cast(str, outcome_row.get("result", "unclear")),
            user_uci=outcome_row.get("user_uci", ""),
            eval_delta=outcome_row.get("eval_delta", 0),
        )

    def _now_utc(self) -> datetime:
        """Return the current UTC time."""

        return datetime.now(UTC)

    def get_dashboard_payload(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, object]:
        """Build a dashboard payload for the store implementation."""

        raise NotImplementedError("Subclasses must implement get_dashboard_payload")
</file>

<file path="define_base_db_store_context__db_store.py">
from __future__ import annotations

import logging
from dataclasses import dataclass

from tactix.config import Settings


@dataclass(slots=True)
class BaseDbStoreContext:
    """Shared context for database stores.

    Attributes:
        settings: Application settings for store configuration.
        logger: Logger for store-specific messages.
    """

    settings: Settings
    logger: logging.Logger
</file>

<file path="define_chess_game__chess_game.py">
from collections.abc import Mapping
from dataclasses import dataclass, field
from io import StringIO

import chess.pgn

from tactix.utils import generate_id

_EMPTY_HEADERS: Mapping[str, str] = {}


@dataclass(slots=True)
class ChessGame:
    _raw_pgn: str
    game_id: str = field(default_factory=generate_id)

    @property
    def game(self) -> chess.pgn.Game | None:
        return chess.pgn.read_game(StringIO(self._raw_pgn))

    @property
    def headers(self) -> Mapping[str, str]:
        game = self.game
        return game.headers if game else _EMPTY_HEADERS

    @property
    def timestamp(self) -> str | None:
        return self.headers.get("UTCDate")
</file>

<file path="define_chesscom_settings__config.py">
from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path

from tactix.define_config_defaults__config import DEFAULT_CHESSCOM_CHECKPOINT


@dataclass(slots=True)
class ChesscomSettings:
    """Chess.com-specific configuration."""

    user: str = os.getenv("CHESSCOM_USERNAME", os.getenv("CHESSCOM_USER", "chesscom"))
    token: str | None = os.getenv("CHESSCOM_TOKEN")
    time_class: str = os.getenv("CHESSCOM_TIME_CLASS", "blitz")
    profile: str = os.getenv("TACTIX_CHESSCOM_PROFILE", "")
    max_retries: int = int(os.getenv("CHESSCOM_MAX_RETRIES", "3"))
    retry_backoff_ms: int = int(os.getenv("CHESSCOM_RETRY_BACKOFF_MS", "500"))
    checkpoint_path: Path = Path(
        os.getenv("TACTIX_CHESSCOM_CHECKPOINT_PATH", DEFAULT_CHESSCOM_CHECKPOINT)
    )
</file>

<file path="define_config_defaults__config.py">
"""Define default configuration values and environment aliases."""

# pylint: disable=fixme

from __future__ import annotations

import os
from pathlib import Path

from dotenv import load_dotenv

load_dotenv()

_MISSING = object()
_SETTINGS_ALIAS_FIELDS = (
    "lichess_user",
    "lichess_token",
    "lichess_oauth_client_id",
    "lichess_oauth_client_secret",
    "lichess_oauth_refresh_token",
    "lichess_oauth_token_url",
    "chesscom_user",
    "chesscom_token",
    "chesscom_time_class",
    "chesscom_profile",
    "chesscom_max_retries",
    "chesscom_retry_backoff_ms",
    "chesscom_checkpoint_path",
    "stockfish_path",
    "stockfish_checksum",
    "stockfish_checksum_mode",
    "stockfish_threads",
    "stockfish_hash_mb",
    "stockfish_movetime_ms",
    "stockfish_depth",
    "stockfish_multipv",
    "stockfish_skill_level",
    "stockfish_limit_strength",
    "stockfish_uci_elo",
    "stockfish_uci_analyse_mode",
    "stockfish_use_nnue",
    "stockfish_ponder",
    "stockfish_random_seed",
    "stockfish_max_retries",
    "stockfish_retry_backoff_ms",
)

# TODO: store this in the metadata db, not a random file
DEFAULT_DATA_DIR = Path(os.getenv("TACTIX_DATA_DIR", "data"))
DEFAULT_LICHESS_CHECKPOINT = DEFAULT_DATA_DIR / "lichess_since.txt"
DEFAULT_LICHESS_ANALYSIS_CHECKPOINT = DEFAULT_DATA_DIR / "analysis_checkpoint_lichess.json"
DEFAULT_LICHESS_FIXTURE = Path("tests/fixtures/lichess_rapid_sample.pgn")
DEFAULT_CHESSCOM_CHECKPOINT = DEFAULT_DATA_DIR / "chesscom_since.txt"
DEFAULT_CHESSCOM_ANALYSIS_CHECKPOINT = DEFAULT_DATA_DIR / "analysis_checkpoint_chesscom.json"
DEFAULT_CHESSCOM_FIXTURE = Path("tests/fixtures/chesscom_blitz_sample.pgn")
DEFAULT_BULLET_STOCKFISH_DEPTH = 8
DEFAULT_BLITZ_STOCKFISH_DEPTH = 10
DEFAULT_RAPID_STOCKFISH_DEPTH = 12
DEFAULT_CLASSICAL_STOCKFISH_DEPTH = 14
DEFAULT_CORRESPONDENCE_STOCKFISH_DEPTH = 16
_STOCKFISH_PROFILE_DEPTHS = {
    "bullet": DEFAULT_BULLET_STOCKFISH_DEPTH,
    "blitz": DEFAULT_BLITZ_STOCKFISH_DEPTH,
    "rapid": DEFAULT_RAPID_STOCKFISH_DEPTH,
    "classical": DEFAULT_CLASSICAL_STOCKFISH_DEPTH,
    "correspondence": DEFAULT_CORRESPONDENCE_STOCKFISH_DEPTH,
}
</file>

<file path="define_db_schemas__const.py">
"""Database schema name definitions."""

from dataclasses import dataclass


@dataclass(slots=True)
class DbSchemas:
    """
    A container class for database schema names used in the Tactix application.

    Attributes:
        analysis (str): The schema name for analysis-related tables.
        pgn (str): The schema name for PGN (Portable Game Notation) related tables.
    """

    analysis: str = "tactix_analysis"
    pgn: str = "tactix_pgns"


ANALYSIS_SCHEMA = DbSchemas().analysis
PGN_SCHEMA = DbSchemas().pgn

__all__ = [
    "ANALYSIS_SCHEMA",
    "PGN_SCHEMA",
    "DbSchemas",
]
</file>

<file path="define_lichess_settings__config.py">
"""Define Lichess-specific configuration values."""

from __future__ import annotations

import os
from dataclasses import dataclass, field
from pathlib import Path

from tactix.define_config_defaults__config import DEFAULT_DATA_DIR, DEFAULT_LICHESS_CHECKPOINT


@dataclass(slots=True)
class LichessSettings:  # pylint: disable=too-many-instance-attributes
    """Lichess-specific configuration."""

    user: str = os.getenv("LICHESS_USERNAME", os.getenv("LICHESS_USER", "lichess"))
    token: str | None = os.getenv("LICHESS_TOKEN")
    oauth_client_id: str | None = os.getenv("LICHESS_OAUTH_CLIENT_ID")
    oauth_client_secret: str | None = os.getenv("LICHESS_OAUTH_CLIENT_SECRET")
    oauth_refresh_token: str | None = os.getenv("LICHESS_OAUTH_REFRESH_TOKEN")
    oauth_token_url: str = os.getenv("LICHESS_OAUTH_TOKEN_URL", "https://lichess.org/api/token")

    profile: str = os.getenv("TACTIX_LICHESS_PROFILE", "")
    data_dir: Path = Path(os.getenv("LICHESS_DATA_DIR", str(DEFAULT_DATA_DIR)))
    checkpoint_path: Path = Path(
        os.getenv("TACTIX_LICHESS_CHECKPOINT_PATH", DEFAULT_LICHESS_CHECKPOINT)
    )
    token_cache_path: Path = field(init=False)

    def __post_init__(self) -> None:
        """Ensure data directory exists and compute token cache path."""
        if not self.data_dir.exists():
            self.data_dir.mkdir(parents=True, exist_ok=True)
        object.__setattr__(
            self,
            "token_cache_path",
            Path(os.getenv("LICHESS_TOKEN_CACHE_PATH", str(self.data_dir / "lichess_token.json"))),
        )
</file>

<file path="define_mock_store__db.py">
"""Mock database store implementation."""

from __future__ import annotations

from collections.abc import Iterable, Mapping
from datetime import date, datetime
from typing import cast

from tactix.dashboard_query import DashboardQuery, resolve_dashboard_query
from tactix.define_base_db_store__db_store import BaseDbStore
from tactix.define_base_db_store_context__db_store import BaseDbStoreContext


class MockDbStore(BaseDbStore):
    """Mock database store that serves in-memory dashboard payloads."""

    def __init__(
        self, context: BaseDbStoreContext, payload: dict[str, object] | None = None
    ) -> None:
        super().__init__(context)
        self._payload = payload or {
            "source": context.settings.source,
            "user": context.settings.user,
            "metrics": [],
            "recent_games": [],
            "positions": [],
            "tactics": [],
            "metrics_version": 0,
        }

    def get_dashboard_payload(
        self,
        query: DashboardQuery | str | None = None,
        *,
        filters: DashboardQuery | None = None,
        **legacy: object,
    ) -> dict[str, object]:
        query = resolve_dashboard_query(query, filters=filters, **legacy)
        payload = dict(self._payload)
        normalized_source = None if query.source in (None, "all") else query.source
        payload["source"] = normalized_source or "all"
        payload["user"] = self.settings.user
        payload["metrics"] = _filter_rows(
            cast(Iterable[object], payload.get("metrics", [])),
            DashboardQuery(
                source=normalized_source,
                motif=query.motif,
                rating_bucket=query.rating_bucket,
                time_control=query.time_control,
                start_date=query.start_date,
                end_date=query.end_date,
            ),
        )
        payload["recent_games"] = _filter_rows(
            cast(Iterable[object], payload.get("recent_games", [])),
            DashboardQuery(
                source=normalized_source,
                motif=query.motif,
                rating_bucket=query.rating_bucket,
                time_control=query.time_control,
                start_date=query.start_date,
                end_date=query.end_date,
            ),
        )
        payload["positions"] = _filter_rows(
            cast(Iterable[object], payload.get("positions", [])),
            DashboardQuery(
                source=normalized_source,
                motif=query.motif,
                rating_bucket=query.rating_bucket,
                time_control=query.time_control,
                start_date=query.start_date,
                end_date=query.end_date,
            ),
        )
        payload["tactics"] = _filter_rows(
            cast(Iterable[object], payload.get("tactics", [])),
            DashboardQuery(
                source=normalized_source,
                motif=query.motif,
                rating_bucket=query.rating_bucket,
                time_control=query.time_control,
                start_date=query.start_date,
                end_date=query.end_date,
            ),
        )
        return payload


def _filter_rows(
    rows: Iterable[object],
    query: DashboardQuery,
) -> list[object]:
    filtered: list[object] = []
    for row in rows:
        if not isinstance(row, dict):
            filtered.append(row)
            continue
        row_dict = cast(Mapping[str, object], row)
        if not _row_matches_filters(
            row_dict,
            query,
        ):
            continue
        filtered.append(row)
    return filtered


def _row_matches_filters(
    row: Mapping[str, object],
    query: DashboardQuery,
) -> bool:
    return all(
        (
            _matches_value(row, "source", query.source),
            _matches_value(row, "motif", query.motif),
            _matches_value(row, "rating_bucket", query.rating_bucket),
            _matches_value(row, "time_control", query.time_control),
            _matches_date_range(row, query.start_date, query.end_date),
        )
    )


def _matches_value(row: Mapping[str, object], key: str, value: str | None) -> bool:
    if value is None:
        return True
    row_value = row.get(key)
    if row_value is None:
        return True
    return row_value == value


def _matches_date_range(
    row: Mapping[str, object],
    start_date: datetime | None,
    end_date: datetime | None,
) -> bool:
    if start_date is None and end_date is None:
        return True
    row_date = _extract_date(row)
    return _date_in_range(row_date, start_date, end_date)


def _date_in_range(
    row_date: date | None,
    start_date: datetime | None,
    end_date: datetime | None,
) -> bool:
    if row_date is None:
        return True
    start_value = _coerce_bound(start_date)
    end_value = _coerce_bound(end_date)
    return _within_bounds(row_date, start_value, end_value)


def _coerce_bound(value: datetime | None) -> date | None:
    return _coerce_date(value) if value else None


def _within_bounds(value: date, start: date | None, end: date | None) -> bool:
    return (start is None or value >= start) and (end is None or value <= end)


def _extract_date(row: Mapping[str, object]) -> date | None:
    for key in ("created_at", "played_at", "trend_date"):
        value = row.get(key)
        date_value = _coerce_date(value)
        if date_value is not None:
            return date_value
    return None


def _coerce_date(value: object) -> date | None:
    if isinstance(value, datetime):
        return value.date()
    if isinstance(value, date):
        return value
    return None
</file>

<file path="define_outcome_insert_plan__db_store.py">
"""Define the outcome insert plan for persistence."""

from __future__ import annotations

from dataclasses import dataclass


@dataclass(slots=True)
class OutcomeInsertPlan:
    """Payload for inserting an outcome row."""

    result: str
    user_uci: object
    eval_delta: object
</file>

<file path="define_pipeline_state__pipeline.py">
"""Shared pipeline state types and constants."""

from __future__ import annotations

from collections.abc import Callable

from tactix.utils.logger import Logger

logger = Logger("tactix.pipeline")

ANALYSIS_PROGRESS_BUCKETS = 20
DEFAULT_SYNC_LIMIT = 50
INDEX_OFFSET = 1
RESUME_INDEX_START = 0
SINGLE_PGN_CHUNK = 1
ZERO_COUNT = 0
LICHESS_BLACK_PROFILES = {"bullet", "blitz", "rapid", "classical", "correspondence"}
CHESSCOM_BLACK_PROFILES = {
    "bullet",
    "blitz",
    "rapid",
    "classical",
    "correspondence",
    "daily",
}


ProgressCallback = Callable[[dict[str, object]], None]
</file>

<file path="define_settings__config.py">
"""Define top-level application settings."""

from __future__ import annotations

import os
from dataclasses import dataclass, field, fields
from pathlib import Path

from tactix.airflow_settings import AirflowSettings
from tactix.apply_settings_aliases__config import _apply_settings_aliases
from tactix.define_chesscom_settings__config import ChesscomSettings
from tactix.define_config_defaults__config import (
    _MISSING,
    _STOCKFISH_PROFILE_DEPTHS,
    DEFAULT_CHESSCOM_ANALYSIS_CHECKPOINT,
    DEFAULT_CHESSCOM_FIXTURE,
    DEFAULT_DATA_DIR,
    DEFAULT_LICHESS_ANALYSIS_CHECKPOINT,
    DEFAULT_LICHESS_CHECKPOINT,
    DEFAULT_LICHESS_FIXTURE,
)
from tactix.define_lichess_settings__config import LichessSettings
from tactix.define_stockfish_settings__config import StockfishSettings
from tactix.PostgresSettings import PostgresSettings
from tactix.raise_on_unexpected_kwargs__config import _raise_on_unexpected_kwargs
from tactix.read_fork_severity_floor__config import _read_fork_severity_floor
from tactix.resolve_field_value__config import _field_value


def _normalize_source_value(source: str) -> str:
    return (source or "").strip().lower() or "lichess"


def _default_user_value() -> str:
    return os.getenv(
        "TACTIX_USER",
        os.getenv("LICHESS_USERNAME", os.getenv("LICHESS_USER", "lichess")),
    )


def _normalize_profile_value(profile: str | None) -> str:
    return (profile or "").strip().lower()


def _chesscom_time_class_from_profile(profile: str) -> str:
    if profile == "correspondence":
        return "daily"
    return profile


@dataclass(slots=True, init=False)
# pylint: disable=too-many-public-methods,missing-function-docstring,too-many-instance-attributes
class Settings:
    """Central configuration for ingestion, analysis, and UI refresh."""

    api_token: str = os.getenv("TACTIX_API_TOKEN", "local-dev-token")
    user: str = os.getenv(
        "TACTIX_USER",
        os.getenv("LICHESS_USERNAME", os.getenv("LICHESS_USER", "lichess")),
    )
    source: str = os.getenv("TACTIX_SOURCE", "lichess")

    lichess: LichessSettings = field(default_factory=LichessSettings)
    chesscom: ChesscomSettings = field(default_factory=ChesscomSettings)

    stockfish: StockfishSettings = field(default_factory=StockfishSettings)
    postgres: PostgresSettings = field(default_factory=PostgresSettings)
    airflow: AirflowSettings = field(default_factory=AirflowSettings)

    duckdb_path: Path = Path(os.getenv("TACTIX_DUCKDB_PATH", DEFAULT_DATA_DIR / "tactix.duckdb"))
    fork_severity_floor: float | None = _read_fork_severity_floor()
    metrics_version_file: Path = Path(
        os.getenv("TACTIX_METRICS_VERSION_PATH", DEFAULT_DATA_DIR / "metrics_version.txt")
    )
    rapid_perf: str = os.getenv("TACTIX_PERF", "rapid")
    run_context: str = os.getenv("TACTIX_RUN_CONTEXT", "app")
    checkpoint_path: Path = DEFAULT_LICHESS_CHECKPOINT
    analysis_checkpoint_path: Path = DEFAULT_LICHESS_ANALYSIS_CHECKPOINT
    fixture_pgn_path: Path = DEFAULT_LICHESS_FIXTURE
    use_fixture_when_no_token: bool = False
    chesscom_fixture_pgn_path: Path = DEFAULT_CHESSCOM_FIXTURE
    chesscom_use_fixture_when_no_token: bool = False
    _stockfish_path_overridden: bool = False

    def __init__(self, **kwargs: object) -> None:
        for field_info in fields(self):
            name = field_info.name
            setattr(self, name, _field_value(name, field_info, kwargs))
        _apply_settings_aliases(self, kwargs)
        self._apply_compat_kwargs(kwargs)
        _raise_on_unexpected_kwargs(kwargs)

    def _apply_compat_kwargs(self, kwargs: dict[str, object]) -> None:
        compat_fields = (
            "checkpoint_path",
            "analysis_checkpoint_path",
            "fixture_pgn_path",
            "use_fixture_when_no_token",
            "chesscom_fixture_pgn_path",
            "chesscom_use_fixture_when_no_token",
            "lichess_profile",
            "lichess_token_cache_path",
            "airflow_base_url",
            "airflow_username",
            "airflow_password",
            "airflow_api_timeout_s",
            "airflow_poll_interval_s",
            "airflow_poll_timeout_s",
            "airflow_enabled",
            "postgres_dsn",
            "postgres_host",
            "postgres_port",
            "postgres_db",
            "postgres_user",
            "postgres_password",
            "postgres_sslmode",
            "postgres_connect_timeout_s",
            "postgres_analysis_enabled",
            "postgres_pgns_enabled",
        )
        for name in compat_fields:
            value = kwargs.pop(name, _MISSING)
            if value is not _MISSING:
                setattr(self, name, value)

    def apply_source_defaults(self) -> None:
        source = _normalize_source_value(self.source)
        self.source = source
        default_user = _default_user_value()
        if source == "chesscom":
            self._apply_source_defaults__chesscom(default_user)
            return
        self._apply_source_defaults__lichess(default_user)

    def apply_lichess_profile(self, profile: str | None = None) -> None:
        if self.source != "lichess":
            return
        profile_value = (profile or self.lichess_profile or "").strip()
        if not profile_value:
            return
        normalized = profile_value.lower()
        self.lichess_profile = normalized
        self.apply_stockfish_profile(normalized)
        self.checkpoint_path = self.checkpoint_path.with_name(f"lichess_since_{normalized}.txt")
        self.analysis_checkpoint_path = self.analysis_checkpoint_path.with_name(
            f"analysis_checkpoint_lichess_{normalized}.json"
        )
        self.fixture_pgn_path = self.fixture_pgn_path.with_name(f"lichess_{normalized}_sample.pgn")

    def apply_chesscom_profile(self, profile: str | None = None) -> None:
        if self.source != "chesscom":
            return
        normalized = _normalize_profile_value(profile or self.chesscom_profile)
        if not normalized:
            return
        self.chesscom_profile = normalized
        self.apply_stockfish_profile(normalized)
        self._apply_chesscom_profile_paths(normalized)

    def apply_stockfish_profile(self, profile: str | None = None) -> None:
        profile_value = (profile or "").strip().lower()
        if not profile_value:
            return
        if self.stockfish_depth is None:
            depth = _STOCKFISH_PROFILE_DEPTHS.get(profile_value)
            if depth is not None:
                self.stockfish_depth = depth

    def _apply_source_defaults__chesscom(self, default_user: str) -> None:
        if self.user == default_user:
            self.user = self.chesscom.user
        if self.checkpoint_path == DEFAULT_LICHESS_CHECKPOINT:
            self.checkpoint_path = self.chesscom.checkpoint_path
        if self.analysis_checkpoint_path == DEFAULT_LICHESS_ANALYSIS_CHECKPOINT:
            self.analysis_checkpoint_path = DEFAULT_CHESSCOM_ANALYSIS_CHECKPOINT

    def _apply_source_defaults__lichess(self, default_user: str) -> None:
        if self.user == default_user:
            self.user = self.lichess.user
        if self.checkpoint_path == self.chesscom.checkpoint_path:
            self.checkpoint_path = DEFAULT_LICHESS_CHECKPOINT
        if self.analysis_checkpoint_path == DEFAULT_CHESSCOM_ANALYSIS_CHECKPOINT:
            self.analysis_checkpoint_path = DEFAULT_LICHESS_ANALYSIS_CHECKPOINT

    def _apply_chesscom_profile_paths(self, normalized: str) -> None:
        self.chesscom_time_class = _chesscom_time_class_from_profile(normalized)
        self.checkpoint_path = self.checkpoint_path.with_name(f"chesscom_since_{normalized}.txt")
        self.analysis_checkpoint_path = self.analysis_checkpoint_path.with_name(
            f"analysis_checkpoint_chesscom_{normalized}.json"
        )
        self.fixture_pgn_path = self.fixture_pgn_path.with_name(f"chesscom_{normalized}_sample.pgn")
        self.chesscom_fixture_pgn_path = self.chesscom_fixture_pgn_path.with_name(
            f"chesscom_{normalized}_sample.pgn"
        )

    @property
    def data_dir(self) -> Path:
        return self.duckdb_path.parent

    def ensure_dirs(self) -> None:
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.metrics_version_file.parent.mkdir(parents=True, exist_ok=True)
        if self.analysis_checkpoint_path is not None:
            self.analysis_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)

    @property
    def lichess_profile(self) -> str:
        return self.lichess.profile

    @lichess_profile.setter
    def lichess_profile(self, value: str) -> None:
        self.lichess.profile = value

    @property
    def chesscom_profile(self) -> str:
        return self.chesscom.profile

    @chesscom_profile.setter
    def chesscom_profile(self, value: str) -> None:
        self.chesscom.profile = value

    @property
    def lichess_token_cache_path(self) -> Path:
        return self.lichess.token_cache_path

    @lichess_token_cache_path.setter
    def lichess_token_cache_path(self, value: Path) -> None:
        self.lichess.token_cache_path = value

    @property
    def airflow_base_url(self) -> str:
        return self.airflow.base_url

    @airflow_base_url.setter
    def airflow_base_url(self, value: str) -> None:
        self.airflow.base_url = value

    @property
    def airflow_username(self) -> str:
        return self.airflow.username

    @airflow_username.setter
    def airflow_username(self, value: str) -> None:
        self.airflow.username = value

    @property
    def airflow_password(self) -> str:
        return self.airflow.password

    @airflow_password.setter
    def airflow_password(self, value: str) -> None:
        self.airflow.password = value

    @property
    def airflow_api_timeout_s(self) -> int:
        return self.airflow.api_timeout_s

    @airflow_api_timeout_s.setter
    def airflow_api_timeout_s(self, value: int) -> None:
        self.airflow.api_timeout_s = value

    @property
    def airflow_poll_interval_s(self) -> int:
        return self.airflow.poll_interval_s

    @airflow_poll_interval_s.setter
    def airflow_poll_interval_s(self, value: int) -> None:
        self.airflow.poll_interval_s = value

    @property
    def airflow_poll_timeout_s(self) -> int:
        return self.airflow.poll_timeout_s

    @airflow_poll_timeout_s.setter
    def airflow_poll_timeout_s(self, value: int) -> None:
        self.airflow.poll_timeout_s = value

    @property
    def airflow_enabled(self) -> bool:
        return self.airflow.enabled

    @airflow_enabled.setter
    def airflow_enabled(self, value: bool) -> None:
        self.airflow.enabled = value

    @property
    def postgres_dsn(self) -> str | None:
        return self.postgres.dsn

    @postgres_dsn.setter
    def postgres_dsn(self, value: str | None) -> None:
        self.postgres.dsn = value

    @property
    def postgres_host(self) -> str | None:
        return self.postgres.host

    @postgres_host.setter
    def postgres_host(self, value: str | None) -> None:
        self.postgres.host = value

    @property
    def postgres_port(self) -> int:
        return self.postgres.port

    @postgres_port.setter
    def postgres_port(self, value: int) -> None:
        self.postgres.port = value

    @property
    def postgres_db(self) -> str | None:
        return self.postgres.db

    @postgres_db.setter
    def postgres_db(self, value: str | None) -> None:
        self.postgres.db = value

    @property
    def postgres_user(self) -> str | None:
        return self.postgres.user

    @postgres_user.setter
    def postgres_user(self, value: str | None) -> None:
        self.postgres.user = value

    @property
    def postgres_password(self) -> str | None:
        return self.postgres.password

    @postgres_password.setter
    def postgres_password(self, value: str | None) -> None:
        self.postgres.password = value

    @property
    def postgres_sslmode(self) -> str:
        return self.postgres.sslmode

    @postgres_sslmode.setter
    def postgres_sslmode(self, value: str) -> None:
        self.postgres.sslmode = value

    @property
    def postgres_connect_timeout_s(self) -> int:
        return self.postgres.connect_timeout_s

    @postgres_connect_timeout_s.setter
    def postgres_connect_timeout_s(self, value: int) -> None:
        self.postgres.connect_timeout_s = value

    @property
    def postgres_analysis_enabled(self) -> bool:
        return self.postgres.analysis_enabled

    @postgres_analysis_enabled.setter
    def postgres_analysis_enabled(self, value: bool) -> None:
        self.postgres.analysis_enabled = value

    @property
    def postgres_pgns_enabled(self) -> bool:
        return self.postgres.pgns_enabled

    @postgres_pgns_enabled.setter
    def postgres_pgns_enabled(self, value: bool) -> None:
        self.postgres.pgns_enabled = value

    @property
    def lichess_user(self) -> str:
        return self.lichess.user

    @lichess_user.setter
    def lichess_user(self, value: str) -> None:
        self.lichess.user = value

    @property
    def lichess_token(self) -> str | None:
        return self.lichess.token

    @lichess_token.setter
    def lichess_token(self, value: str | None) -> None:
        self.lichess.token = value

    @property
    def lichess_oauth_client_id(self) -> str | None:
        return self.lichess.oauth_client_id

    @lichess_oauth_client_id.setter
    def lichess_oauth_client_id(self, value: str | None) -> None:
        self.lichess.oauth_client_id = value

    @property
    def lichess_oauth_client_secret(self) -> str | None:
        return self.lichess.oauth_client_secret

    @lichess_oauth_client_secret.setter
    def lichess_oauth_client_secret(self, value: str | None) -> None:
        self.lichess.oauth_client_secret = value

    @property
    def lichess_oauth_refresh_token(self) -> str | None:
        return self.lichess.oauth_refresh_token

    @lichess_oauth_refresh_token.setter
    def lichess_oauth_refresh_token(self, value: str | None) -> None:
        self.lichess.oauth_refresh_token = value

    @property
    def lichess_oauth_token_url(self) -> str:
        return self.lichess.oauth_token_url

    @lichess_oauth_token_url.setter
    def lichess_oauth_token_url(self, value: str) -> None:
        self.lichess.oauth_token_url = value

    @property
    def chesscom_user(self) -> str:
        return self.chesscom.user

    @chesscom_user.setter
    def chesscom_user(self, value: str) -> None:
        self.chesscom.user = value

    @property
    def chesscom_token(self) -> str | None:
        return self.chesscom.token

    @chesscom_token.setter
    def chesscom_token(self, value: str | None) -> None:
        self.chesscom.token = value

    @property
    def chesscom_time_class(self) -> str:
        return self.chesscom.time_class

    @chesscom_time_class.setter
    def chesscom_time_class(self, value: str) -> None:
        self.chesscom.time_class = value

    @property
    def chesscom_max_retries(self) -> int:
        return self.chesscom.max_retries

    @chesscom_max_retries.setter
    def chesscom_max_retries(self, value: int) -> None:
        self.chesscom.max_retries = value

    @property
    def chesscom_retry_backoff_ms(self) -> int:
        return self.chesscom.retry_backoff_ms

    @chesscom_retry_backoff_ms.setter
    def chesscom_retry_backoff_ms(self, value: int) -> None:
        self.chesscom.retry_backoff_ms = value

    @property
    def chesscom_checkpoint_path(self) -> Path:
        return self.chesscom.checkpoint_path

    @chesscom_checkpoint_path.setter
    def chesscom_checkpoint_path(self, value: Path) -> None:
        self.chesscom.checkpoint_path = value

    @property
    def stockfish_path(self) -> Path:
        return self.stockfish.path

    @stockfish_path.setter
    def stockfish_path(self, value: Path) -> None:
        self.stockfish.path = value
        self._stockfish_path_overridden = True

    @property
    def stockfish_checksum(self) -> str | None:
        return self.stockfish.checksum

    @stockfish_checksum.setter
    def stockfish_checksum(self, value: str | None) -> None:
        self.stockfish.checksum = value

    @property
    def stockfish_checksum_mode(self) -> str:
        return self.stockfish.checksum_mode

    @stockfish_checksum_mode.setter
    def stockfish_checksum_mode(self, value: str) -> None:
        self.stockfish.checksum_mode = value

    @property
    def stockfish_threads(self) -> int:
        return self.stockfish.threads

    @stockfish_threads.setter
    def stockfish_threads(self, value: int) -> None:
        self.stockfish.threads = value

    @property
    def stockfish_hash_mb(self) -> int:
        return self.stockfish.hash_mb

    @stockfish_hash_mb.setter
    def stockfish_hash_mb(self, value: int) -> None:
        self.stockfish.hash_mb = value

    @property
    def stockfish_movetime_ms(self) -> int:
        return self.stockfish.movetime_ms

    @stockfish_movetime_ms.setter
    def stockfish_movetime_ms(self, value: int) -> None:
        self.stockfish.movetime_ms = value

    @property
    def stockfish_depth(self) -> int | None:
        return self.stockfish.depth

    @stockfish_depth.setter
    def stockfish_depth(self, value: int | None) -> None:
        self.stockfish.depth = value

    @property
    def stockfish_multipv(self) -> int:
        return self.stockfish.multipv

    @stockfish_multipv.setter
    def stockfish_multipv(self, value: int) -> None:
        self.stockfish.multipv = value

    @property
    def stockfish_skill_level(self) -> int:
        return self.stockfish.skill_level

    @stockfish_skill_level.setter
    def stockfish_skill_level(self, value: int) -> None:
        self.stockfish.skill_level = value

    @property
    def stockfish_limit_strength(self) -> bool:
        return self.stockfish.limit_strength

    @stockfish_limit_strength.setter
    def stockfish_limit_strength(self, value: bool) -> None:
        self.stockfish.limit_strength = value

    @property
    def stockfish_uci_elo(self) -> int | None:
        return self.stockfish.uci_elo

    @stockfish_uci_elo.setter
    def stockfish_uci_elo(self, value: int | None) -> None:
        self.stockfish.uci_elo = value

    @property
    def stockfish_uci_analyse_mode(self) -> bool:
        return self.stockfish.uci_analyse_mode

    @stockfish_uci_analyse_mode.setter
    def stockfish_uci_analyse_mode(self, value: bool) -> None:
        self.stockfish.uci_analyse_mode = value

    @property
    def stockfish_use_nnue(self) -> bool:
        return self.stockfish.use_nnue

    @stockfish_use_nnue.setter
    def stockfish_use_nnue(self, value: bool) -> None:
        self.stockfish.use_nnue = value

    @property
    def stockfish_ponder(self) -> bool:
        return self.stockfish.ponder

    @stockfish_ponder.setter
    def stockfish_ponder(self, value: bool) -> None:
        self.stockfish.ponder = value

    @property
    def stockfish_random_seed(self) -> int | None:
        return self.stockfish.random_seed

    @stockfish_random_seed.setter
    def stockfish_random_seed(self, value: int | None) -> None:
        self.stockfish.random_seed = value

    @property
    def stockfish_max_retries(self) -> int:
        return self.stockfish.max_retries

    @stockfish_max_retries.setter
    def stockfish_max_retries(self, value: int) -> None:
        self.stockfish.max_retries = value

    @property
    def stockfish_retry_backoff_ms(self) -> int:
        return self.stockfish.retry_backoff_ms

    @stockfish_retry_backoff_ms.setter
    def stockfish_retry_backoff_ms(self, value: int) -> None:
        self.stockfish.retry_backoff_ms = value
</file>

<file path="define_stockfish_settings__config.py">
"""Define Stockfish engine configuration settings."""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path

import chess
import chess.engine


def _env_str(name: str, default: str) -> str:
    return os.getenv(name, default)


def _env_str_or_none(*names: str) -> str | None:
    for name in names:
        value = os.getenv(name)
        if value:
            return value
    return None


def _env_int(name: str, default: int) -> int:
    return int(os.getenv(name, str(default)))


def _env_int_or_none(name: str) -> int | None:
    value = int(os.getenv(name, "0"))
    return value or None


def _env_bool(name: str, default: bool) -> bool:
    fallback = "1" if default else "0"
    return os.getenv(name, fallback) == "1"


@dataclass(slots=True)
class StockfishSettings:  # pylint: disable=too-many-instance-attributes
    """Stockfish engine configuration."""

    path: Path = Path(_env_str("STOCKFISH_PATH", "stockfish"))
    checksum: str | None = _env_str_or_none("STOCKFISH_SHA256", "STOCKFISH_CHECKSUM")
    checksum_mode: str = _env_str("STOCKFISH_CHECKSUM_MODE", "warn")
    threads: int = _env_int("STOCKFISH_THREADS", 1)
    hash_mb: int = _env_int("STOCKFISH_HASH", 256)
    movetime_ms: int = _env_int("STOCKFISH_MOVETIME_MS", 150)
    depth: int | None = _env_int_or_none("STOCKFISH_DEPTH")
    multipv: int = _env_int("STOCKFISH_MULTIPV", 3)
    skill_level: int = _env_int("STOCKFISH_SKILL_LEVEL", 20)
    limit_strength: bool = _env_bool("STOCKFISH_LIMIT_STRENGTH", False)
    uci_elo: int | None = _env_int_or_none("STOCKFISH_UCI_ELO")
    uci_analyse_mode: bool = _env_bool("STOCKFISH_UCI_ANALYSE_MODE", True)
    use_nnue: bool = _env_bool("STOCKFISH_USE_NNUE", True)
    ponder: bool = _env_bool("STOCKFISH_PONDER", False)
    random_seed: int | None = _env_int_or_none("STOCKFISH_RANDOM_SEED")
    max_retries: int = _env_int("STOCKFISH_MAX_RETRIES", 2)
    retry_backoff_ms: int = _env_int("STOCKFISH_RETRY_BACKOFF_MS", 250)

    @property
    def limit(self) -> chess.engine.Limit:
        """Get the analysis limit based on depth or movetime.

        Returns:
            A `chess.engine.Limit` instance configured with depth or time.
        """

        return _build_stockfish_limit(self.depth, self.movetime_ms)


def _build_stockfish_limit(depth: int | None, movetime_ms: int) -> chess.engine.Limit:
    if depth:
        return chess.engine.Limit(depth=depth)
    return chess.engine.Limit(time=movetime_ms / 1000)
</file>

<file path="define_tactic_insert_plan__db_store.py">
"""Define the tactic insert plan for persistence."""

from __future__ import annotations

from dataclasses import dataclass


@dataclass(slots=True)
class TacticInsertPlan:  # pylint: disable=too-many-instance-attributes
    """Payload for inserting a tactic row."""

    game_id: object
    position_id: object
    motif: str
    severity: object
    best_uci: object
    best_san: object
    explanation: object
    eval_cp: object
</file>

<file path="define_time_controls__const.py">
"""Supported time control labels."""

TIME_CONTROLS = frozenset({"bullet", "blitz", "rapid", "classical", "correspondence"})
</file>

<file path="detect_tactics__motifs.py">
"""Detect tactical motifs from chess positions."""

# pylint: disable=too-few-public-methods

from __future__ import annotations

from collections.abc import Iterable

import chess

from tactix._is_line_tactic import _is_line_tactic
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.CaptureDetector import CaptureDetector
from tactix.DiscoveredAttackDetector import DiscoveredAttackDetector
from tactix.DiscoveredCheckDetector import DiscoveredCheckDetector
from tactix.ForkDetector import ForkDetector
from tactix.HangingPieceDetector import HangingPieceDetector
from tactix.line_tactic_helpers import LineTacticInputs, build_line_tactic_context
from tactix.PinDetector import PinDetector
from tactix.SkewerDetector import SkewerDetector
from tactix.TacticContext import TacticContext

MISSED_DELTA_THRESHOLD = -300
FAILED_ATTEMPT_THRESHOLD = -100
BOARD_EDGE = 7
MIN_FORK_TARGETS = 2
MIN_FORK_CHECK_TARGETS = 1
ORTHOGONAL_STEPS = (1, -1, 8, -8)
DIAGONAL_STEPS = (7, -7, 9, -9)
QUEEN_STEPS = ORTHOGONAL_STEPS + DIAGONAL_STEPS
SLIDER_STEPS = {
    chess.ROOK: ORTHOGONAL_STEPS,
    chess.BISHOP: DIAGONAL_STEPS,
    chess.QUEEN: QUEEN_STEPS,
}
HIGH_VALUE_PIECES = (
    chess.QUEEN,
    chess.ROOK,
    chess.BISHOP,
    chess.KNIGHT,
)
KING_THREAT_PIECES = (
    chess.KING,
    chess.QUEEN,
    chess.ROOK,
    chess.BISHOP,
    chess.KNIGHT,
)
PIECE_VALUES = {
    chess.KING: 10000,
    chess.QUEEN: 900,
    chess.ROOK: 500,
    chess.BISHOP: 300,
    chess.KNIGHT: 300,
    chess.PAWN: 100,
}


def _is_pin_in_step(
    detector: BaseTacticDetector,
    board: chess.Board,
    start: chess.Square,
    step: int,
    opponent: bool,
) -> bool:
    return _is_line_tactic(
        build_line_tactic_context(LineTacticInputs(detector, board, start, step, opponent, False))
    )


class MateDetector(BaseTacticDetector):
    """Detect checkmate tactics."""

    motif = "mate"

    def detect(self, context: TacticContext) -> bool:
        """Return True when the move delivers checkmate."""
        return context.board_after.is_checkmate()


class CheckDetector(BaseTacticDetector):
    """Detect checking moves."""

    motif = "check"

    def detect(self, context: TacticContext) -> bool:
        """Return True when the move gives check."""
        return context.board_after.is_check()


class EscapeDetector(BaseTacticDetector):
    """Detect escape tactics where a piece escapes attack."""

    motif = "escape"

    def detect(self, context: TacticContext) -> bool:
        """Return True when the moved piece escapes attack."""
        return context.board_before.is_attacked_by(
            not context.mover_color, context.best_move.from_square
        ) and not context.board_before.is_attacked_by(
            not context.mover_color, context.best_move.to_square
        )


class MotifDetectorSuite:
    """Collection of detectors used to infer a tactic motif."""

    def __init__(self, detectors: Iterable[BaseTacticDetector]) -> None:
        """Initialize the detector suite."""
        self._detectors = tuple(detectors)

    def infer_motif(self, board: chess.Board, best_move: chess.Move | None) -> str:
        """Infer the best motif label for a move on the given board."""
        if best_move is None:
            return "initiative"
        mover_color = board.turn
        board_after = board.copy()
        board_after.push(best_move)
        context = TacticContext(
            board_before=board,
            board_after=board_after,
            best_move=best_move,
            mover_color=mover_color,
        )
        for detector in self._detectors:
            if detector.detect(context):
                return detector.motif
        return "initiative"


def build_default_motif_detector_suite() -> MotifDetectorSuite:
    """Build the default set of motif detectors."""
    return MotifDetectorSuite(
        [
            MateDetector(),
            DiscoveredAttackDetector(),
            SkewerDetector(),
            PinDetector(),
            ForkDetector(),
            DiscoveredCheckDetector(),
            HangingPieceDetector(),
            CaptureDetector(),
            CheckDetector(),
            EscapeDetector(),
        ]
    )


_VULTURE_USED = (BOARD_EDGE, SLIDER_STEPS, KING_THREAT_PIECES, _is_pin_in_step)
</file>

<file path="DiscoveredAttackContext.py">
"""Context for discovered attack detection."""

# pylint: disable=invalid-name

from __future__ import annotations

from dataclasses import dataclass

import chess

from tactix.BaseTacticDetector import BaseTacticDetector


@dataclass(frozen=True)
class DiscoveredAttackContext:
    """Inputs for discovered attack evaluation."""

    detector: BaseTacticDetector
    board_before: chess.Board
    board_after: chess.Board
    mover_color: bool
    opponent: bool
    exclude_square: chess.Square | None = None
</file>

<file path="DiscoveredAttackDetector.py">
"""Detector for discovered attack motifs."""

from __future__ import annotations

from tactix._has_discovered_attack import _has_discovered_attack
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.DiscoveredAttackContext import DiscoveredAttackContext
from tactix.TacticContext import TacticContext


class DiscoveredAttackDetector(BaseTacticDetector):
    """Detect discovered attack motifs."""

    motif = "discovered_attack"

    def detect(self, context: TacticContext) -> bool:
        """Return True when a discovered attack is present."""
        return _has_discovered_attack(
            DiscoveredAttackContext(
                detector=self,
                board_before=context.board_before,
                board_after=context.board_after,
                mover_color=context.mover_color,
                opponent=not context.mover_color,
                exclude_square=context.best_move.to_square,
            )
        )
</file>

<file path="DiscoveredCheckContext.py">
"""Context for discovered check detection."""

# pylint: disable=invalid-name

from __future__ import annotations

from dataclasses import dataclass

import chess

from tactix.BaseTacticDetector import BaseTacticDetector


@dataclass(frozen=True)
class DiscoveredCheckContext:
    """Inputs for discovered check evaluation."""

    detector: BaseTacticDetector
    board_before: chess.Board
    board_after: chess.Board
    mover_color: bool
    king_square: chess.Square
    exclude_square: chess.Square | None = None
</file>

<file path="DiscoveredCheckDetector.py">
"""Detector for discovered check motifs."""

from __future__ import annotations

from tactix._has_discovered_check import _has_discovered_check
from tactix._opponent_king_square import _opponent_king_square
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.DiscoveredCheckContext import DiscoveredCheckContext
from tactix.TacticContext import TacticContext


class DiscoveredCheckDetector(BaseTacticDetector):
    """Detect discovered checks."""

    motif = "discovered_check"

    def detect(self, context: TacticContext) -> bool:
        """Return True when a discovered check is present."""
        king_square = _opponent_king_square(context.board_after, context.mover_color)
        if king_square is None:
            return False
        return _has_discovered_check(
            DiscoveredCheckContext(
                detector=self,
                board_before=context.board_before,
                board_after=context.board_after,
                mover_color=context.mover_color,
                king_square=king_square,
                exclude_square=context.best_move.to_square,
            )
        )
</file>

<file path="empty_conversion_payload__pipeline.py">
"""Build an empty conversion payload for pipelines."""

from __future__ import annotations

from tactix.config import Settings
from tactix.conversion_payload__pipeline import _build_conversion_payload


def _empty_conversion_payload(settings: Settings) -> dict[str, object]:
    """Return a zeroed conversion payload."""
    return _build_conversion_payload(
        settings,
        games=0,
        inserted_games=0,
        positions=0,
    )
</file>

<file path="engine_result.py">
"""Normalize engine analysis results."""

from __future__ import annotations

from dataclasses import dataclass
from typing import cast

import chess
import chess.engine


@dataclass(slots=True)
class EngineResult:
    """Normalized engine analysis result."""

    best_move: chess.Move | None
    score_cp: int
    depth: int
    mate_in: int | None = None

    @classmethod
    def from_engine_result(
        cls,
        result: chess.engine.InfoDict | chess.engine.AnalysisResult | list[chess.engine.InfoDict],
        board: chess.Board | None = None,
    ) -> EngineResult:
        """Build an EngineResult from a python-chess analysis result."""
        info = _select_engine_info(result)
        pov_score = _resolve_pov_score(info.get("score"), board)
        if pov_score is None:
            return cls(best_move=None, score_cp=0, depth=0)
        value, mate_in = _score_value_and_mate(pov_score)
        best_move = _best_move_from_info(info)
        depth = _depth_from_info(info)
        return cls(best_move=best_move, score_cp=int(value or 0), depth=depth, mate_in=mate_in)

    @classmethod
    def empty(cls) -> EngineResult:
        """Return an empty engine result placeholder."""
        return cls(best_move=None, score_cp=0, depth=0)


def _select_engine_info(
    result: chess.engine.InfoDict | chess.engine.AnalysisResult | list[chess.engine.InfoDict],
) -> chess.engine.InfoDict:
    """Select the primary info dictionary from an engine result."""
    if isinstance(result, list):
        return _select_info_from_list(cast(list[chess.engine.InfoDict], result))
    return _select_info_from_single(result)


def _resolve_pov_score(
    score: chess.engine.Score | chess.engine.PovScore | None,
    board: chess.Board | None,
) -> chess.engine.Score | None:
    """Resolve a POV score using the board's turn."""
    if score is None:
        return None
    if isinstance(score, chess.engine.PovScore):
        return _pov_score_from_pov(score, board)
    return _pov_score_from_score(score, board)


def _score_value_and_mate(
    pov_score: chess.engine.Score,
) -> tuple[int, int | None]:
    """Return centipawn value and mate distance from a score."""
    mate_in = pov_score.mate()
    value = pov_score.score(mate_score=100000)
    return int(value or 0), mate_in


def _best_move_from_info(info: chess.engine.InfoDict) -> chess.Move | None:
    """Return the best move from the PV info."""
    pv = info.get("pv") or []
    return pv[0] if pv else None


def _depth_from_info(info: chess.engine.InfoDict) -> int:
    """Return the search depth from the info payload."""
    return int(info.get("depth", 0) or 0)


def _select_info_from_single(
    result: chess.engine.InfoDict | chess.engine.AnalysisResult,
) -> chess.engine.InfoDict:
    """Return the info dict from a single analysis result."""
    return result.info if isinstance(result, chess.engine.AnalysisResult) else result


def _select_info_from_list(items: list[chess.engine.InfoDict]) -> chess.engine.InfoDict:
    """Return the primary info dict from a list."""
    if not items:
        return {}
    return next((item for item in items if item.get("multipv") == 1), items[0])


def _pov_score_from_pov(
    score: chess.engine.PovScore,
    board: chess.Board | None,
) -> chess.engine.Score:
    """Convert a POV score to the board's perspective."""
    return score.pov(board.turn) if board is not None else score.relative


def _pov_score_from_score(
    score: chess.engine.Score | chess.engine.PovScore,
    board: chess.Board | None,
) -> chess.engine.Score | None:
    """Resolve a raw score to the POV score."""
    if not isinstance(score, chess.engine.Score):
        return None
    if board is not None and board.turn == chess.BLACK:
        return -score
    return score
</file>

<file path="ensure_airflow_success__airflow_jobs.py">
"""Validate Airflow job run states."""

from __future__ import annotations


def _ensure_airflow_success(state: str) -> None:
    """Raise when the airflow state indicates failure."""
    if state != "success":
        raise RuntimeError(f"Airflow run failed with state={state}")
</file>

<file path="expand_pgn_rows__pipeline.py">
from __future__ import annotations

from tactix.config import Settings
from tactix.expand_single_pgn_row__pipeline import _expand_single_pgn_row
from tactix.GameRow import GameRow


def _expand_pgn_rows(rows: list[GameRow], settings: Settings) -> list[GameRow]:
    expanded: list[GameRow] = []
    for row in rows:
        expanded.extend(_expand_single_pgn_row(row, settings))
    return expanded
</file>

<file path="expand_single_pgn_row__pipeline.py">
"""Expand a PGN row into chunk rows when needed."""

from __future__ import annotations

from tactix.build_chunk_row__pipeline import _build_chunk_row
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import SINGLE_PGN_CHUNK
from tactix.GameRow import GameRow
from tactix.split_pgn_chunks import split_pgn_chunks


def _expand_single_pgn_row(row: GameRow, settings: Settings) -> list[GameRow]:
    pgn_text = row.get("pgn", "")
    chunks = split_pgn_chunks(pgn_text)
    if len(chunks) <= SINGLE_PGN_CHUNK:
        return [row]
    return [_build_chunk_row(row, chunk, settings) for chunk in chunks]
</file>

<file path="extract_api_token__request_auth.py">
"""Extract API tokens from request headers."""

from __future__ import annotations

from fastapi import Request


def _extract_api_token(request: Request) -> str | None:
    """Return bearer token or API key from the request headers."""
    auth_header = request.headers.get("authorization")
    if auth_header and auth_header.lower().startswith("bearer "):
        return auth_header.split(" ", 1)[1].strip()
    api_key = request.headers.get("x-api-key")
    if api_key:
        return api_key.strip()
    return None
</file>

<file path="extract_game_id.py">
"""Helpers for extracting game identifiers from PGNs."""

from io import StringIO

import chess.pgn

from tactix._extract_site_id import _extract_site_id


def extract_game_id(pgn: str) -> str:
    """Extract a stable game id from a PGN string."""
    game = chess.pgn.read_game(StringIO(pgn))
    return _extract_site_id(game) or str(abs(hash(pgn)))
</file>

<file path="extract_last_timestamp_ms.py">
"""Extract timestamp metadata from PGN strings."""

import re
import time
from collections.abc import Mapping
from datetime import UTC, datetime
from io import StringIO

import chess.pgn

from tactix._parse_utc_start_ms import _parse_utc_start_ms


def extract_last_timestamp_ms(pgn: str) -> int:
    """Return the last timestamp in milliseconds for a PGN."""
    game = chess.pgn.read_game(StringIO(pgn))
    if not game:
        return int(time.time() * 1000)
    timestamp_ms = _parse_utc_header_timestamp(game.headers)
    if timestamp_ms is not None:
        return timestamp_ms
    return _parse_date_header_timestamp(game.headers)


def _parse_utc_header_timestamp(headers: Mapping[str, str]) -> int | None:
    utc_date = headers.get("UTCDate")
    utc_time = headers.get("UTCTime")
    timestamp_ms = _parse_utc_start_ms(utc_date, utc_time)
    if not timestamp_ms:
        return None
    return timestamp_ms


def _parse_date_header_timestamp(headers: Mapping[str, str]) -> int:
    date_value = headers.get("Date")
    end_time = headers.get("EndTime")
    if not date_value:
        return int(time.time() * 1000)
    try:
        dt = _parse_date_time_value(date_value, end_time)
        return int(dt.timestamp() * 1000)
    except ValueError:
        return int(time.time() * 1000)


def _parse_date_time_value(date_value: str, end_time: str | None) -> datetime:
    time_match = re.search(r"(\d{2}:\d{2}:\d{2})", end_time or "")
    if time_match:
        return datetime.strptime(
            f"{date_value} {time_match.group(1)}",
            "%Y.%m.%d %H:%M:%S",
        ).replace(tzinfo=UTC)
    return datetime.strptime(date_value, "%Y.%m.%d").replace(tzinfo=UTC)
</file>

<file path="extract_pgn_metadata.py">
"""Extract metadata from PGN headers."""

from io import StringIO

import chess.pgn

from tactix._empty_pgn_metadata import _empty_pgn_metadata
from tactix._extract_metadata_from_headers import _extract_metadata_from_headers


def extract_pgn_metadata(pgn: str, user: str) -> dict[str, object]:
    """Return parsed metadata for a PGN and user."""
    if not pgn.strip().startswith("["):
        return _empty_pgn_metadata()
    game = chess.pgn.read_game(StringIO(pgn))
    if not game or getattr(game, "errors", None):
        return _empty_pgn_metadata()
    return _extract_metadata_from_headers(game.headers, user)
</file>

<file path="extract_positions__pgn.py">
"""Extract positions from PGN data using Python or Rust fallback."""

from __future__ import annotations

import importlib
import os

from tactix._extract_positions_fallback import _extract_positions_fallback
from tactix._extract_positions_python import _extract_positions_python
from tactix.build_extractor_request import build_extractor_request
from tactix.extract_positions_with_fallback__pgn import _extract_positions_with_fallback
from tactix.extractor_context import ExtractorDependencies, ExtractorRequest
from tactix.pgn_context_kwargs import PgnContextInputs
from tactix.utils import Logger, funclogger

logger = Logger(__name__)


@funclogger
def extract_positions(
    pgn: str,
    user: str,
    source: str,
    game_id: str | None = None,
    side_to_move_filter: str | None = None,
) -> list[dict[str, object]]:
    """Extract positions from a PGN string."""
    request = build_extractor_request(
        PgnContextInputs(pgn, user, source, game_id, side_to_move_filter)
    )
    return _extract_positions_with_fallback(
        request,
        ExtractorDependencies(
            getenv=os.getenv,
            load_rust_extractor=_load_rust_extractor,
            call_rust_extractor=_call_rust_extractor,
            extract_positions_fallback=_extract_positions_fallback,
        ),
    )


@funclogger
def _load_rust_extractor():
    try:
        _core = importlib.import_module("tactix._core")
    except ImportError:  # pragma: no cover - optional Rust extension
        return None
    return getattr(_core, "extract_positions", None)


@funclogger
def _call_rust_extractor(
    rust_extractor,
    request: ExtractorRequest,
) -> list[dict[str, object]]:
    try:
        result = rust_extractor(
            request.pgn,
            request.user,
            request.source,
            request.game_id,
            request.side_to_move_filter,
        )
        return (
            _extract_positions_fallback(request)
            if not result and "[SetUp" in request.pgn
            else result
        )
    except (RuntimeError, TypeError, ValueError) as exc:  # pragma: no cover - rust fallback
        logger.warning("Rust extractor failed; falling back to Python: %s", exc)
        return _extract_positions_fallback(request)


__all__ = [
    "_call_rust_extractor",
    "_extract_positions_python",
    "_load_rust_extractor",
    "extract_positions",
]
</file>

<file path="extract_positions_for_new_games__pipeline.py">
"""Extract positions for newly ingested games."""

from __future__ import annotations

from tactix.attach_position_ids__pipeline import _attach_position_ids
from tactix.collect_game_ids__pipeline import _collect_game_ids
from tactix.config import Settings
from tactix.db.position_repository_provider import (
    fetch_position_counts,
    insert_positions,
)
from tactix.extract_positions_for_rows__pipeline import _extract_positions_for_rows
from tactix.filter_unprocessed_games__pipeline import _filter_unprocessed_games


def _extract_positions_for_new_games(
    conn, settings: Settings, raw_pgns: list[dict[str, object]]
) -> tuple[list[dict[str, object]], list[str]]:
    game_ids = _collect_game_ids(raw_pgns)
    if not game_ids:
        return [], []
    position_counts = fetch_position_counts(conn, game_ids, settings.source)
    to_process = _filter_unprocessed_games(raw_pgns, position_counts)
    if not to_process:
        return [], []
    positions = _extract_positions_for_rows(to_process, settings)
    position_ids = insert_positions(conn, positions)
    _attach_position_ids(positions, position_ids)
    return positions, _collect_game_ids(to_process)
</file>

<file path="extract_positions_for_rows__pipeline.py">
"""Extract positions from raw PGN rows."""

from __future__ import annotations

from tactix.app.use_cases.pipeline_support import _resolve_side_to_move_filter
from tactix.config import Settings
from tactix.extract_positions__pgn import extract_positions


def _extract_positions_for_rows(
    rows: list[dict[str, object]],
    settings: Settings,
) -> list[dict[str, object]]:
    """Return extracted positions for the given rows."""
    positions: list[dict[str, object]] = []
    side_to_move_filter = _resolve_side_to_move_filter(settings)
    for row in rows:
        positions.extend(
            extract_positions(
                str(row.get("pgn", "")),
                str(row.get("user") or settings.user),
                str(row.get("source") or settings.source),
                game_id=str(row.get("game_id", "")),
                side_to_move_filter=side_to_move_filter,
            )
        )
    return positions
</file>

<file path="extract_positions_from_games__pipeline.py">
"""Extract positions from fetched game PGNs."""

from __future__ import annotations

from tactix.app.use_cases.pipeline_support import _resolve_side_to_move_filter
from tactix.config import Settings
from tactix.db.position_repository_provider import insert_positions
from tactix.extract_positions__pgn import extract_positions
from tactix.GameRow import GameRow


def _extract_positions_from_games(
    conn,
    games_to_process: list[GameRow],
    settings: Settings,
) -> list[dict[str, object]]:
    side_to_move_filter = _resolve_side_to_move_filter(settings)
    positions: list[dict[str, object]] = []
    for game in games_to_process:
        positions.extend(
            extract_positions(
                game["pgn"],
                settings.user,
                settings.source,
                game_id=game["game_id"],
                side_to_move_filter=side_to_move_filter,
            )
        )
    position_ids = insert_positions(conn, positions)
    for pos, pos_id in zip(positions, position_ids, strict=False):
        pos["position_id"] = pos_id
    return positions
</file>

<file path="extract_positions_with_fallback__pgn.py">
"""Route extraction to Rust when available, otherwise use fallback."""

from __future__ import annotations

from tactix.extractor_context import ExtractorDependencies, ExtractorRequest


def _extract_positions_with_fallback(
    request: ExtractorRequest,
    deps: ExtractorDependencies,
) -> list[dict[str, object]]:
    if deps.getenv("PYTEST_CURRENT_TEST"):
        return deps.extract_positions_fallback(request)
    rust_extractor = deps.load_rust_extractor()
    if rust_extractor is None:
        return deps.extract_positions_fallback(request)
    return deps.call_rust_extractor(rust_extractor, request)
</file>

<file path="extract_positions.py">
"""Public wrapper for extracting positions from PGNs."""

from tactix._fallback_kwargs import _fallback_kwargs
from tactix.build_extractor_request import build_extractor_request
from tactix.extract_positions_with_fallback__pgn import _extract_positions_with_fallback
from tactix.pgn_context_kwargs import PgnContextInputs


def extract_positions(
    pgn: str,
    user: str,
    source: str,
    game_id: str | None = None,
    side_to_move_filter: str | None = None,
) -> list[dict[str, object]]:
    """Extract positions from PGN text with Rust/Python fallback."""
    request = build_extractor_request(
        PgnContextInputs(pgn, user, source, game_id, side_to_move_filter)
    )
    return _extract_positions_with_fallback(request, _fallback_kwargs())
</file>

<file path="extractor_context.py">
"""Context objects for position extraction dependencies."""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass


@dataclass(frozen=True)
class ExtractorRequest:
    """Input parameters for position extraction."""

    pgn: str
    user: str
    source: str
    game_id: str | None
    side_to_move_filter: str | None


@dataclass(frozen=True)
class ExtractorDependencies:
    """Dependency callbacks for position extraction."""

    getenv: Callable[[str], str | None]
    load_rust_extractor: Callable[[], object | None]
    call_rust_extractor: Callable[[object, ExtractorRequest], list[dict[str, object]]]
    extract_positions_fallback: Callable[[ExtractorRequest], list[dict[str, object]]]
</file>

<file path="fetch_analysis_tactics.py">
"""Fetch recent analysis tactics from Postgres."""

from typing import Any

from psycopg2.extras import RealDictCursor

from tactix.config import Settings
from tactix.define_db_schemas__const import ANALYSIS_SCHEMA
from tactix.init_analysis_schema import init_analysis_schema
from tactix.postgres_analysis_enabled import postgres_analysis_enabled
from tactix.postgres_connection import postgres_connection


def fetch_analysis_tactics(settings: Settings, limit: int = 10) -> list[dict[str, Any]]:
    """Return recent tactics for the given settings."""
    with postgres_connection(settings) as conn:
        if conn is None or not postgres_analysis_enabled(settings):
            return []
        init_analysis_schema(conn)
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(
                f"""
                SELECT
                    t.tactic_id,
                    t.game_id,
                    t.position_id,
                    t.motif,
                    t.severity,
                    t.best_uci,
                    t.best_san,
                    t.explanation,
                    t.eval_cp,
                    t.created_at,
                    o.result,
                    o.user_uci,
                    o.eval_delta,
                    o.created_at AS outcome_created_at
                FROM {ANALYSIS_SCHEMA}.tactics t
                LEFT JOIN {ANALYSIS_SCHEMA}.tactic_outcomes o
                    ON o.tactic_id = t.tactic_id
                ORDER BY t.created_at DESC
                LIMIT %s
                """,
                (limit,),
            )
            rows = cur.fetchall()
        return [dict(row) for row in rows]
</file>

<file path="fetch_chesscom_games__pipeline.py">
"""Fetch Chess.com games and build a fetch context."""

from __future__ import annotations

from tactix.app.use_cases.pipeline_support import _cursor_last_timestamp
from tactix.chesscom_raw_games__pipeline import _chesscom_raw_games
from tactix.config import Settings
from tactix.FetchContext import FetchContext
from tactix.infra.clients.chesscom_client import read_cursor as read_chesscom_cursor
from tactix.ports.game_source_client import GameSourceClient
from tactix.request_chesscom_games__pipeline import _request_chesscom_games


def _fetch_chesscom_games(
    settings: Settings,
    client: GameSourceClient,
    backfill_mode: bool,
) -> FetchContext:
    """Fetch Chess.com games and return a populated fetch context."""
    cursor_before = read_chesscom_cursor(settings.checkpoint_path)
    cursor_value = None if backfill_mode else cursor_before
    last_timestamp_value = _cursor_last_timestamp(cursor_value)
    chesscom_result = _request_chesscom_games(client, cursor_value, backfill_mode)
    raw_games = _chesscom_raw_games(chesscom_result)
    next_cursor = chesscom_result.next_cursor or cursor_value
    last_timestamp_value = chesscom_result.last_timestamp_ms
    return FetchContext(
        raw_games=raw_games,
        since_ms=0,
        cursor_before=cursor_before,
        cursor_value=cursor_value,
        next_cursor=next_cursor,
        chesscom_result=chesscom_result,
        last_timestamp_ms=last_timestamp_value,
    )
</file>

<file path="fetch_dag_run__airflow_api.py">
"""Fetch Airflow DAG run payloads."""

from __future__ import annotations

from typing import Any

from tactix.config import Settings
from tactix.fetch_json__airflow_api import fetch_json__airflow_api


def fetch_dag_run__airflow_api(settings: Settings, dag_id: str, run_id: str) -> dict[str, Any]:
    """Fetch a specific Airflow DAG run via the API.

    Args:
        settings: Application settings.
        dag_id: Airflow DAG identifier.
        run_id: DAG run identifier.

    Returns:
        Airflow DAG run payload.
    """
    return fetch_json__airflow_api(settings, "get", f"/api/v1/dags/{dag_id}/dagRuns/{run_id}")
</file>

<file path="fetch_incremental_games__pipeline.py">
"""Fetch incremental games for the pipeline."""

from __future__ import annotations

from tactix.config import Settings
from tactix.fetch_chesscom_games__pipeline import _fetch_chesscom_games
from tactix.fetch_lichess_games__pipeline import _fetch_lichess_games
from tactix.FetchContext import FetchContext
from tactix.ports.game_source_client import GameSourceClient


def _fetch_incremental_games(
    settings: Settings,
    client: GameSourceClient,
    backfill_mode: bool,
    window_start_ms: int | None,
    window_end_ms: int | None,
) -> FetchContext:
    """Fetch incremental games for the configured source."""
    if settings.source == "chesscom":
        return _fetch_chesscom_games(settings, client, backfill_mode)
    return _fetch_lichess_games(
        settings,
        client,
        backfill_mode,
        window_start_ms,
        window_end_ms,
    )
</file>

<file path="fetch_json__airflow_api.py">
"""Airflow API JSON request helpers."""

from __future__ import annotations

from typing import Any

import requests

from tactix.config import Settings
from tactix.gather_auth__airflow_credentials import gather_auth__airflow_credentials
from tactix.gather_url__airflow_base import gather_url__airflow_base
from tactix.prepare_error__http_status import prepare_error__http_status


def fetch_json__airflow_api(
    settings: Settings, method: str, path: str, payload: dict[str, Any] | None = None
) -> dict[str, Any]:
    """Perform an Airflow API request and return JSON.

    Args:
        settings: Application settings containing Airflow configuration.
        method: HTTP method to use.
        path: API path to request.
        payload: Optional JSON payload.

    Returns:
        Parsed JSON response.
    """
    url = f"{gather_url__airflow_base(settings)}{path}"
    response = requests.request(
        method,
        url,
        json=payload,
        auth=gather_auth__airflow_credentials(settings),
        headers={"Accept": "application/json"},
        timeout=settings.airflow_api_timeout_s,
    )
    prepare_error__http_status(response, f"Airflow API {method.upper()} {path}")
    return response.json()
</file>

<file path="fetch_lichess_games__pipeline.py">
"""Fetch Lichess games as part of the pipeline."""

from __future__ import annotations

from collections.abc import Mapping
from typing import cast

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ZERO_COUNT
from tactix.FetchContext import FetchContext
from tactix.infra.clients.lichess_client import LichessFetchRequest, read_checkpoint
from tactix.ports.game_source_client import GameSourceClient


def _fetch_lichess_games(
    settings: Settings,
    client: GameSourceClient,
    backfill_mode: bool,
    window_start_ms: int | None,
    window_end_ms: int | None,
) -> FetchContext:
    checkpoint_before = read_checkpoint(settings.checkpoint_path)
    since_ms = window_start_ms if backfill_mode else checkpoint_before
    if since_ms is None:
        since_ms = ZERO_COUNT
    until_ms = window_end_ms if backfill_mode else None
    raw_games = [
        cast(Mapping[str, object], row)
        for row in client.fetch_incremental_games(
            LichessFetchRequest(since_ms=since_ms, until_ms=until_ms)
        ).games
    ]
    return FetchContext(
        raw_games=raw_games,
        since_ms=since_ms,
        cursor_before=None,
        cursor_value=None,
        next_cursor=None,
        chesscom_result=None,
        last_timestamp_ms=since_ms,
    )
</file>

<file path="fetch_ops_events.py">
"""Fetch operations events from Postgres."""

from typing import Any

from psycopg2.extras import RealDictCursor

from tactix.config import Settings
from tactix.init_postgres_schema import init_postgres_schema
from tactix.postgres_connection import postgres_connection


def fetch_ops_events(settings: Settings, limit: int = 10) -> list[dict[str, Any]]:
    """Return recent ops events for the given settings."""
    with postgres_connection(settings) as conn:
        if conn is None:
            return []
        init_postgres_schema(conn)
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(
                """
                SELECT id, component, event_type, source, profile, metadata, created_at
                FROM tactix_ops.ops_events
                ORDER BY created_at DESC
                LIMIT %s
                """,
                (limit,),
            )
            rows = cur.fetchall()
        return [dict(row) for row in rows]
</file>

<file path="fetch_postgres_raw_pgns_summary.py">
"""Fetch raw PGN summary payloads from Postgres."""

from typing import Any

from psycopg2.extras import RealDictCursor

from tactix._build_raw_pgn_summary import (
    _build_raw_pgn_summary,
)
from tactix._disabled_raw_pgn_summary import _disabled_raw_pgn_summary
from tactix._fetch_raw_pgn_summary import _fetch_raw_pgn_summary
from tactix.config import Settings
from tactix.init_pgn_schema import init_pgn_schema
from tactix.postgres_connection import postgres_connection
from tactix.postgres_pgns_enabled import postgres_pgns_enabled


def fetch_postgres_raw_pgns_summary(settings: Settings) -> dict[str, Any]:
    """Return Postgres raw PGN summary payload."""
    with postgres_connection(settings) as conn:
        if conn is None or not postgres_pgns_enabled(settings):
            return _disabled_raw_pgn_summary()
        init_pgn_schema(conn)
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            sources, totals = _fetch_raw_pgn_summary(cur)
        return _build_raw_pgn_summary(sources, totals)
</file>

<file path="FetchContext.py">
from collections.abc import Mapping
from dataclasses import dataclass

from tactix.infra.clients.chesscom_client import ChesscomFetchResult


@dataclass(slots=True)
class FetchContext:
    """Inputs describing fetched game batches."""

    raw_games: list[Mapping[str, object]]
    since_ms: int
    cursor_before: str | None = None
    cursor_value: str | None = None
    next_cursor: str | None = None
    chesscom_result: ChesscomFetchResult | None = None
    last_timestamp_ms: int = 0
</file>

<file path="filter_backfill_games__pipeline.py">
from __future__ import annotations

from importlib import import_module

from tactix.GameRow import GameRow
from tactix.should_skip_backfill__pipeline import _should_skip_backfill


def _filter_backfill_games(
    conn,
    rows: list[GameRow],
    source: str,
) -> tuple[list[GameRow], list[GameRow]]:
    if not rows:
        return [], []
    game_ids = [row["game_id"] for row in rows]
    pipeline_module = import_module("tactix.pipeline")
    latest_hashes = pipeline_module.fetch_latest_pgn_hashes(conn, game_ids, source)
    position_counts = pipeline_module.fetch_position_counts(conn, game_ids, source)
    to_process: list[GameRow] = []
    skipped: list[GameRow] = []
    for game in rows:
        if _should_skip_backfill(game, latest_hashes, position_counts):
            skipped.append(game)
        else:
            to_process.append(game)
    return to_process, skipped
</file>

<file path="filter_games_by_window__pipeline.py">
from __future__ import annotations

from tactix.GameRow import GameRow
from tactix.within_window__pipeline import _within_window


def _filter_games_by_window(
    rows: list[GameRow],
    start_ms: int | None,
    end_ms: int | None,
) -> list[GameRow]:
    if start_ms is None and end_ms is None:
        return rows
    return [game for game in rows if _within_window(game, start_ms, end_ms)]
</file>

<file path="filter_games_for_window__pipeline.py">
"""Filter games to a requested time window."""

from __future__ import annotations

from tactix.filter_games_by_window__pipeline import _filter_games_by_window
from tactix.GameRow import GameRow


def _filter_games_for_window(
    games: list[GameRow],
    window_start_ms: int | None,
    window_end_ms: int | None,
) -> tuple[list[GameRow], int]:
    """Return filtered games and the count removed."""
    pre_window_count = len(games)
    filtered = _filter_games_by_window(games, window_start_ms, window_end_ms)
    return filtered, pre_window_count - len(filtered)
</file>

<file path="filter_positions_to_process__pipeline.py">
"""Filter positions to process based on prior analysis."""

from __future__ import annotations

from tactix.collect_game_ids__pipeline import _collect_game_ids
from tactix.config import Settings
from tactix.db.position_repository_provider import fetch_position_counts
from tactix.filter_unprocessed_games__pipeline import _filter_unprocessed_games


def _filter_positions_to_process(
    conn,
    raw_pgns: list[dict[str, object]],
    settings: Settings,
) -> list[dict[str, object]]:
    """Return raw PGNs that still need position extraction."""
    game_ids = _collect_game_ids(raw_pgns)
    position_counts = fetch_position_counts(conn, game_ids, settings.source)
    return _filter_unprocessed_games(raw_pgns, position_counts)
</file>

<file path="filter_unprocessed_games__pipeline.py">
"""Filter out games that already have positions."""

from __future__ import annotations

from tactix.define_pipeline_state__pipeline import ZERO_COUNT


def _filter_unprocessed_games(
    raw_pgns: list[dict[str, object]],
    position_counts: dict[str, int],
) -> list[dict[str, object]]:
    return [
        row
        for row in raw_pgns
        if position_counts.get(str(row.get("game_id")), ZERO_COUNT) == ZERO_COUNT
    ]
</file>

<file path="FIXTURE_SPLIT_RE.py">
"""Regex for splitting fixture payloads."""

# pylint: disable=invalid-name

import re

FIXTURE_SPLIT_RE: re.Pattern[str] = re.compile(r"\n{2,}(?=\[Event )")
</file>

<file path="ForkDetector.py">
"""Detector for fork tactics."""

from __future__ import annotations

from tactix._forks_meet_threshold import _forks_meet_threshold
from tactix._is_fork_piece import _is_fork_piece
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.TacticContext import TacticContext


class ForkDetector(BaseTacticDetector):
    """Detect fork motifs that attack multiple high-value targets."""

    motif = "fork"

    def detect(self, context: TacticContext) -> bool:
        """Return True when the move creates a fork."""
        to_square = context.best_move.to_square
        piece = context.board_after.piece_at(to_square)
        if not _is_fork_piece(piece):
            return False
        fork_targets = self.count_high_value_targets(
            context.board_after,
            to_square,
            context.mover_color,
        )
        return _forks_meet_threshold(fork_targets, context.board_after)
</file>

<file path="format_sse__api_streaming.py">
"""Format Server-Sent Events payloads."""

from __future__ import annotations

import json


def _format_sse(event: str, payload: dict[str, object]) -> bytes:
    """Return an SSE-formatted payload as bytes."""
    return f"event: {event}\ndata: {json.dumps(payload)}\n\n".encode()
</file>

<file path="format_tactics__explanation.py">
"""Format tactic explanations for UI/API payloads."""

from __future__ import annotations

from tactix._best_san_from_fen import _best_san_from_fen

_SAN_PIECE_LABELS = {
    "N": "knight",
    "B": "bishop",
    "R": "rook",
    "Q": "queen",
    "K": "king",
}


def _piece_label_from_san(san: str | None) -> str | None:
    if not san:
        return None
    if san.startswith("O-O"):
        return "king"
    return _SAN_PIECE_LABELS.get(san[0])


def format_tactic_explanation(
    fen: str | None, best_uci: str, motif: str | None
) -> tuple[str | None, str | None]:
    """Return a best SAN and explanation string for a tactic."""
    if not best_uci:
        return None, None
    best_san = _best_san_from_fen(fen, best_uci)
    line = best_san or best_uci
    piece_label = _piece_label_from_san(best_san)
    motif_label = motif or "tactic"
    if piece_label:
        explanation = f"{motif_label} tactic. Best line ({piece_label}): {line}."
    else:
        explanation = f"{motif_label} tactic. Best line: {line}."
    return best_san, explanation
</file>

<file path="GameRow.py">
from datetime import datetime
from typing import TypedDict


class GameRow(TypedDict):
    """Typed dictionary for raw game rows."""

    game_id: str
    user: str
    source: str
    fetched_at: datetime
    pgn: str
    last_timestamp_ms: int
</file>

<file path="gather_auth__airflow_credentials.py">
from __future__ import annotations

from tactix.config import Settings


def gather_auth__airflow_credentials(settings: Settings) -> tuple[str, str] | None:
    """Retrieve Airflow authentication credentials from settings.

    Args:
        settings: Application settings containing Airflow credentials.

    Returns:
        Tuple of (username, password) if both are set; otherwise, None.
    """
    if not settings.airflow_username or not settings.airflow_password:
        return None
    return (settings.airflow_username, settings.airflow_password)
</file>

<file path="gather_url__airflow_base.py">
"""Resolve the Airflow base URL from settings."""

from __future__ import annotations

from tactix.config import Settings


def gather_url__airflow_base(settings: Settings) -> str:
    """Return the Airflow base URL without any trailing slash.

    Args:
        settings: Application settings containing `airflow_base_url`.

    Returns:
        The base URL string with trailing "/" characters removed.
    """
    return settings.airflow_base_url.rstrip("/")
</file>

<file path="get_airflow_run_id__airflow_response.py">
"""Extract Airflow run ids from API responses."""

from __future__ import annotations


def _airflow_run_id(payload: dict[str, object]) -> str:
    """Return the Airflow run id from a response payload."""
    run_id = payload.get("dag_run_id") or payload.get("run_id")
    if not run_id:
        raise ValueError("Airflow response missing dag_run_id")
    return str(run_id)
</file>

<file path="get_airflow_state__airflow_jobs.py">
"""Fetch the current Airflow DAG run state."""

from __future__ import annotations

from tactix.fetch_dag_run__airflow_api import fetch_dag_run__airflow_api


def _airflow_state(settings, run_id: str) -> str:
    payload = fetch_dag_run__airflow_api(settings, "daily_game_sync", run_id)
    return str(payload.get("state") or "unknown")
</file>

<file path="get_auth_token__api.py">
"""API endpoint that returns the configured auth token."""

from __future__ import annotations

from tactix.config import get_settings


def auth_token() -> dict[str, str]:
    """Return the current API token payload."""
    settings = get_settings()
    return {
        "status": "ok",
        "token": settings.api_token,
        "token_type": "bearer",
        "user": settings.user,
    }
</file>

<file path="get_cached_dashboard_payload__api_cache.py">
from __future__ import annotations

import time as time_module

from tactix.dashboard_cache_state__api_cache import (
    _DASHBOARD_CACHE,
    _DASHBOARD_CACHE_LOCK,
    _DASHBOARD_CACHE_TTL_S,
)


def _get_cached_dashboard_payload(key: tuple[object, ...]) -> dict[str, object] | None:
    now = time_module.time()
    with _DASHBOARD_CACHE_LOCK:
        cached = _DASHBOARD_CACHE.get(key)
        if not cached:
            return None
        cached_at, payload = cached
        if now - cached_at > _DASHBOARD_CACHE_TTL_S:
            _DASHBOARD_CACHE.pop(key, None)
            return None
        _DASHBOARD_CACHE.move_to_end(key)
        return payload
</file>

<file path="get_dashboard__api.py">
"""API handler for dashboard payloads."""

from typing import Annotated

from fastapi import Depends, Query

from tactix._resolve_dashboard_filters import _resolve_dashboard_filters
from tactix.build_dashboard_cache_key__api_cache import _dashboard_cache_key
from tactix.dashboard_query import DashboardQuery
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.get_cached_dashboard_payload__api_cache import _get_cached_dashboard_payload
from tactix.pipeline import get_dashboard_payload
from tactix.set_dashboard_cache__api_cache import _set_dashboard_cache


def get_dashboard(
    filters: Annotated[DashboardQueryFilters, Depends()],
    motif: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    """Return dashboard payload for the provided filters."""
    start_datetime, end_datetime, normalized_source, settings = _resolve_dashboard_filters(
        filters,
    )
    query = DashboardQuery(
        source=normalized_source,
        motif=motif,
        rating_bucket=filters.rating_bucket,
        time_control=filters.time_control,
        start_date=start_datetime,
        end_date=end_datetime,
    )
    cache_key = _dashboard_cache_key(settings, query)
    cached = _get_cached_dashboard_payload(cache_key)
    if cached is not None:
        return cached
    payload = get_dashboard_payload(query, settings)
    _set_dashboard_cache(cache_key, payload)
    return payload


__all__ = [
    "_get_cached_dashboard_payload",
    "get_dashboard",
    "get_dashboard_payload",
]
</file>

<file path="get_dashboard_payload__pipeline.py">
"""Build dashboard payloads for API responses."""

from __future__ import annotations

from tactix.config import Settings, get_settings
from tactix.dashboard_query import DashboardQuery
from tactix.db.duckdb_store import DuckDbStore
from tactix.define_base_db_store__db_store import BaseDbStore
from tactix.define_base_db_store_context__db_store import BaseDbStoreContext
from tactix.define_pipeline_state__pipeline import logger


def get_dashboard_payload(
    query: DashboardQuery | None = None,
    settings: Settings | None = None,
    store: BaseDbStore | None = None,
) -> dict[str, object]:
    """Return dashboard payload data for the given query."""
    query = query or DashboardQuery()
    normalized_source = None if query.source in (None, "all") else query.source
    settings = _resolve_dashboard_settings(settings, normalized_source)
    if store is None:
        store = DuckDbStore(
            BaseDbStoreContext(settings=settings, logger=logger),
            db_path=settings.duckdb_path,
        )
    return store.get_dashboard_payload(
        DashboardQuery(
            source=normalized_source,
            motif=query.motif,
            rating_bucket=query.rating_bucket,
            time_control=query.time_control,
            start_date=query.start_date,
            end_date=query.end_date,
        )
    )


def _resolve_dashboard_settings(
    settings: Settings | None,
    normalized_source: str | None,
) -> Settings:
    resolved = settings or get_settings(source=normalized_source)
    if normalized_source:
        resolved.source = normalized_source
    resolved.apply_source_defaults()
    return resolved
</file>

<file path="get_dashboard_summary__api.py">
"""API handler for dashboard summary totals."""

from __future__ import annotations

from pathlib import Path
from typing import Annotated

from fastapi import Depends, Query

from tactix._resolve_dashboard_filters import _resolve_dashboard_filters
from tactix.dashboard_query import DashboardQuery
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.db.dashboard_repository_provider import fetch_pipeline_table_counts
from tactix.db.duckdb_store import get_connection, init_schema


def dashboard_summary(  # pragma: no cover
    filters: Annotated[DashboardQueryFilters, Depends()],
    db_name: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    """Return dashboard summary totals for the provided filters."""
    start_datetime, end_datetime, normalized_source, settings = _resolve_dashboard_filters(
        filters,
    )
    if db_name:
        safe_name = Path(db_name).name
        filename = safe_name if safe_name.endswith(".duckdb") else f"{safe_name}.duckdb"
        settings.duckdb_path = settings.data_dir / filename
    conn = get_connection(settings.duckdb_path)
    try:
        init_schema(conn)
        summary = fetch_pipeline_table_counts(
            conn,
            DashboardQuery(
                source=normalized_source,
                rating_bucket=filters.rating_bucket,
                time_control=filters.time_control,
                start_date=start_datetime,
                end_date=end_datetime,
            ),
        )
    finally:
        conn.close()
    response_source = "all" if normalized_source is None else normalized_source
    return {
        "source": response_source,
        "summary": summary,
    }


__all__ = ["dashboard_summary"]
</file>

<file path="get_game_detail__api.py">
"""API handler for game detail retrieval."""

from __future__ import annotations

from typing import Annotated

from fastapi import HTTPException, Query

from tactix.config import get_settings
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.db.tactic_repository_provider import tactic_repository
from tactix.normalize_source__source import _normalize_source


def game_detail(
    game_id: str,
    source: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    """Return game detail payload for a game id."""
    normalized_source = _normalize_source(source)
    settings = get_settings(source=normalized_source)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    payload = tactic_repository(conn).fetch_game_detail(
        game_id,
        settings.user,
        normalized_source,
    )
    if not payload.get("pgn"):
        raise HTTPException(status_code=404, detail="Game not found")
    return payload
</file>

<file path="get_health__api.py">
"""API health check handler."""

from tactix.utils.now import Now

_HEALTH_SERVICE = "tactix"
_HEALTH_VERSION = "0.1.0"


def health() -> dict[str, str]:
    """Return health check payload."""
    return {
        "status": "ok",
        "service": _HEALTH_SERVICE,
        "version": _HEALTH_VERSION,
        "timestamp": Now.as_datetime().isoformat(),
    }
</file>

<file path="get_job_status__api_jobs.py">
"""API handler for checking job status."""

from __future__ import annotations

import time as time_module
from typing import Annotated

from fastapi import HTTPException, Query

from tactix.check_airflow_enabled__airflow_settings import _airflow_enabled
from tactix.config import get_settings
from tactix.normalize_source__source import _normalize_source
from tactix.resolve_backfill_end_ms__airflow_jobs import _resolve_backfill_end_ms

_SUPPORTED_JOBS = {
    "daily_game_sync",
    "refresh_metrics",
    "migrations",
}


def get_job_status(
    job_id: str,
    source: Annotated[str | None, Query()] = None,
    profile: Annotated[str | None, Query()] = None,
    backfill_start_ms: Annotated[int | None, Query(ge=0)] = None,
    backfill_end_ms: Annotated[int | None, Query(ge=0)] = None,
) -> dict[str, object]:
    """Return metadata for a supported job id."""
    if job_id not in _SUPPORTED_JOBS:
        raise HTTPException(status_code=404, detail="Job not supported")
    normalized_source = _normalize_source(source)
    settings = get_settings(source=normalized_source, profile=profile)
    requested_at_ms = int(time_module.time() * 1000)
    effective_end_ms = _resolve_backfill_end_ms(
        backfill_start_ms,
        backfill_end_ms,
        requested_at_ms,
    )
    return {
        "status": "ok",
        "job": job_id,
        "job_id": job_id,
        "source": normalized_source,
        "profile": profile,
        "backfill_start_ms": backfill_start_ms,
        "backfill_end_ms": effective_end_ms,
        "requested_at_ms": requested_at_ms,
        "airflow_enabled": _airflow_enabled(settings),
    }
</file>

<file path="get_material_value.py">
"""Material value helpers for chess pieces."""

import chess

from tactix.utils import funclogger


@funclogger
def get_material_value(piece_type: chess.PieceType | str) -> int:
    """Get the material value of a given piece type."""
    if isinstance(piece_type, str):
        piece_type = chess.PIECE_SYMBOLS.index(piece_type.lower())
    return {
        chess.PAWN: 100,
        chess.KNIGHT: 300,
        chess.BISHOP: 300,
        chess.ROOK: 500,
        chess.QUEEN: 900,
    }.get(piece_type, 0)
</file>

<file path="get_postgres_analysis__api.py">
from __future__ import annotations

from fastapi import Query

from tactix.config import get_settings
from tactix.db.postgres_repository import (
    PostgresRepository,
    default_postgres_repository_dependencies,
)


def postgres_analysis(limit: int = Query(10, ge=1, le=200)) -> dict[str, object]:  # noqa: B008
    settings = get_settings()
    repo = PostgresRepository(
        settings,
        dependencies=default_postgres_repository_dependencies(),
    )
    tactics = repo.fetch_analysis_tactics(limit=limit)
    return {"status": "ok", "tactics": tactics}
</file>

<file path="get_postgres_raw_pgns__api.py">
"""API handler for Postgres raw PGN summaries."""

from __future__ import annotations

from tactix.config import get_settings
from tactix.db.postgres_repository import (
    PostgresRepository,
    default_postgres_repository_dependencies,
)


def postgres_raw_pgns() -> dict[str, object]:
    """Return the raw PGN summary from Postgres."""
    settings = get_settings()
    repo = PostgresRepository(
        settings,
        dependencies=default_postgres_repository_dependencies(),
    )
    return repo.fetch_raw_pgns_summary()
</file>

<file path="get_postgres_status__api.py">
"""API endpoint for Postgres status payloads."""

from __future__ import annotations

from fastapi import Query

from tactix.config import get_settings
from tactix.db.postgres_repository import (
    PostgresRepository,
    default_postgres_repository_dependencies,
)
from tactix.serialize_status import serialize_status


def postgres_status(limit: int = Query(10, ge=1, le=50)) -> dict[str, object]:  # noqa: B008
    """Return Postgres status and recent ops events."""
    settings = get_settings()
    repo = PostgresRepository(
        settings,
        dependencies=default_postgres_repository_dependencies(),
    )
    status = repo.get_status()
    payload = serialize_status(status)
    payload["events"] = repo.fetch_ops_events(limit=limit)
    return payload
</file>

<file path="get_postgres_status.py">
"""Resolve Postgres connectivity status."""

import time

import psycopg2

from tactix._build_schema_label import _build_schema_label
from tactix._collect_tables import _collect_tables
from tactix._connection_kwargs import _connection_kwargs
from tactix.config import Settings
from tactix.init_postgres_schema import init_postgres_schema
from tactix.postgres_enabled import postgres_enabled
from tactix.postgres_status import PostgresStatus


def get_postgres_status(settings: Settings) -> PostgresStatus:
    """Return the Postgres status for the provided settings."""
    if not postgres_enabled(settings):
        return PostgresStatus(enabled=False, status="disabled")
    kwargs = _connection_kwargs(settings)
    if not kwargs:
        return PostgresStatus(enabled=False, status="disabled")
    start = time.monotonic()
    try:
        conn = psycopg2.connect(**kwargs)
    except Exception as exc:  # noqa: BLE001
        return PostgresStatus(
            enabled=True,
            status="unreachable",
            error=str(exc),
        )
    latency_ms = (time.monotonic() - start) * 1000
    try:
        conn.autocommit = True
        init_postgres_schema(conn)
        tables = _collect_tables(conn, settings)
        schema_label = _build_schema_label(settings)
        return PostgresStatus(
            enabled=True,
            status="ok",
            latency_ms=round(latency_ms, 2),
            schema=schema_label,
            tables=tables,
        )
    finally:
        conn.close()
</file>

<file path="get_practice_next__api.py">
"""API endpoint for the next practice item."""

from __future__ import annotations

from typing import Annotated

from fastapi import Query

from tactix.config import get_settings
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.db.tactic_repository_provider import tactic_repository
from tactix.normalize_source__source import _normalize_source


def practice_next(
    source: Annotated[str | None, Query()] = None,
    include_failed_attempt: bool = Query(False),  # noqa: B008
) -> dict[str, object]:
    """Fetch the next practice item for the requested source."""
    normalized_source = _normalize_source(source)
    settings = get_settings(source=normalized_source)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    items = tactic_repository(conn).fetch_practice_queue(
        limit=1,
        source=normalized_source or settings.source,
        include_failed_attempt=include_failed_attempt,
        exclude_seen=True,
    )
    return {
        "source": normalized_source or settings.source,
        "include_failed_attempt": include_failed_attempt,
        "item": items[0] if items else None,
    }
</file>

<file path="get_practice_queue__api.py">
"""API endpoint to fetch the practice queue."""

from __future__ import annotations

from typing import Annotated

from fastapi import Query

from tactix.config import get_settings
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.db.tactic_repository_provider import tactic_repository
from tactix.normalize_source__source import _normalize_source


def practice_queue(
    source: Annotated[str | None, Query()] = None,
    include_failed_attempt: bool = Query(False),  # noqa: B008
    limit: int = Query(20, ge=1, le=200),  # noqa: B008
) -> dict[str, object]:
    """Fetch a practice queue payload for the requested source."""
    normalized_source = _normalize_source(source)
    settings = get_settings(source=normalized_source)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    queue = tactic_repository(conn).fetch_practice_queue(
        limit=limit,
        source=normalized_source or settings.source,
        include_failed_attempt=include_failed_attempt,
    )
    return {
        "source": normalized_source or settings.source,
        "include_failed_attempt": include_failed_attempt,
        "items": queue,
    }
</file>

<file path="get_raw_pgns_summary__api.py">
"""API endpoint for DuckDB raw PGN summaries."""

from __future__ import annotations

from typing import Annotated

from fastapi import Query

from tactix.config import get_settings
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.db.raw_pgn_repository_provider import fetch_raw_pgns_summary
from tactix.normalize_source__source import _normalize_source


def raw_pgns_summary(
    source: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    """Return raw PGN summary payload for the given source."""
    normalized_source = _normalize_source(source)
    settings = get_settings(source=normalized_source)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    active_source = normalized_source or settings.source
    return {
        "source": active_source,
        "summary": fetch_raw_pgns_summary(conn, source=active_source),
    }
</file>

<file path="get_settings__config.py">
"""Load settings with environment overrides."""

from __future__ import annotations

import importlib
from typing import TYPE_CHECKING

from tactix.apply_env_user_overrides__config import _apply_env_user_overrides

if TYPE_CHECKING:
    from tactix.define_settings__config import Settings


def get_settings(source: str | None = None, profile: str | None = None) -> Settings:
    """Return a Settings instance for the given source/profile."""
    config_module = importlib.import_module("tactix.config")
    defaults_module = importlib.import_module("tactix.define_config_defaults__config")
    settings_module = importlib.import_module("tactix.define_settings__config")

    config_module.load_dotenv()
    importlib.reload(defaults_module)
    settings_module = importlib.reload(settings_module)

    settings = settings_module.Settings()
    _apply_env_user_overrides(settings)
    if source:
        settings.source = source
    settings.apply_source_defaults()
    settings.apply_lichess_profile(profile)
    settings.apply_chesscom_profile(profile)
    settings.ensure_dirs()
    return settings
</file>

<file path="get_tactics_search__api.py">
"""API handler for tactics search."""

from __future__ import annotations

from typing import Annotated

from fastapi import Depends, Query

from tactix.coerce_date_to_datetime__datetime import _coerce_date_to_datetime
from tactix.config import get_settings
from tactix.dashboard_query import DashboardQuery
from tactix.db.dashboard_repository_provider import fetch_recent_tactics
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.normalize_source__source import _normalize_source
from tactix.tactics_search_filters import TacticsSearchFilters


def tactics_search(
    filters: Annotated[TacticsSearchFilters, Depends()],
    limit: int = Query(20, ge=1, le=200),  # noqa: B008
) -> dict[str, object]:
    """Fetch recent tactics that match the given filters."""
    normalized_source = _normalize_source(filters.source)
    settings = get_settings(source=normalized_source)
    start_datetime = _coerce_date_to_datetime(filters.start_date)
    end_datetime = _coerce_date_to_datetime(filters.end_date, end_of_day=True)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    tactics = fetch_recent_tactics(
        conn,
        DashboardQuery(
            source=normalized_source,
            motif=filters.motif,
            rating_bucket=filters.rating_bucket,
            time_control=filters.time_control,
            start_date=start_datetime,
            end_date=end_datetime,
        ),
        limit=limit,
    )
    response_source = "all" if normalized_source is None else normalized_source
    return {"source": response_source, "limit": limit, "tactics": tactics}
</file>

<file path="HangingPieceDetector.py">
"""Detector for hanging piece tactics."""

from __future__ import annotations

from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.TacticContext import TacticContext


class HangingPieceDetector(BaseTacticDetector):
    """Detect captures that win a hanging piece or trade."""

    motif = "hanging_piece"

    def detect(self, context: TacticContext) -> bool:
        """Return True when the move captures a hanging piece."""
        return self.is_hanging_capture(
            context.board_before,
            context.board_after,
            context.best_move,
            context.mover_color,
        )
</file>

<file path="init_analysis_schema_if_needed__pipeline.py">
"""Initialize analysis schema when Postgres is enabled."""

from __future__ import annotations

from tactix.init_analysis_schema import init_analysis_schema


def _init_analysis_schema_if_needed(pg_conn, analysis_pg_enabled: bool) -> None:
    if pg_conn is not None and analysis_pg_enabled:
        init_analysis_schema(pg_conn)
</file>

<file path="init_analysis_schema.py">
"""Initialize analysis schema objects in Postgres."""

from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix.define_db_schemas__const import ANALYSIS_SCHEMA


def init_analysis_schema(conn: PgConnection) -> None:
    """Create analysis schema tables and indices if missing."""
    with conn.cursor() as cur:
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {ANALYSIS_SCHEMA}")
        cur.execute(f"CREATE SEQUENCE IF NOT EXISTS {ANALYSIS_SCHEMA}.tactics_tactic_id_seq")
        cur.execute(
            f"CREATE SEQUENCE IF NOT EXISTS {ANALYSIS_SCHEMA}.tactic_outcomes_outcome_id_seq"
        )
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {ANALYSIS_SCHEMA}.positions (
                position_id BIGINT PRIMARY KEY,
                game_id TEXT,
                source TEXT,
                fen TEXT,
                ply INTEGER,
                move_number INTEGER,
                side_to_move TEXT,
                uci TEXT,
                san TEXT,
                clock_seconds DOUBLE PRECISION,
                is_legal BOOLEAN,
                created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
            )
            """
        )
        cur.execute(
            f"CREATE INDEX IF NOT EXISTS positions_game_idx "
            f"ON {ANALYSIS_SCHEMA}.positions (game_id)"
        )
        cur.execute(
            f"CREATE INDEX IF NOT EXISTS positions_created_at_idx "
            f"ON {ANALYSIS_SCHEMA}.positions (created_at DESC)"
        )
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {ANALYSIS_SCHEMA}.tactics (
                tactic_id BIGINT PRIMARY KEY
                    DEFAULT nextval('{ANALYSIS_SCHEMA}.tactics_tactic_id_seq'),
                game_id TEXT,
                position_id BIGINT NOT NULL,
                motif TEXT,
                severity DOUBLE PRECISION,
                best_uci TEXT,
                best_san TEXT,
                explanation TEXT,
                eval_cp INTEGER,
                created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
            )
            """
        )
        cur.execute(
            f"CREATE INDEX IF NOT EXISTS tactics_position_idx "
            f"ON {ANALYSIS_SCHEMA}.tactics (position_id)"
        )
        cur.execute(
            f"CREATE INDEX IF NOT EXISTS tactics_created_at_idx "
            f"ON {ANALYSIS_SCHEMA}.tactics (created_at DESC)"
        )
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {ANALYSIS_SCHEMA}.tactic_outcomes (
                outcome_id BIGINT PRIMARY KEY
                    DEFAULT nextval('{ANALYSIS_SCHEMA}.tactic_outcomes_outcome_id_seq'),
                tactic_id BIGINT NOT NULL
                    REFERENCES {ANALYSIS_SCHEMA}.tactics(tactic_id) ON DELETE CASCADE,
                result TEXT,
                user_uci TEXT,
                eval_delta INTEGER,
                created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
            )
            """
        )
        cur.execute(
            f"CREATE INDEX IF NOT EXISTS tactic_outcomes_created_at_idx "
            f"ON {ANALYSIS_SCHEMA}.tactic_outcomes (created_at DESC)"
        )
</file>

<file path="init_pgn_schema.py">
"""Initialize Postgres schema for PGN storage."""

from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix.define_db_schemas__const import PGN_SCHEMA


def init_pgn_schema(conn: PgConnection) -> None:
    """Ensure the PGN schema and tables exist."""
    with conn.cursor() as cur:
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {PGN_SCHEMA}")
        cur.execute(f"CREATE SEQUENCE IF NOT EXISTS {PGN_SCHEMA}.raw_pgns_raw_pgn_id_seq")
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {PGN_SCHEMA}.raw_pgns (
                raw_pgn_id BIGINT PRIMARY KEY
                    DEFAULT nextval('{PGN_SCHEMA}.raw_pgns_raw_pgn_id_seq'),
                game_id TEXT NOT NULL,
                source TEXT NOT NULL,
                player_username TEXT,
                fetched_at TIMESTAMPTZ,
                pgn_raw TEXT,
                pgn_normalized TEXT,
                pgn_hash TEXT,
                pgn_version INTEGER,
                user_rating INTEGER,
                time_control TEXT,
                ingested_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                last_timestamp_ms BIGINT,
                cursor TEXT,
                white_player TEXT,
                black_player TEXT,
                white_elo INTEGER,
                black_elo INTEGER,
                result TEXT,
                event TEXT,
                site TEXT,
                utc_date TEXT,
                utc_time TEXT,
                termination TEXT,
                start_timestamp_ms BIGINT
            )
            """
        )
        cur.execute(
            f"""
            CREATE UNIQUE INDEX IF NOT EXISTS raw_pgns_game_version_uniq
            ON {PGN_SCHEMA}.raw_pgns (game_id, source, pgn_version)
            """
        )
        cur.execute(
            f"""
            CREATE UNIQUE INDEX IF NOT EXISTS raw_pgns_game_hash_uniq
            ON {PGN_SCHEMA}.raw_pgns (game_id, source, pgn_hash)
            """
        )
        cur.execute(
            f"""
            CREATE INDEX IF NOT EXISTS raw_pgns_source_ts_idx
            ON {PGN_SCHEMA}.raw_pgns (source, last_timestamp_ms DESC)
            """
        )
        cur.execute(
            f"""
            CREATE INDEX IF NOT EXISTS raw_pgns_source_result_idx
            ON {PGN_SCHEMA}.raw_pgns (source, result)
            """
        )
        cur.execute(
            f"""
            CREATE INDEX IF NOT EXISTS raw_pgns_source_time_control_idx
            ON {PGN_SCHEMA}.raw_pgns (source, time_control)
            """
        )
        cur.execute(
            f"""
            CREATE INDEX IF NOT EXISTS raw_pgns_player_idx
            ON {PGN_SCHEMA}.raw_pgns (player_username)
            """
        )
</file>

<file path="init_postgres_schema.py">
"""Initialize Postgres ops schema."""

from psycopg2.extensions import connection as PgConnection  # noqa: N812


def init_postgres_schema(conn: PgConnection) -> None:
    """Ensure the ops_events table exists."""
    with conn.cursor() as cur:
        cur.execute("CREATE SCHEMA IF NOT EXISTS tactix_ops")
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS tactix_ops.ops_events (
                id BIGSERIAL PRIMARY KEY,
                component TEXT NOT NULL,
                event_type TEXT NOT NULL,
                source TEXT,
                profile TEXT,
                metadata JSONB,
                created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
            )
            """
        )
        cur.execute(
            "CREATE INDEX IF NOT EXISTS ops_events_created_at_idx "
            "ON tactix_ops.ops_events (created_at DESC)"
        )
</file>

<file path="is_backfill_mode__pipeline.py">
"""Helpers for determining backfill mode."""

from __future__ import annotations


def _is_backfill_mode(window_start_ms: int | None, window_end_ms: int | None) -> bool:
    """Return True when a backfill window is provided."""
    return window_start_ms is not None or window_end_ms is not None
</file>

<file path="is_latest_hash__db_store.py">
from __future__ import annotations


def _is_latest_hash(latest_hash: str | None, pgn_hash: str) -> bool:
    return latest_hash == pgn_hash
</file>

<file path="job_stream.py">
"""Job stream contexts, helpers, and API endpoints."""

from __future__ import annotations

import time as time_module
from collections.abc import Callable, Iterator
from dataclasses import dataclass
from datetime import datetime
from queue import Empty, Queue
from threading import Thread
from typing import Annotated, cast

from fastapi import Depends, Query
from fastapi.encoders import jsonable_encoder
from fastapi.responses import StreamingResponse

from tactix._resolve_dashboard_filters import _resolve_dashboard_filters
from tactix.airflow_daily_sync_context import AirflowDailySyncTriggerContext
from tactix.check_airflow_enabled__airflow_settings import _airflow_enabled
from tactix.config import Settings, get_settings
from tactix.dashboard_query import DashboardQuery
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.ensure_airflow_success__airflow_jobs import _ensure_airflow_success
from tactix.format_sse__api_streaming import _format_sse
from tactix.get_airflow_state__airflow_jobs import _airflow_state
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.list_sources_for_cache_refresh__api_cache import _sources_for_cache_refresh
from tactix.normalize_source__source import _normalize_source
from tactix.pipeline import (
    get_dashboard_payload,
    run_daily_game_sync,
    run_migrations,
    run_refresh_metrics,
)
from tactix.raise_unsupported_job__api_jobs import _raise_unsupported_job
from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async
from tactix.resolve_backfill_end_ms__airflow_jobs import _resolve_backfill_end_ms
from tactix.sync_contexts import DailyGameSyncRequest
from tactix.trigger_airflow_daily_sync__airflow_jobs import _trigger_airflow_daily_sync
from tactix.utils import funclogger
from tactix.utils.logger import Logger

logger = Logger("tactix.api")


@dataclass(frozen=True)
class StreamJobRequest:
    """Incoming stream job request parameters."""

    job: str
    source: str | None
    profile: str | None
    backfill_start_ms: int | None
    backfill_end_ms: int | None


@dataclass(frozen=True)
class BackfillWindow:
    """Common backfill window parameters."""

    source: str | None
    profile: str | None
    backfill_start_ms: int | None
    backfill_end_ms: int | None
    triggered_at_ms: int


class _BackfillWindowAccessors:
    window: BackfillWindow

    @property
    def source(self) -> str | None:
        return self.window.source

    @property
    def profile(self) -> str | None:
        return self.window.profile

    @property
    def backfill_start_ms(self) -> int | None:
        return self.window.backfill_start_ms

    @property
    def backfill_end_ms(self) -> int | None:
        return self.window.backfill_end_ms

    @property
    def triggered_at_ms(self) -> int:
        return self.window.triggered_at_ms


@dataclass(frozen=True)
class StreamJobWorkerContext(_BackfillWindowAccessors):
    """Context for streaming job worker execution."""

    settings: Settings
    queue: Queue[object]
    sentinel: object
    job: str
    window: BackfillWindow


@dataclass(frozen=True)
class StreamJobRunContext(_BackfillWindowAccessors):
    """Context for executing a stream job run."""

    settings: Settings
    queue: Queue[object]
    job: str
    window: BackfillWindow
    progress: Callable[[dict[str, object]], None]


@dataclass(frozen=True)
class AirflowDailySyncContext(_BackfillWindowAccessors):
    """Context for airflow daily sync jobs."""

    settings: Settings
    queue: Queue[object]
    job: str
    window: BackfillWindow


@dataclass(frozen=True)
class MetricsFilters:
    """Filters used for streaming metrics results."""

    normalized_source: str | None
    motif: str | None
    rating_bucket: str | None
    time_control: str | None
    start_date: datetime | None
    end_date: datetime | None


@dataclass(frozen=True)
class MetricsStreamContext:
    """Context for streaming metrics results."""

    queue: Queue[object]
    sentinel: object
    settings: Settings
    filters: MetricsFilters

    @property
    def normalized_source(self) -> str | None:
        return self.filters.normalized_source

    @property
    def motif(self) -> str | None:
        return self.filters.motif

    @property
    def rating_bucket(self) -> str | None:
        return self.filters.rating_bucket

    @property
    def time_control(self) -> str | None:
        return self.filters.time_control

    @property
    def start_date(self) -> datetime | None:
        return self.filters.start_date

    @property
    def end_date(self) -> datetime | None:
        return self.filters.end_date


@funclogger
def build_stream_job_kwargs(
    *,
    job: str,
    source: str | None,
    profile: str | None,
    backfill_start_ms: int | None,
    backfill_end_ms: int | None,
) -> dict[str, object]:
    """Build a kwargs dict for stream job contexts."""
    return {
        "job": job,
        "source": source,
        "profile": profile,
        "backfill_start_ms": backfill_start_ms,
        "backfill_end_ms": backfill_end_ms,
    }


@funclogger
def build_stream_job_request(
    *,
    job: str,
    source: str | None,
    profile: str | None,
    backfill_start_ms: int | None,
    backfill_end_ms: int | None,
) -> StreamJobRequest:
    """Create a StreamJobRequest from common inputs."""
    return StreamJobRequest(
        **build_stream_job_kwargs(
            job=job,
            source=source,
            profile=profile,
            backfill_start_ms=backfill_start_ms,
            backfill_end_ms=backfill_end_ms,
        )
    )


def build_stream_job_request_from_values(
    values: dict[str, object],
) -> StreamJobRequest:
    """Create a StreamJobRequest from a values mapping."""
    return StreamJobRequest(
        job=cast(str, values["job"]),
        source=cast(str | None, values["source"]),
        profile=cast(str | None, values["profile"]),
        backfill_start_ms=cast(int | None, values["backfill_start_ms"]),
        backfill_end_ms=cast(int | None, values["backfill_end_ms"]),
    )


BACKFILL_WINDOW_KEYS = (
    "source",
    "profile",
    "backfill_start_ms",
    "backfill_end_ms",
    "triggered_at_ms",
)


@funclogger
def _noop_progress(_payload: dict[str, object]) -> None:
    return None


def _queue_job_event(
    queue: Queue[object],
    event: str,
    job: str,
    payload: dict[str, object],
) -> None:
    payload["job"] = job
    payload["job_id"] = job
    queue.put((event, payload))


def _queue_job_complete(
    queue: Queue[object],
    job: str,
    message: str,
    result: dict[str, object] | None = None,
) -> None:
    payload: dict[str, object] = {
        "step": "complete",
        "message": message,
    }
    if result is not None:
        payload["result"] = result
    _queue_job_event(queue, "complete", job, payload)


def _queue_job_error(queue: Queue[object], job: str, message: str) -> None:
    _queue_job_event(
        queue,
        "error",
        job,
        {
            "step": "error",
            "message": message,
        },
    )


def _queue_progress(
    queue: Queue[object],
    job: str,
    step: str,
    message: str | None = None,
    extra: dict[str, object] | None = None,
) -> None:
    payload: dict[str, object] = {
        "job": job,
        "step": step,
        "timestamp": int(time_module.time()),
    }
    if message:
        payload["message"] = message
    if extra:
        payload.update(extra)
    queue.put(("progress", payload))


def _queue_backfill_window(
    queue: Queue[object],
    job: str,
    backfill_start_ms: int | None,
    backfill_end_ms: int | None,
    triggered_at_ms: int,
) -> None:
    if backfill_start_ms is None and backfill_end_ms is None:
        return
    _queue_progress(
        queue,
        job,
        "backfill_window",
        message="Backfill window resolved",
        extra={
            "backfill_start_ms": backfill_start_ms,
            "backfill_end_ms": backfill_end_ms,
            "triggered_at_ms": triggered_at_ms,
        },
    )


def _resolve_backfill_window(
    backfill_start_ms: int | None,
    backfill_end_ms: int | None,
) -> tuple[int, int | None]:
    triggered_at_ms = int(time_module.time() * 1000)
    effective_end_ms = _resolve_backfill_end_ms(
        backfill_start_ms,
        backfill_end_ms,
        triggered_at_ms,
    )
    return triggered_at_ms, effective_end_ms


def _event_stream(queue: Queue[object], sentinel: object) -> Iterator[bytes]:
    yield b"retry: 1000\n\n"
    while True:
        try:
            item = queue.get(timeout=1)
        except Empty:
            yield b": keep-alive\n\n"
            continue
        if item is sentinel:
            break
        event, payload = cast(tuple[str, dict[str, object]], item)
        yield _format_sse(event, payload)


def _streaming_response(queue: Queue[object], sentinel: object) -> StreamingResponse:
    """Return a streaming response for the job event stream."""
    return StreamingResponse(
        _event_stream(queue, sentinel),
        media_type="text/event-stream",
    )


@funclogger
def _collect_stream_job_values(
    args: tuple[object, ...],
    legacy: dict[str, object],
) -> dict[str, object]:
    ordered_keys = ("queue", "job", *BACKFILL_WINDOW_KEYS, "progress")
    values = init_legacy_values(ordered_keys)
    apply_legacy_kwargs(values, ordered_keys, legacy)
    apply_legacy_args(values, ordered_keys, args)
    return values


@funclogger
def _build_stream_job_context(
    settings: Settings,
    values: dict[str, object],
) -> StreamJobRunContext:
    if values["queue"] is None or values["job"] is None or values["triggered_at_ms"] is None:
        raise TypeError("settings, queue, job, and triggered_at_ms are required")
    queue = cast(Queue[object], values["queue"])
    triggered_at_ms = cast(int, values["triggered_at_ms"])
    progress = cast(ProgressCallback | None, values["progress"])
    if progress is None:
        progress = _noop_progress
    request = build_stream_job_request_from_values(values)
    return StreamJobRunContext(
        settings=settings,
        queue=queue,
        job=request.job,
        window=BackfillWindow(
            source=request.source,
            profile=request.profile,
            backfill_start_ms=request.backfill_start_ms,
            backfill_end_ms=request.backfill_end_ms,
            triggered_at_ms=triggered_at_ms,
        ),
        progress=progress,
    )


def _run_airflow_daily_sync_job(
    context: AirflowDailySyncContext,
) -> dict[str, object]:
    """Trigger an Airflow daily sync run and report progress."""
    _queue_progress(
        context.queue,
        context.job,
        "start",
        message="Starting Airflow daily_game_sync",
    )
    _queue_backfill_window(
        context.queue,
        context.job,
        context.backfill_start_ms,
        context.backfill_end_ms,
        context.triggered_at_ms,
    )
    run_id = _trigger_airflow_daily_sync(
        context.settings,
        context.source,
        context.profile,
        backfill_start_ms=context.backfill_start_ms,
        backfill_end_ms=context.backfill_end_ms,
        triggered_at_ms=context.triggered_at_ms,
    )
    _queue_progress(
        context.queue,
        context.job,
        "airflow_triggered",
        message="Airflow DAG triggered",
        extra={"run_id": run_id},
    )
    state = _wait_for_airflow_run(context.settings, context.queue, context.job, run_id)
    _ensure_airflow_success(state)
    payload = get_dashboard_payload(
        DashboardQuery(source=context.source),
        get_settings(source=context.source),
    )
    _queue_progress(
        context.queue,
        context.job,
        "fetch_games",
        message="Airflow DAG completed game ingestion",
    )
    _queue_progress(
        context.queue,
        context.job,
        "raw_pgns",
        message="Airflow DAG completed raw PGN fetch",
    )
    _queue_progress(
        context.queue,
        context.job,
        "raw_pgns_persisted",
        message="Airflow DAG persisted raw PGNs",
    )
    _queue_progress(
        context.queue,
        context.job,
        "extract_positions",
        message="Airflow DAG extracted positions",
    )
    _queue_progress(
        context.queue,
        context.job,
        "positions_ready",
        message="Airflow DAG stored positions",
    )
    _queue_progress(
        context.queue,
        context.job,
        "analyze_positions",
        message="Airflow DAG analyzed tactics",
    )
    _queue_progress(
        context.queue,
        context.job,
        "metrics_refreshed",
        message="Airflow DAG refreshed metrics",
        extra={"metrics_version": payload.get("metrics_version")},
    )
    logger.info(
        "Airflow daily_game_sync completed; metrics_version=%s",
        payload.get("metrics_version"),
    )
    return {
        "airflow_run_id": run_id,
        "state": state,
        "metrics_version": payload.get("metrics_version"),
    }


def _wait_for_airflow_run(
    settings: Settings,
    queue: Queue[object],
    job: str,
    run_id: str,
) -> str:
    start = time_module.time()
    last_state: str | None = None
    while True:
        state = _airflow_state(settings, run_id)
        if state != last_state:
            _queue_progress(
                queue,
                job,
                "airflow_state",
                message=state,
                extra={"state": state, "run_id": run_id},
            )
            last_state = state
        if state in {"success", "failed"}:
            return state
        if time_module.time() - start > settings.airflow_poll_timeout_s:
            raise TimeoutError("Airflow run timed out")
        time_module.sleep(settings.airflow_poll_interval_s)


@funclogger
def _run_stream_job(
    context: StreamJobRunContext | Settings,
    *args: object,
    **legacy: object,
) -> dict[str, object]:
    if isinstance(context, StreamJobRunContext):
        ctx = context
    else:
        values = _collect_stream_job_values(args, legacy)
        ctx = _build_stream_job_context(context, values)

    def run_daily_sync() -> dict[str, object]:
        if _airflow_enabled(ctx.settings):
            return _run_airflow_daily_sync_job(
                AirflowDailySyncContext(
                    settings=ctx.settings,
                    queue=ctx.queue,
                    job=ctx.job,
                    window=ctx.window,
                )
            )
        return run_daily_game_sync(
            DailyGameSyncRequest(
                settings=ctx.settings,
                source=ctx.source,
                progress=ctx.progress,
                profile=ctx.profile,
                window_start_ms=ctx.backfill_start_ms,
                window_end_ms=ctx.backfill_end_ms,
            )
        )

    handlers: dict[str, Callable[[], dict[str, object]]] = {
        "daily_game_sync": run_daily_sync,
        "refresh_metrics": lambda: run_refresh_metrics(
            ctx.settings,
            source=ctx.source,
            progress=ctx.progress,
        ),
        "migrations": lambda: run_migrations(
            ctx.settings,
            source=ctx.source,
            progress=ctx.progress,
        ),
    }
    handler = handlers.get(ctx.job)
    if handler is None:
        _raise_unsupported_job(ctx.job)
    return cast(Callable[[], dict[str, object]], handler)()


def _stream_job_worker(
    context: StreamJobWorkerContext,
) -> None:
    def progress(payload: dict[str, object]) -> None:
        _queue_job_event(context.queue, "progress", context.job, payload)

    try:
        result = _run_stream_job(
            StreamJobRunContext(
                settings=context.settings,
                queue=context.queue,
                job=context.job,
                window=context.window,
                progress=progress,
            )
        )
        if context.job in {"daily_game_sync", "refresh_metrics"}:
            _refresh_dashboard_cache_async(_sources_for_cache_refresh(context.source))
        _queue_job_complete(context.queue, context.job, "Job complete", result=result)
    except Exception as exc:  # pragma: no cover - defensive
        _queue_job_error(context.queue, context.job, str(exc))
    finally:
        context.queue.put(context.sentinel)


def _stream_job_response(
    request: StreamJobRequest,
    *,
    settings_factory: Callable[..., Settings] = get_settings,
) -> StreamingResponse:
    normalized_source = _normalize_source(request.source)
    settings = settings_factory(source=normalized_source, profile=request.profile)
    queue: Queue[object] = Queue()
    sentinel = object()
    triggered_at_ms, effective_end_ms = _resolve_backfill_window(
        request.backfill_start_ms,
        request.backfill_end_ms,
    )
    Thread(
        target=_stream_job_worker,
        args=(
            StreamJobWorkerContext(
                settings=settings,
                queue=queue,
                sentinel=sentinel,
                job=request.job,
                window=BackfillWindow(
                    source=normalized_source,
                    profile=request.profile,
                    backfill_start_ms=request.backfill_start_ms,
                    backfill_end_ms=effective_end_ms,
                    triggered_at_ms=triggered_at_ms,
                ),
            ),
        ),
        daemon=True,
    ).start()

    return _streaming_response(queue, sentinel)


@funclogger
def stream_jobs(  # pragma: no cover
    job: Annotated[str, Query()] = "daily_game_sync",
    source: Annotated[str | None, Query()] = None,
    profile: Annotated[str | None, Query()] = None,
    backfill_start_ms: Annotated[int | None, Query(ge=0)] = None,
    backfill_end_ms: Annotated[int | None, Query(ge=0)] = None,
) -> StreamingResponse:
    """Stream job events for the requested background task."""
    return _stream_job_response(
        StreamJobRequest(job, source, profile, backfill_start_ms, backfill_end_ms),
        settings_factory=get_settings,
    )


def stream_job_by_id(
    job_id: str,
    source: Annotated[str | None, Query()] = None,
    profile: Annotated[str | None, Query()] = None,
    backfill_start_ms: Annotated[int | None, Query(ge=0)] = None,
    backfill_end_ms: Annotated[int | None, Query(ge=0)] = None,
) -> StreamingResponse:
    """Return a streaming response for the requested job id."""
    return _stream_job_response(
        StreamJobRequest(
            job=job_id,
            source=source,
            profile=profile,
            backfill_start_ms=backfill_start_ms,
            backfill_end_ms=backfill_end_ms,
        )
    )


def _stream_metrics_worker(  # pragma: no cover
    context: MetricsStreamContext,
) -> None:
    """Run metrics refresh and emit streaming job events."""

    def progress(payload: dict[str, object]) -> None:
        _queue_job_event(context.queue, "progress", "refresh_metrics", payload)

    try:
        result = run_refresh_metrics(
            context.settings,
            source=context.normalized_source,
            progress=progress,
        )
        _refresh_dashboard_cache_async(_sources_for_cache_refresh(context.normalized_source))
        payload = get_dashboard_payload(
            DashboardQuery(
                source=context.normalized_source,
                motif=context.motif,
                rating_bucket=context.rating_bucket,
                time_control=context.time_control,
                start_date=context.start_date,
                end_date=context.end_date,
            ),
            context.settings,
        )
        metrics_payload = jsonable_encoder(
            {
                "step": "metrics_update",
                "job": "refresh_metrics",
                "job_id": "refresh_metrics",
                "source": payload.get("source"),
                "metrics_version": payload.get("metrics_version"),
                "metrics": payload.get("metrics"),
            }
        )
        _queue_job_event(context.queue, "metrics_update", "refresh_metrics", metrics_payload)
        _queue_job_complete(
            context.queue,
            "refresh_metrics",
            "Metrics refresh complete",
            result=result,
        )
    except Exception as exc:  # pragma: no cover - defensive
        _queue_job_error(context.queue, "refresh_metrics", str(exc))
    finally:
        context.queue.put(context.sentinel)


def stream_metrics(
    filters: Annotated[DashboardQueryFilters, Depends()],
    motif: Annotated[str | None, Query()] = None,
) -> StreamingResponse:
    """Stream metrics updates based on dashboard filters."""
    start_datetime, end_datetime, normalized_source, settings = _resolve_dashboard_filters(
        filters,
    )
    queue: Queue[object] = Queue()
    sentinel = object()
    context = MetricsStreamContext(
        queue=queue,
        sentinel=sentinel,
        settings=settings,
        filters=MetricsFilters(
            normalized_source=normalized_source,
            motif=motif,
            rating_bucket=filters.rating_bucket,
            time_control=filters.time_control,
            start_date=start_datetime,
            end_date=end_datetime,
        ),
    )

    Thread(
        target=_stream_metrics_worker,
        args=(context,),
        daemon=True,
    ).start()

    return StreamingResponse(
        _event_stream(queue, sentinel),
        media_type="text/event-stream",
    )


_VULTURE_USED = (build_stream_job_request,)

__all__ = [
    "BACKFILL_WINDOW_KEYS",
    "AirflowDailySyncContext",
    "AirflowDailySyncTriggerContext",
    "MetricsStreamContext",
    "StreamJobRequest",
    "StreamJobRunContext",
    "StreamJobWorkerContext",
    "_event_stream",
    "_queue_backfill_window",
    "_queue_job_complete",
    "_queue_job_error",
    "_queue_job_event",
    "_queue_progress",
    "_resolve_backfill_window",
    "_run_airflow_daily_sync_job",
    "_run_stream_job",
    "_stream_job_response",
    "_stream_job_worker",
    "_stream_metrics_worker",
    "_streaming_response",
    "_wait_for_airflow_run",
    "build_stream_job_kwargs",
    "build_stream_job_request",
    "build_stream_job_request_from_values",
    "stream_job_by_id",
    "stream_jobs",
    "stream_metrics",
]
</file>

<file path="latest_timestamp.py">
"""Utilities for working with timestamps."""

from collections.abc import Iterable, Mapping


def latest_timestamp(rows: Iterable[Mapping[str, object]]) -> int:
    """Return the latest timestamp value from row mappings."""
    ts = 0
    for row in rows:
        value = row.get("last_timestamp_ms", 0)
        if isinstance(value, (int, float, bool)):
            current = int(value)
        elif isinstance(value, str):
            try:
                current = int(value)
            except ValueError:
                current = 0
        else:
            current = 0
        ts = max(ts, current)
    return ts
</file>

<file path="legacy_args.py">
"""Helpers for resolving legacy positional/keyword arguments."""

from __future__ import annotations

from typing import Any


def init_legacy_values(
    ordered_keys: tuple[str, ...],
    initial: dict[str, object] | None = None,
) -> dict[str, object]:
    """Initialize a values dictionary for legacy argument parsing."""
    values: dict[str, object] = dict.fromkeys(ordered_keys)
    if initial:
        values.update(initial)
    return values


def apply_legacy_kwargs(
    values: dict[str, object],
    ordered_keys: tuple[str, ...],
    legacy: dict[str, object],
) -> None:
    """Apply legacy keyword arguments into a values mapping."""
    for key in ordered_keys:
        if key in legacy:
            values[key] = legacy.pop(key)
    if legacy:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(legacy))}")


def apply_legacy_args(
    values: dict[str, object],
    ordered_keys: tuple[str, ...],
    args: tuple[Any, ...],
) -> None:
    """Apply positional legacy arguments into a values mapping."""
    if not args:
        return
    if len(args) > len(ordered_keys):
        raise TypeError("Too many positional arguments")
    for key, value in zip(ordered_keys, args, strict=False):
        if values[key] is not None:
            raise TypeError(f"{key} provided multiple times")
        values[key] = value
</file>

<file path="line_tactic_helpers.py">
"""Helpers for constructing line tactic contexts."""

from dataclasses import dataclass

import chess

from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.LineTacticContext import LineTacticContext
from tactix.utils.logger import funclogger


@dataclass(frozen=True)
class LineTacticInputs:
    """Grouped inputs for line tactic contexts."""

    detector: BaseTacticDetector
    board: chess.Board
    start: chess.Square
    step: int
    opponent: bool
    target_stronger: bool


@funclogger
def build_line_tactic_context(inputs: LineTacticInputs) -> LineTacticContext:
    """Build a LineTacticContext for line tactic checks."""
    return LineTacticContext(
        detector=inputs.detector,
        board=inputs.board,
        start=inputs.start,
        step=inputs.step,
        opponent=inputs.opponent,
        target_stronger=inputs.target_stronger,
    )
</file>

<file path="LineTacticContext.py">
"""Context for line tactic evaluations."""

# pylint: disable=invalid-name

from __future__ import annotations

from dataclasses import dataclass

import chess

from tactix.BaseTacticDetector import BaseTacticDetector


@dataclass(frozen=True)
class LineTacticContext:
    """Inputs for detecting line tactics such as pins or skewers."""

    detector: BaseTacticDetector
    board: chess.Board
    start: chess.Square
    step: int
    opponent: bool
    target_stronger: bool
</file>

<file path="list_sources_for_cache_refresh__api_cache.py">
"""Resolve sources to refresh for dashboard cache."""

from __future__ import annotations

from tactix.normalize_source__source import _normalize_source


def _sources_for_cache_refresh(source: str | None) -> list[str | None]:
    """Return normalized sources for cache refresh requests."""
    normalized = _normalize_source(source)
    sources: list[str | None] = [None]
    if normalized is not None:
        sources.append(normalized)
    return sources
</file>

<file path="load_fixture_games.py">
"""Load fixture games from disk for testing or demos."""

import logging
from collections.abc import Callable, Iterable
from dataclasses import dataclass
from pathlib import Path
from typing import cast

from tactix._coerce_fixture_rows import _coerce_fixture_rows
from tactix._filter_fixture_games import _filter_fixture_games
from tactix._resolve_fixture_message import _resolve_fixture_message
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.split_pgn_chunks import split_pgn_chunks
from tactix.utils import Logger


@dataclass(frozen=True)
class FixtureGamesRequest:  # pylint: disable=too-many-instance-attributes
    """Inputs for loading fixture games."""

    fixture_path: Path
    user: str
    source: str
    since_ms: int
    until_ms: int | None = None
    logger: logging.Logger | None = None
    missing_message: str | None = None
    loaded_message: str | None = None
    coerce_rows: Callable[[Iterable[dict[str, object]]], list[dict]] | None = None


def _collect_fixture_values(
    args: tuple[object, ...],
    legacy: dict[str, object],
) -> dict[str, object]:
    ordered_keys = (
        "user",
        "source",
        "since_ms",
        "until_ms",
        "logger",
        "missing_message",
        "loaded_message",
        "coerce_rows",
    )
    values = init_legacy_values(ordered_keys)
    apply_legacy_kwargs(values, ordered_keys, legacy)
    apply_legacy_args(values, ordered_keys, args)
    return values


def _build_fixture_request(
    request: Path | str,
    values: dict[str, object],
) -> FixtureGamesRequest:
    if values["user"] is None or values["source"] is None or values["since_ms"] is None:
        raise TypeError("fixture_path, user, source, and since_ms are required")
    user = cast(str, values["user"])
    source = cast(str, values["source"])
    since_ms = cast(int, values["since_ms"])
    until_ms = cast(int | None, values["until_ms"])
    logger = cast(logging.Logger | None, values["logger"])
    missing_message = cast(str | None, values["missing_message"])
    loaded_message = cast(str | None, values["loaded_message"])
    coerce_rows = cast(
        Callable[[Iterable[dict[str, object]]], list[dict]] | None,
        values["coerce_rows"],
    )
    return FixtureGamesRequest(
        fixture_path=Path(request),
        user=user,
        source=source,
        since_ms=since_ms,
        until_ms=until_ms,
        logger=logger,
        missing_message=missing_message,
        loaded_message=loaded_message,
        coerce_rows=coerce_rows,
    )


def load_fixture_games(
    request: FixtureGamesRequest | Path | str,
    *args: object,
    **legacy: object,
) -> list[dict[str, object]]:
    """Load fixture PGNs from disk and apply since/until timestamp filters."""
    if isinstance(request, FixtureGamesRequest):
        resolved_request = request
    else:
        values = _collect_fixture_values(args, legacy)
        resolved_request = _build_fixture_request(request, values)
    active_logger = resolved_request.logger or Logger(__name__)
    resolved_missing_message = _resolve_fixture_message(
        resolved_request.missing_message,
        "Fixture PGN path missing: %s",
    )
    resolved_loaded_message = _resolve_fixture_message(
        resolved_request.loaded_message,
        "Loaded %s fixture PGNs from %s",
    )
    if not resolved_request.fixture_path.exists():
        active_logger.warning(resolved_missing_message, resolved_request.fixture_path)
        return []
    chunks = split_pgn_chunks(resolved_request.fixture_path.read_text(encoding="utf-8"))
    games = _filter_fixture_games(
        chunks,
        resolved_request.user,
        resolved_request.source,
        resolved_request.since_ms,
        resolved_request.until_ms,
    )
    active_logger.info(
        resolved_loaded_message,
        len(games),
        resolved_request.fixture_path,
    )
    return _coerce_fixture_rows(games, resolved_request.coerce_rows)
</file>

<file path="load_resume_positions__pipeline.py">
"""Load positions to resume analysis based on checkpoints."""

from __future__ import annotations

from tactix.analysis_signature__pipeline import _analysis_signature
from tactix.app.use_cases.pipeline_support import (
    _clear_analysis_checkpoint,
    _read_analysis_checkpoint,
)
from tactix.db.position_repository_provider import fetch_positions_for_games
from tactix.define_pipeline_state__pipeline import RESUME_INDEX_START, logger


def _load_resume_positions(
    conn,
    analysis_checkpoint_path,
    game_ids: list[str],
    source: str,
) -> tuple[list[dict[str, object]], int, str]:
    positions: list[dict[str, object]] = []
    resume_index = -1
    analysis_signature = ""
    if analysis_checkpoint_path.exists():
        existing_positions = fetch_positions_for_games(conn, game_ids)
        if existing_positions:
            analysis_signature = _analysis_signature(game_ids, len(existing_positions), source)
            resume_index = _read_analysis_checkpoint(analysis_checkpoint_path, analysis_signature)
            if resume_index >= RESUME_INDEX_START:
                logger.info("Resuming analysis at index=%s for source=%s", resume_index, source)
                positions = existing_positions
            else:
                _clear_analysis_checkpoint(analysis_checkpoint_path)
        else:
            _clear_analysis_checkpoint(analysis_checkpoint_path)
    return positions, resume_index, analysis_signature
</file>

<file path="log_raw_pgns_persisted__pipeline.py">
"""Log counts of raw PGNs persisted during sync."""

from __future__ import annotations

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import logger
from tactix.GameRow import GameRow


def _log_raw_pgns_persisted(
    settings: Settings,
    raw_pgns_inserted: int,
    raw_pgns_hashed: int,
    raw_pgns_matched: int,
    games_to_process: list[GameRow],
) -> None:
    logger.info(
        "Raw PGNs persisted: raw_pgns_inserted=%s raw_pgns_hashed=%s "
        "raw_pgns_matched=%s source=%s total=%s",
        raw_pgns_inserted,
        raw_pgns_hashed,
        raw_pgns_matched,
        settings.source,
        len(games_to_process),
    )
</file>

<file path="manage_lifespan__fastapi.py">
"""FastAPI lifespan hook for cache refresh."""

from __future__ import annotations

from contextlib import asynccontextmanager

from fastapi import FastAPI

from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async


@asynccontextmanager
async def lifespan(_: FastAPI):
    """Initialize background tasks during application lifespan."""
    _refresh_dashboard_cache_async([None, "lichess", "chesscom"])
    yield
</file>

<file path="mate_outcome.py">
from dataclasses import dataclass

from tactix._should_upgrade_mate_result import _should_upgrade_mate_result
from tactix.analyze_tactics__positions import MATE_IN_ONE, MATE_IN_TWO
from tactix.outcome_context import BaseOutcomeContext


@dataclass(frozen=True)
class MateOutcome:
    """Context for mate outcome evaluation."""

    outcome: BaseOutcomeContext
    after_cp: int
    mate_in_one: bool
    mate_in_two: bool

    @property
    def mate_in(self) -> int | None:
        if self.mate_in_one:
            return MATE_IN_ONE
        if self.mate_in_two:
            return MATE_IN_TWO
        return None

    @property
    def should_be_upgraded(self) -> bool:
        """Return True if the mate outcome should be upgraded."""
        return _should_upgrade_mate_result(self.outcome, self.mate_in)
</file>

<file path="maybe_sync_analysis_results__pipeline.py">
from __future__ import annotations

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ZERO_COUNT
from tactix.sync_postgres_analysis_results__pipeline import _sync_postgres_analysis_results


def _maybe_sync_analysis_results(
    conn,
    settings: Settings,
    pg_conn,
    analysis_pg_enabled: bool,
    postgres_written: int,
) -> tuple[int, int]:
    if pg_conn is None or not analysis_pg_enabled or postgres_written != ZERO_COUNT:
        return 0, postgres_written
    postgres_synced = _sync_postgres_analysis_results(conn, pg_conn, settings)
    return postgres_synced, postgres_written + postgres_synced
</file>

<file path="maybe_upsert_postgres_analysis__pipeline.py">
"""Maybe upsert analysis rows into Postgres."""

from __future__ import annotations

# pylint: disable=broad-exception-caught
from tactix.define_pipeline_state__pipeline import logger
from tactix.upsert_analysis_tactic_with_outcome import upsert_analysis_tactic_with_outcome


def _maybe_upsert_postgres_analysis(
    pg_conn,
    analysis_pg_enabled: bool,
    tactic_row: dict[str, object],
    outcome_row: dict[str, object],
) -> bool:
    if pg_conn is None or not analysis_pg_enabled:
        return False
    try:
        upsert_analysis_tactic_with_outcome(
            pg_conn,
            tactic_row,
            outcome_row,
        )
    except Exception as exc:
        logger.warning("Postgres analysis upsert failed: %s", exc)
        return False
    return True
</file>

<file path="motif_stats.py">
"""API helpers for motif statistics."""

from typing import Annotated

from fastapi import Depends

from tactix.build_dashboard_stats_payload__api import _build_dashboard_stats_payload
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.db.dashboard_repository_provider import fetch_motif_stats


def motif_stats(
    filters: Annotated[DashboardQueryFilters, Depends()],
) -> dict[str, object]:
    """Return motif statistics payload for the dashboard."""
    return _build_dashboard_stats_payload(filters, fetch_motif_stats, "motifs")
</file>

<file path="no_games_payload_contexts.py">
"""Helpers for constructing no-games payload contexts."""

from __future__ import annotations

from tactix.sync_contexts import (
    NoGamesAfterDedupeContext,
    NoGamesAfterDedupePayloadContext,
    NoGamesContext,
    NoGamesPayloadContext,
)
from tactix.utils.logger import funclogger


@funclogger
def build_no_games_payload_context(context: NoGamesContext) -> NoGamesPayloadContext:
    """Create a NoGamesPayloadContext from a NoGamesContext."""
    return NoGamesPayloadContext(
        settings=context.settings,
        conn=context.conn,
        backfill_mode=context.backfill_mode,
        window=context.window,
    )


@funclogger
def build_no_games_after_dedupe_payload_context(
    context: NoGamesAfterDedupeContext,
) -> NoGamesAfterDedupePayloadContext:
    """Create a NoGamesAfterDedupePayloadContext from a NoGamesAfterDedupeContext."""
    return NoGamesAfterDedupePayloadContext(
        settings=context.settings,
        conn=context.conn,
        backfill_mode=context.backfill_mode,
        games=context.games,
        window=context.window,
    )
</file>

<file path="normalize_and_expand_games__pipeline.py">
from __future__ import annotations

from collections.abc import Mapping

from tactix.config import Settings
from tactix.dedupe_games__pipeline import _dedupe_games
from tactix.expand_pgn_rows__pipeline import _expand_pgn_rows
from tactix.GameRow import GameRow
from tactix.normalize_game_row__pipeline import _normalize_game_row


def _normalize_and_expand_games(
    raw_games: list[Mapping[str, object]],
    settings: Settings,
) -> list[GameRow]:
    games = [_normalize_game_row(game, settings) for game in raw_games]
    games = _expand_pgn_rows(games, settings)
    return _dedupe_games(games)
</file>

<file path="normalize_game_row__pipeline.py">
"""Normalize raw game rows for pipeline ingestion."""

from __future__ import annotations

from collections.abc import Mapping
from datetime import UTC, datetime

from tactix.app.use_cases.pipeline_support import _coerce_int, _coerce_pgn, _coerce_str
from tactix.config import Settings
from tactix.GameRow import GameRow


def _normalize_game_row(row: Mapping[str, object], settings: Settings) -> GameRow:
    fetched_at = row.get("fetched_at")
    if not isinstance(fetched_at, datetime):
        fetched_at = datetime.now(UTC)
    return {
        "game_id": _coerce_str(row.get("game_id")),
        "user": _coerce_str(row.get("user")) or settings.user,
        "source": _coerce_str(row.get("source")) or settings.source,
        "fetched_at": fetched_at,
        "pgn": _coerce_pgn(row.get("pgn")),
        "last_timestamp_ms": _coerce_int(row.get("last_timestamp_ms")),
    }
</file>

<file path="normalize_pgn_text__db_store.py">
from __future__ import annotations

from collections.abc import Callable


def _normalize_pgn_text(
    pgn_text: str,
    normalize_pgn: Callable[[str], str] | None,
) -> str | None:
    return normalize_pgn(pgn_text) if normalize_pgn else None
</file>

<file path="normalize_pgn.py">
"""Helpers for normalizing PGN strings."""

from io import StringIO

import chess.pgn


def normalize_pgn(pgn: str) -> str:
    """Normalize a PGN by re-exporting its game data."""
    game = chess.pgn.read_game(StringIO(pgn))
    if not game:
        return pgn.strip()
    exporter = chess.pgn.StringExporter(headers=True, variations=True, comments=True, columns=80)
    normalized = game.accept(exporter)
    return normalized.strip()
</file>

<file path="normalize_source__source.py">
"""Normalize source values for API inputs."""

from __future__ import annotations


def _normalize_source(source: str | None) -> str | None:
    """Return a normalized source string or None for all."""
    if source is None:
        return None
    trimmed = source.strip().lower()
    return None if trimmed == "all" else trimmed
</file>

<file path="ops_event.py">
"""Ops event payload model."""

from __future__ import annotations

from dataclasses import dataclass

from tactix.config import Settings


@dataclass(frozen=True)
class OpsEvent:
    """Structured event metadata for operational logging."""

    settings: Settings
    component: str
    event_type: str
    source: str | None
    profile: str | None
    metadata: dict[str, object] | None = None
</file>

<file path="orchestrate_dag_run__airflow_trigger.py">
from __future__ import annotations

from typing import Any

from tactix.config import Settings
from tactix.fetch_json__airflow_api import fetch_json__airflow_api


def orchestrate_dag_run__airflow_trigger(
    settings: Settings, dag_id: str, conf: dict[str, Any] | None = None
) -> dict[str, Any]:
    """Trigger an Airflow DAG run via the API.

    Args:
        settings: Application settings.
        dag_id: Airflow DAG identifier.
        conf: Optional DAG run configuration.

    Returns:
        Airflow DAG run payload.
    """
    payload = {"conf": conf or {}}
    return fetch_json__airflow_api(settings, "post", f"/api/v1/dags/{dag_id}/dagRuns", payload)
</file>

<file path="outcome_context.py">
"""Context objects for outcome evaluation."""

from __future__ import annotations

from dataclasses import dataclass
from enum import StrEnum

from tactix.config import Settings


class OutcomeResultEnum(StrEnum):
    """Possible outcome results."""

    SUCCESS = "success"
    MISSED = "missed"
    UNCLEAR = "unclear"
    FAILED_ATTEMPT = "failed_attempt"


@dataclass(frozen=True)
class BaseOutcomeContext:
    """Base outcome context for tactic evaluation."""

    result: OutcomeResultEnum
    motif: str
    best_move: str | None
    user_move_uci: str
    swing: int | None
    after_cp: int | None = None

    @property
    def was_missed(self) -> bool:
        """Return True if the outcome was missed."""
        return self.result == OutcomeResultEnum.MISSED

    @property
    def was_unclear(self) -> bool:
        """Return True if the mate was unclear."""
        return self.result == OutcomeResultEnum.UNCLEAR


@dataclass(frozen=True)
class OutcomeOverridesContext:
    """Context for applying outcome overrides."""

    outcome: BaseOutcomeContext
    best_motif: str | None
    after_cp: int
    mate_in_one: bool
    mate_in_two: bool
    settings: Settings | None


_VULTURE_USED = (
    OutcomeResultEnum.SUCCESS,
    OutcomeResultEnum.FAILED_ATTEMPT,
    BaseOutcomeContext.was_missed,
    BaseOutcomeContext.was_unclear,
)
</file>

<file path="OutcomeDetails.py">
from dataclasses import dataclass


@dataclass(frozen=True)
class OutcomeDetails:
    result: str
    user_move_uci: str
    delta: int
</file>

<file path="OutcomeRow.py">
from __future__ import annotations

from dataclasses import dataclass

from tactix.TacticRowInput import TacticRowInput
from tactix.utils.logger import funclogger


@dataclass
class OutcomeRow:
    tactic_id: int | None
    result: str
    user_uci: str
    eval_delta: int

    @classmethod
    @funclogger
    def from_inputs(cls, inputs: TacticRowInput) -> OutcomeRow:
        return cls(
            tactic_id=None,  # filled by caller
            result=inputs.outcome.result,
            user_uci=inputs.outcome.user_move_uci,
            eval_delta=inputs.outcome.delta,
        )
</file>

<file path="persist_and_extract_positions__pipeline.py">
"""Persist raw PGNs and extract positions for analysis."""

from __future__ import annotations

from dataclasses import dataclass

import duckdb

from tactix.analysis_signature__pipeline import _analysis_signature
from tactix.app.use_cases.pipeline_support import _emit_progress
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.extract_positions_from_games__pipeline import _extract_positions_from_games
from tactix.GameRow import GameRow
from tactix.persist_raw_pgns__pipeline import PersistRawPgnsContext, _persist_raw_pgns
from tactix.upsert_postgres_raw_pgns_if_enabled__pipeline import (
    _upsert_postgres_raw_pgns_if_enabled,
)


@dataclass(frozen=True)
class PersistAndExtractPositionsContext:
    """Context for persisting raw PGNs and extracting positions."""

    conn: duckdb.DuckDBPyConnection
    settings: Settings
    games_to_process: list[GameRow]
    progress: ProgressCallback | None
    profile: str | None
    game_ids: list[str]


def _persist_and_extract_positions(
    context: PersistAndExtractPositionsContext,
) -> tuple[list[dict[str, object]], str, tuple[int, int, int], int]:
    """Persist raw PGNs, extract positions, and return metrics."""
    raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched = _persist_raw_pgns(
        PersistRawPgnsContext(
            conn=context.conn,
            games_to_process=context.games_to_process,
            settings=context.settings,
            progress=context.progress,
            profile=context.profile,
            delete_existing=True,
            emit_start=True,
        )
    )
    postgres_raw_pgns_inserted = _upsert_postgres_raw_pgns_if_enabled(
        context.settings,
        context.games_to_process,
        context.progress,
        context.profile,
    )
    _emit_progress(
        context.progress,
        "extract_positions",
        source=context.settings.source,
        message="Extracting positions",
    )
    positions = _extract_positions_from_games(
        context.conn,
        context.games_to_process,
        context.settings,
    )
    analysis_signature = _analysis_signature(
        context.game_ids,
        len(positions),
        context.settings.source,
    )
    return (
        positions,
        analysis_signature,
        (raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched),
        postgres_raw_pgns_inserted,
    )
</file>

<file path="persist_raw_pgns__pipeline.py">
"""Persist raw PGNs into the database."""

from __future__ import annotations

from dataclasses import dataclass

import duckdb

from tactix.app.use_cases.pipeline_support import _emit_progress
from tactix.config import Settings
from tactix.db.delete_game_rows import delete_game_rows
from tactix.db.raw_pgn_repository_provider import raw_pgn_repository
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.GameRow import GameRow
from tactix.ops_event import OpsEvent
from tactix.record_ops_event import record_ops_event
from tactix.validate_raw_pgn_hashes__pipeline import _validate_raw_pgn_hashes


@dataclass(frozen=True)
class PersistRawPgnsContext:
    """Context for persisting raw PGNs."""

    conn: duckdb.DuckDBPyConnection
    games_to_process: list[GameRow]
    settings: Settings
    progress: ProgressCallback | None
    profile: str | None
    delete_existing: bool
    emit_start: bool


def _persist_raw_pgns(context: PersistRawPgnsContext) -> tuple[int, int, int]:
    """Persist raw PGNs and return insert/hash metrics."""
    repository = raw_pgn_repository(context.conn)
    if context.emit_start:
        _emit_progress(
            context.progress,
            "raw_pgns",
            source=context.settings.source,
            message="Persisting raw PGNs",
        )
    if context.delete_existing:
        delete_game_rows(
            context.conn,
            [game["game_id"] for game in context.games_to_process],
        )
    raw_pgns_inserted = repository.upsert_raw_pgns(context.games_to_process)
    hash_metrics = _validate_raw_pgn_hashes(
        context.games_to_process,
        context.settings.source,
        repository.fetch_latest_pgn_hashes,
    )
    raw_pgns_hashed = hash_metrics["computed"]
    raw_pgns_matched = hash_metrics["matched"]
    _emit_progress(
        context.progress,
        "raw_pgns_persisted",
        source=context.settings.source,
        inserted=raw_pgns_inserted,
        total=len(context.games_to_process),
    )
    _emit_progress(
        context.progress,
        "raw_pgns_hashed",
        source=context.settings.source,
        computed=raw_pgns_hashed,
        matched=raw_pgns_matched,
    )
    record_ops_event(
        OpsEvent(
            settings=context.settings,
            component="ingestion",
            event_type="raw_pgns_persisted",
            source=context.settings.source,
            profile=context.profile,
            metadata={
                "inserted": raw_pgns_inserted,
                "computed": raw_pgns_hashed,
                "matched": raw_pgns_matched,
                "total": len(context.games_to_process),
            },
        )
    )
    return raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched
</file>

<file path="pgn_context_kwargs.py">
"""Shared helpers for building PGN context keyword arguments."""

from __future__ import annotations

from dataclasses import dataclass

from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.PgnContextKwargs import PgnContextKwargs

_PGN_CONTEXT_KEYS = (
    "pgn",
    "user",
    "source",
    "game_id",
    "side_to_move_filter",
)


@dataclass(frozen=True)
class PgnContextInputs:
    """Grouped inputs for PGN context helpers."""

    pgn: str
    user: str
    source: str
    game_id: str | None = None
    side_to_move_filter: str | None = None


def build_pgn_context_inputs_from_values(
    pgn: str,
    user: str,
    source: str,
    game_id: str | None = None,
    side_to_move_filter: str | None = None,
) -> PgnContextInputs:
    """Build PgnContextInputs from raw values."""
    return PgnContextInputs(
        pgn=pgn,
        user=user,
        source=source,
        game_id=game_id,
        side_to_move_filter=side_to_move_filter,
    )


def build_pgn_context_kwargs_from_values(
    pgn: str,
    user: str,
    source: str,
    game_id: str | None = None,
    side_to_move_filter: str | None = None,
) -> PgnContextKwargs:
    """Build PGN context kwargs from raw values."""
    return build_pgn_context_kwargs(
        inputs=build_pgn_context_inputs_from_values(
            pgn,
            user,
            source,
            game_id=game_id,
            side_to_move_filter=side_to_move_filter,
        )
    )


def build_pgn_context_kwargs(
    *args: object,
    inputs: PgnContextInputs | None = None,
    **legacy: object,
) -> PgnContextKwargs:
    """Return a keyword argument dict for PGN-derived contexts."""
    if inputs is None:
        values = init_legacy_values(_PGN_CONTEXT_KEYS)
    else:
        values = init_legacy_values(
            _PGN_CONTEXT_KEYS,
            initial={
                "pgn": inputs.pgn,
                "user": inputs.user,
                "source": inputs.source,
                "game_id": inputs.game_id,
                "side_to_move_filter": inputs.side_to_move_filter,
            },
        )
    apply_legacy_kwargs(values, _PGN_CONTEXT_KEYS, legacy)
    apply_legacy_args(values, _PGN_CONTEXT_KEYS, args)
    return PgnContextKwargs(
        pgn=values["pgn"],
        user=values["user"],
        source=values["source"],
        game_id=values["game_id"],
        side_to_move_filter=values["side_to_move_filter"],
    )
</file>

<file path="pgn_headers.py">
"""PGN header parsing and normalization."""

from __future__ import annotations

import datetime
import re
from dataclasses import dataclass
from functools import singledispatch
from io import StringIO

import chess.pgn

from tactix._get_game_result_for_user_from_pgn_headers import (
    _get_game_result_for_user_from_pgn_headers,
)
from tactix._parse_optional_int import _parse_optional_int
from tactix.chess_game_result import ChessGameResult
from tactix.chess_time_control import ChessTimeControl
from tactix.utils import Now


@dataclass
class PgnHeaders:  # pylint: disable=too-many-instance-attributes
    """Dataclass representing PGN headers for a chess game."""

    event: str
    site: str
    date: datetime.date | str | None
    time_control: ChessTimeControl
    round: int | None = None
    white_player: str | None = None
    black_player: str | None = None
    result: ChessGameResult | None = None
    white_elo: int | None = None
    black_elo: int | None = None
    end_time: datetime.datetime | None = None
    termination: str | None = None

    def __post_init__(self):
        """Normalize header fields after initialization."""
        self.date = _coerce_pgn_date(self.date)
        self._convert_round_to_int()
        if not self.result:
            self.result = ChessGameResult.INCOMPLETE

    def _convert_round_to_int(self):
        """Normalize the round value into an integer."""
        if self.round is None:
            return
        try:
            self.round = int(self.round)
        except (TypeError, ValueError):
            self.round = None

    @classmethod
    def from_pgn_string(cls, pgn_string: str, game_index: int = 0) -> PgnHeaders:
        """Create a PgnHeaders dataclass directly from a PGN string.

        Parameters
        ----------
        pgn_string : str
            A string containing the PGN data.

        Returns
        -------
        PgnHeaders
            An instance of PgnHeaders populated with data from the PGN string.
        """
        chunks = re.split(r"\n\n(?=\[Event\s+\".*?\"\])", pgn_string)
        if game_index >= len(chunks):
            error_msg = f"Requested game index {game_index} but only {len(chunks)} games found."
            raise IndexError(error_msg)
        headers = chess.pgn.read_headers(StringIO(chunks[game_index]))
        if headers is None:
            headers = chess.pgn.Headers()
        try:
            result = _get_game_result_for_user_from_pgn_headers(headers, "chesscom")
        except ValueError:
            result = None
        return cls(
            event=headers.get("Event", ""),
            site=headers.get("Site", ""),
            date=headers.get("Date", ""),
            round=_parse_optional_int(headers.get("Round")),
            white_player=headers.get("White"),
            black_player=headers.get("Black"),
            result=result,
            white_elo=_parse_optional_int(headers.get("WhiteElo")),
            black_elo=_parse_optional_int(headers.get("BlackElo")),
            time_control=ChessTimeControl.from_pgn_string(headers.get("TimeControl", "-"))
            or ChessTimeControl(initial=0),
            end_time=None,
            termination=headers.get("Termination"),
        )

    @classmethod
    def from_file(cls, file_path: str, game_index: int = 0) -> PgnHeaders:
        """Create a PgnHeaders dataclass directly from a PGN file.

        If the file contains multiple games, this will read the first one by default,
        but you can specify a different game index.

        Parameters
        ----------
        file_path : str
            Path to the PGN file.
        game_index : int, optional
            Index of the game to read (default is 0).

        Returns
        -------
        PgnHeaders
            An instance of PgnHeaders populated with data from the PGN file.
        """
        with open(file_path, encoding="utf-8") as f:
            raw_pgn = f.read()

        chunks = re.split(r"\n\n(?=\[Event\s+\".*?\"\])", raw_pgn)
        if game_index >= len(chunks):
            error_msg = f"Requested game index {game_index} but only {len(chunks)} games found."
            raise IndexError(error_msg)

        if game_index == 0:
            info_message = f"Reading first game of {len(chunks)} from {file_path}."
            print(info_message)

        return cls.from_pgn_string(chunks[game_index])

    @classmethod
    def from_chess_pgn_headers(cls, headers: chess.pgn.Headers) -> PgnHeaders:
        """Create a PgnHeaders dataclass from chess.pgn.Headers.

        Parameters
        ----------
        headers : chess.pgn.Headers
            The PGN headers object from python-chess.

        Returns
        -------
        PgnHeaders
            An instance of PgnHeaders populated with data from the chess.pgn.Headers.
        """
        return cls(
            event=headers.get("Event", ""),
            site=headers.get("Site", ""),
            date=headers.get("Date", ""),
            round=_parse_optional_int(headers.get("Round")),
            white_player=headers.get("White"),
            black_player=headers.get("Black"),
            result=None,
            white_elo=_parse_optional_int(headers.get("WhiteElo")),
            black_elo=_parse_optional_int(headers.get("BlackElo")),
            time_control=ChessTimeControl.from_pgn_string(headers.get("TimeControl", "-"))
            or ChessTimeControl(initial=0),
            end_time=None,
            termination=headers.get("Termination"),
        )

    @staticmethod
    def extract_all_from_str(pgn_string: str) -> list[PgnHeaders]:
        """Extract PGN headers from all games in a PGN file.

        Parameters
        ----------
        pgn_string : str
            String containing PGN data for one or more games.

        Returns
        -------
        list[PgnHeaders]
            A list of PgnHeaders instances, one for each game in the PGN string.
        """

        chunks = re.split(r"\n\n(?=\[Event\s+\".*?\"\])", pgn_string)

        return [PgnHeaders.from_pgn_string(chunk) for chunk in chunks if chunk.strip()]


@singledispatch
def _coerce_pgn_date(_value: object) -> datetime.date | None:
    return None


@_coerce_pgn_date.register
def _coerce_pgn_datetime_dispatch(value: datetime.datetime) -> datetime.date | None:
    return _coerce_pgn_datetime(value)


@_coerce_pgn_date.register
def _coerce_pgn_date_dispatch(value: datetime.date) -> datetime.date | None:
    return value


@_coerce_pgn_date.register
def _coerce_pgn_str_dispatch(value: str) -> datetime.date | None:
    return _parse_pgn_date_str(value)


_VULTURE_USED = (
    PgnHeaders.from_file,
    PgnHeaders.from_chess_pgn_headers,
    PgnHeaders.extract_all_from_str,
    PgnHeaders.termination,
    _coerce_pgn_datetime_dispatch,
    _coerce_pgn_date_dispatch,
    _coerce_pgn_str_dispatch,
)


def _coerce_pgn_datetime(value: datetime.datetime) -> datetime.date | None:
    coerced = Now.to_utc(value)
    return coerced.date() if coerced else None


def _parse_pgn_date_str(value: str) -> datetime.date | None:
    if not value:
        return None
    try:
        return datetime.datetime.strptime(value, "%Y.%m.%d").replace(tzinfo=datetime.UTC).date()
    except ValueError:
        return None
</file>

<file path="pgn_utils.py">
"""Convenience exports for PGN parsing utilities."""

from __future__ import annotations

import time

import chess

from tactix._extract_site_id import _extract_site_id
from tactix.extract_game_id import extract_game_id
from tactix.extract_last_timestamp_ms import extract_last_timestamp_ms
from tactix.extract_pgn_metadata import extract_pgn_metadata
from tactix.latest_timestamp import latest_timestamp
from tactix.load_fixture_games import load_fixture_games
from tactix.normalize_pgn import normalize_pgn
from tactix.split_pgn_chunks import split_pgn_chunks

__all__ = [
    "_extract_site_id",
    "chess",
    "extract_game_id",
    "extract_last_timestamp_ms",
    "extract_pgn_metadata",
    "latest_timestamp",
    "load_fixture_games",
    "normalize_pgn",
    "split_pgn_chunks",
    "time",
]
</file>

<file path="PgnContext.py">
"""PGN parsing context and helpers."""

# pylint: disable=invalid-name

from dataclasses import dataclass
from io import StringIO

import chess
import chess.pgn


@dataclass
class PgnContext:
    """Holds parsed PGN data and cached derived values."""

    pgn: str
    user: str
    source: str
    game_id: str | None = None
    side_to_move_filter: str | None = None
    _game: chess.pgn.Game | None = None
    _board: chess.Board | None = None

    def __post_init__(self):
        """Normalize user name and initialize cached game."""
        self.user = self.user.lower()
        if self._game is None:
            self._get_game()

    def _get_game(self):
        """Parse the PGN text into a game if needed."""
        if self._game is None:
            self._game = chess.pgn.read_game(StringIO(self.pgn))

    @property
    def game(self) -> chess.pgn.Game | None:
        """Return the parsed PGN game."""
        self._get_game()
        return self._game

    @property
    def headers(self) -> dict[str, str]:
        """Return PGN headers as a dictionary."""
        self._get_game()
        if self._game is None:
            return {}
        return dict(self._game.headers)

    @property
    def fen(self) -> str | None:
        """Return the current board FEN for the PGN."""
        board = self.board
        if board is None:
            return None
        return board.fen()

    @property
    def board(self) -> chess.Board | None:
        """Return the cached board for the PGN."""
        if self._game is None:
            self._get_game()
        if self._game is None:
            return None
        if self._board is None:
            self._board = self._game.board()
        return self._board

    @property
    def white(self) -> str:
        """Return the normalized white player name."""
        game = self.game
        if game is None:
            return ""
        return game.headers.get("White", "").lower()

    @property
    def black(self) -> str:
        """Return the normalized black player name."""
        game = self.game
        if game is None:
            return ""
        return game.headers.get("Black", "").lower()

    @property
    def ply(self) -> int:
        """Return the current ply count for the PGN."""
        board = self.board
        if board is None:
            return 0
        return board.ply()

    @property
    def move_number(self) -> int:
        """Return the current fullmove number for the PGN."""
        board = self.board
        if board is None:
            return 0
        return board.fullmove_number

    @property
    def side_to_move(self) -> str | None:
        """Return which side is to move at the current board state."""
        board = self.board
        if board is None:
            return None
        return "white" if board.turn == chess.WHITE else "black"
</file>

<file path="PgnContextKwargs.py">
from __future__ import annotations

from typing import TypedDict


class PgnContextKwargs(TypedDict):
    pgn: str
    user: str
    source: str
    game_id: str | None
    side_to_move_filter: str | None
</file>

<file path="PgnUpsertInputs.py">
from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass
from datetime import datetime


@dataclass(frozen=True)
class PgnUpsertHashing:
    normalize_pgn: Callable[[str], str] | None
    hash_pgn: Callable[[str], str] | None


@dataclass(frozen=True)
class PgnUpsertTimestamps:
    fetched_at: datetime | None
    ingested_at: datetime | None
    last_timestamp_ms: int


@dataclass(frozen=True)
class PgnUpsertInputs:
    pgn_text: str
    user: str
    latest_hash: str | None
    latest_version: int
    hashing: PgnUpsertHashing
    timestamps: PgnUpsertTimestamps
    cursor: object

    @property
    def normalize_pgn(self) -> Callable[[str], str] | None:
        return self.hashing.normalize_pgn

    @property
    def hash_pgn(self) -> Callable[[str], str] | None:
        return self.hashing.hash_pgn

    @property
    def fetched_at(self) -> datetime | None:
        return self.timestamps.fetched_at

    @property
    def ingested_at(self) -> datetime | None:
        return self.timestamps.ingested_at

    @property
    def last_timestamp_ms(self) -> int:
        return self.timestamps.last_timestamp_ms
</file>

<file path="PgnUpsertPlan.py">
from datetime import datetime

from pydantic import BaseModel


class PgnUpsertPlan(BaseModel):
    """Payload for inserting or updating PGN rows."""

    pgn_text: str
    pgn_hash: str
    pgn_version: int
    normalized_pgn: str | None
    metadata: dict[str, object]
    fetched_at: datetime
    ingested_at: datetime
    last_timestamp_ms: int
    cursor: object
</file>

<file path="PinDetector.py">
"""Detector for pin motifs."""

from __future__ import annotations

import chess

from tactix._is_line_tactic import _is_line_tactic
from tactix._skewer_sources import _skewer_sources
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.LineTacticContext import LineTacticContext
from tactix.TacticContext import TacticContext


class PinDetector(BaseTacticDetector):
    """Detect pin motifs."""

    motif = "pin"

    def detect(self, context: TacticContext) -> bool:
        """Return True when a pin is created."""
        after_pins = self._pin_signatures(context.board_after, context.mover_color)
        if not after_pins:
            return False
        before_pins = self._pin_signatures(context.board_before, context.mover_color)
        return bool(after_pins - before_pins)

    def _pin_signatures(self, board: chess.Board, mover_color: bool) -> set[tuple[int, int]]:
        opponent = not mover_color
        signatures: set[tuple[int, int]] = set()
        for square, steps in _skewer_sources(board, mover_color):
            for step in steps:
                if _is_line_tactic(LineTacticContext(self, board, square, step, opponent, False)):
                    signatures.add((square, step))
        return signatures
</file>

<file path="pipeline_run_filters.py">
"""Pipeline run query filters."""

from __future__ import annotations

from datetime import date

from pydantic import BaseModel


class PipelineRunFilters(BaseModel):
    """Filters accepted by the pipeline run endpoint."""

    source: str | None = None
    profile: str | None = None
    user_id: str | None = None
    start_date: date | None = None
    end_date: date | None = None
    use_fixture: bool = False
    fixture_name: str | None = None
    db_name: str | None = None
    reset_db: bool = False
</file>

<file path="pipeline.py">
"""Pipeline orchestrators for analysis and sync flows."""

from __future__ import annotations

import time

import chess.engine

from tactix.analyse_with_retries__pipeline import _analyse_with_retries
from tactix.analysis_progress_interval__pipeline import _analysis_progress_interval
from tactix.analysis_signature__pipeline import _analysis_signature
from tactix.AnalysisPrepResult import AnalysisPrepResult
from tactix.analyze_position import analyze_position
from tactix.analyze_positions__pipeline import _analyze_positions
from tactix.analyze_positions_with_progress__pipeline import (
    _analyze_positions_with_progress,
)
from tactix.app.use_cases.pipeline_support import (
    _apply_no_games_dedupe_checkpoint,
    _black_profiles_for_source,
    _build_no_games_after_dedupe_payload,
    _build_no_games_payload,
    _clear_analysis_checkpoint,
    _coerce_int,
    _coerce_pgn,
    _coerce_str,
    _cursor_last_timestamp,
    _emit_backfill_window_filtered,
    _emit_daily_sync_start,
    _emit_fetch_progress,
    _emit_positions_ready,
    _emit_progress,
    _handle_no_games,
    _handle_no_games_after_dedupe,
    _log_skipped_backfill,
    _maybe_clear_analysis_checkpoint,
    _maybe_emit_analysis_progress,
    _maybe_emit_window_filtered,
    _maybe_write_analysis_checkpoint,
    _no_games_checkpoint,
    _no_games_cursor,
    _normalized_profile_for_source,
    _read_analysis_checkpoint,
    _resolve_chesscom_last_timestamp,
    _resolve_last_timestamp_value,
    _resolve_side_to_move_filter,
    _side_filter_for_profile,
    _update_chesscom_checkpoint,
    _update_daily_checkpoint,
    _update_lichess_checkpoint,
    _write_analysis_checkpoint,
)
from tactix.apply_backfill_filter__pipeline import _apply_backfill_filter
from tactix.attach_position_ids__pipeline import _attach_position_ids
from tactix.build_chess_client__pipeline import _build_chess_client
from tactix.build_chunk_row__pipeline import _build_chunk_row
from tactix.build_daily_sync_payload__pipeline import _build_daily_sync_payload
from tactix.build_pipeline_settings__pipeline import _build_pipeline_settings
from tactix.chesscom_raw_games__pipeline import _chesscom_raw_games
from tactix.collect_game_ids__pipeline import _collect_game_ids
from tactix.collect_positions_for_monitor__pipeline import _collect_positions_for_monitor
from tactix.compute_pgn_hashes__pipeline import _compute_pgn_hashes
from tactix.config import Settings
from tactix.conversion_payload__pipeline import _conversion_payload
from tactix.convert_raw_pgns_to_positions__pipeline import convert_raw_pgns_to_positions
from tactix.count_hash_matches__pipeline import _count_hash_matches
from tactix.DailyAnalysisResult import DailyAnalysisResult
from tactix.db.position_repository_provider import fetch_position_counts
from tactix.db.raw_pgn_repository_provider import fetch_latest_pgn_hashes, hash_pgn
from tactix.dedupe_games__pipeline import _dedupe_games
from tactix.define_pipeline_state__pipeline import (
    ANALYSIS_PROGRESS_BUCKETS,
    CHESSCOM_BLACK_PROFILES,
    DEFAULT_SYNC_LIMIT,
    INDEX_OFFSET,
    LICHESS_BLACK_PROFILES,
    RESUME_INDEX_START,
    SINGLE_PGN_CHUNK,
    ZERO_COUNT,
    ProgressCallback,
    logger,
)
from tactix.empty_conversion_payload__pipeline import _empty_conversion_payload
from tactix.expand_pgn_rows__pipeline import _expand_pgn_rows
from tactix.expand_single_pgn_row__pipeline import _expand_single_pgn_row
from tactix.extract_positions_for_new_games__pipeline import (
    _extract_positions_for_new_games,
)
from tactix.extract_positions_for_rows__pipeline import _extract_positions_for_rows
from tactix.extract_positions_from_games__pipeline import _extract_positions_from_games
from tactix.fetch_chesscom_games__pipeline import _fetch_chesscom_games
from tactix.fetch_incremental_games__pipeline import _fetch_incremental_games
from tactix.fetch_lichess_games__pipeline import _fetch_lichess_games
from tactix.FetchContext import FetchContext
from tactix.filter_backfill_games__pipeline import _filter_backfill_games
from tactix.filter_games_by_window__pipeline import _filter_games_by_window
from tactix.filter_games_for_window__pipeline import _filter_games_for_window
from tactix.filter_positions_to_process__pipeline import _filter_positions_to_process
from tactix.filter_unprocessed_games__pipeline import _filter_unprocessed_games
from tactix.GameRow import GameRow
from tactix.get_dashboard_payload__pipeline import get_dashboard_payload
from tactix.init_analysis_schema_if_needed__pipeline import _init_analysis_schema_if_needed
from tactix.is_backfill_mode__pipeline import _is_backfill_mode
from tactix.load_resume_positions__pipeline import _load_resume_positions
from tactix.log_raw_pgns_persisted__pipeline import _log_raw_pgns_persisted
from tactix.maybe_sync_analysis_results__pipeline import _maybe_sync_analysis_results
from tactix.maybe_upsert_postgres_analysis__pipeline import _maybe_upsert_postgres_analysis
from tactix.normalize_and_expand_games__pipeline import _normalize_and_expand_games
from tactix.normalize_game_row__pipeline import _normalize_game_row
from tactix.persist_and_extract_positions__pipeline import _persist_and_extract_positions
from tactix.persist_raw_pgns__pipeline import _persist_raw_pgns
from tactix.prepare_analysis_inputs__pipeline import _prepare_analysis_inputs
from tactix.prepare_games_for_sync__pipeline import _prepare_games_for_sync
from tactix.process_analysis_position__pipeline import _process_analysis_position
from tactix.raise_for_hash_mismatch__pipeline import _raise_for_hash_mismatch
from tactix.record_daily_sync_complete__pipeline import _record_daily_sync_complete
from tactix.refresh_raw_pgns_for_existing_positions__pipeline import (
    _refresh_raw_pgns_for_existing_positions,
)
from tactix.request_chesscom_games__pipeline import _request_chesscom_games
from tactix.run_analysis_and_metrics__pipeline import _run_analysis_and_metrics
from tactix.run_analysis_loop__pipeline import _run_analysis_loop
from tactix.run_daily_game_sync__pipeline import _run_daily_game_sync, run_daily_game_sync
from tactix.run_migrations__pipeline import run_migrations
from tactix.run_monitor_new_positions__pipeline import run_monitor_new_positions
from tactix.run_refresh_metrics__pipeline import run_refresh_metrics
from tactix.should_skip_backfill__pipeline import _should_skip_backfill
from tactix.StockfishEngine import StockfishEngine
from tactix.sync_postgres_analysis_results__pipeline import _sync_postgres_analysis_results
from tactix.update_metrics_and_version__pipeline import _update_metrics_and_version
from tactix.upsert_analysis_tactic_with_outcome import upsert_analysis_tactic_with_outcome
from tactix.upsert_postgres_raw_pgns_if_enabled__pipeline import (
    _upsert_postgres_raw_pgns_if_enabled,
)
from tactix.validate_raw_pgn_hashes__pipeline import _validate_raw_pgn_hashes
from tactix.within_window__pipeline import _within_window


def analyse_with_retries(
    engine,
    position: dict[str, object],
    settings: Settings,
) -> tuple[dict[str, object], dict[str, object]] | None:
    return _analyse_with_retries(engine, position, settings)


__all__ = [
    "ANALYSIS_PROGRESS_BUCKETS",
    "CHESSCOM_BLACK_PROFILES",
    "DEFAULT_SYNC_LIMIT",
    "INDEX_OFFSET",
    "LICHESS_BLACK_PROFILES",
    "RESUME_INDEX_START",
    "SINGLE_PGN_CHUNK",
    "ZERO_COUNT",
    "AnalysisPrepResult",
    "DailyAnalysisResult",
    "FetchContext",
    "GameRow",
    "ProgressCallback",
    "StockfishEngine",
    "_analyse_with_retries",
    "_analysis_progress_interval",
    "_analysis_signature",
    "_analyze_positions",
    "_analyze_positions_with_progress",
    "_apply_backfill_filter",
    "_apply_no_games_dedupe_checkpoint",
    "_attach_position_ids",
    "_black_profiles_for_source",
    "_build_chess_client",
    "_build_chunk_row",
    "_build_daily_sync_payload",
    "_build_no_games_after_dedupe_payload",
    "_build_no_games_payload",
    "_build_pipeline_settings",
    "_chesscom_raw_games",
    "_clear_analysis_checkpoint",
    "_coerce_int",
    "_coerce_pgn",
    "_coerce_str",
    "_collect_game_ids",
    "_collect_positions_for_monitor",
    "_compute_pgn_hashes",
    "_conversion_payload",
    "_count_hash_matches",
    "_cursor_last_timestamp",
    "_dedupe_games",
    "_emit_backfill_window_filtered",
    "_emit_daily_sync_start",
    "_emit_fetch_progress",
    "_emit_positions_ready",
    "_emit_progress",
    "_empty_conversion_payload",
    "_expand_pgn_rows",
    "_expand_single_pgn_row",
    "_extract_positions_for_new_games",
    "_extract_positions_for_rows",
    "_extract_positions_from_games",
    "_fetch_chesscom_games",
    "_fetch_incremental_games",
    "_fetch_lichess_games",
    "_filter_backfill_games",
    "_filter_games_by_window",
    "_filter_games_for_window",
    "_filter_positions_to_process",
    "_filter_unprocessed_games",
    "_handle_no_games",
    "_handle_no_games_after_dedupe",
    "_init_analysis_schema_if_needed",
    "_is_backfill_mode",
    "_load_resume_positions",
    "_log_raw_pgns_persisted",
    "_log_skipped_backfill",
    "_maybe_clear_analysis_checkpoint",
    "_maybe_emit_analysis_progress",
    "_maybe_emit_window_filtered",
    "_maybe_sync_analysis_results",
    "_maybe_upsert_postgres_analysis",
    "_maybe_write_analysis_checkpoint",
    "_no_games_checkpoint",
    "_no_games_cursor",
    "_normalize_and_expand_games",
    "_normalize_game_row",
    "_normalized_profile_for_source",
    "_persist_and_extract_positions",
    "_persist_raw_pgns",
    "_prepare_analysis_inputs",
    "_prepare_games_for_sync",
    "_process_analysis_position",
    "_raise_for_hash_mismatch",
    "_read_analysis_checkpoint",
    "_record_daily_sync_complete",
    "_refresh_raw_pgns_for_existing_positions",
    "_request_chesscom_games",
    "_resolve_chesscom_last_timestamp",
    "_resolve_last_timestamp_value",
    "_resolve_side_to_move_filter",
    "_run_analysis_and_metrics",
    "_run_analysis_loop",
    "_run_daily_game_sync",
    "_should_skip_backfill",
    "_side_filter_for_profile",
    "_sync_postgres_analysis_results",
    "_update_chesscom_checkpoint",
    "_update_daily_checkpoint",
    "_update_lichess_checkpoint",
    "_update_metrics_and_version",
    "_upsert_postgres_raw_pgns_if_enabled",
    "_validate_raw_pgn_hashes",
    "_within_window",
    "_write_analysis_checkpoint",
    "analyse_with_retries",
    "analyze_position",
    "chess",
    "convert_raw_pgns_to_positions",
    "fetch_latest_pgn_hashes",
    "fetch_position_counts",
    "get_dashboard_payload",
    "hash_pgn",
    "logger",
    "run_daily_game_sync",
    "run_migrations",
    "run_monitor_new_positions",
    "run_refresh_metrics",
    "time",
    "upsert_analysis_tactic_with_outcome",
]
</file>

<file path="PositionContext.py">
"""Context for a single chess position."""

# pylint: disable=invalid-name

from dataclasses import dataclass


@dataclass
class PositionContext:  # pylint: disable=too-many-instance-attributes
    """Structured position data extracted from a game."""

    game_id: str
    user: str
    source: str
    fen: str
    ply: int
    move_number: int
    side_to_move: str
    user_to_move: bool
    uci: str
    san: str
    clock_seconds: float | None
    is_legal: bool


def _vulture_use_user_to_move(context: PositionContext) -> bool:
    return context.user_to_move


_VULTURE_USED = (_vulture_use_user_to_move,)
</file>

<file path="post_practice_attempt__api.py">
"""API handler for practice attempt submissions."""

from __future__ import annotations

import time as time_module

from fastapi import HTTPException

from tactix.config import get_settings
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.db.tactic_repository_provider import tactic_repository
from tactix.models import PracticeAttemptRequest


def practice_attempt(payload: PracticeAttemptRequest) -> dict[str, object]:
    """Grade a practice attempt and return the result."""
    settings = get_settings(source=payload.source)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    latency_ms: int | None = None
    if payload.served_at_ms is not None:
        now_ms = int(time_module.time() * 1000)
        latency_ms = max(0, now_ms - payload.served_at_ms)
    try:
        repo = tactic_repository(conn)
        result = repo.grade_practice_attempt(
            payload.tactic_id,
            payload.position_id,
            payload.attempted_uci,
            latency_ms=latency_ms,
        )
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    return result
</file>

<file path="postgres_analysis_enabled.py">
"""Check whether Postgres analysis is enabled."""

from tactix.config import Settings
from tactix.postgres_enabled import postgres_enabled


def postgres_analysis_enabled(settings: Settings) -> bool:
    """Return True when analysis outputs should write to Postgres."""
    return settings.postgres_analysis_enabled and postgres_enabled(settings)
</file>

<file path="postgres_connection.py">
from collections.abc import Iterator
from contextlib import contextmanager

import psycopg2
from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix._connection_kwargs import _connection_kwargs
from tactix.config import Settings
from tactix.utils.logger import Logger

logger = Logger(__name__)


@contextmanager
def postgres_connection(settings: Settings) -> Iterator[PgConnection | None]:
    kwargs = _connection_kwargs(settings)
    if not kwargs:
        yield None
        return
    conn: PgConnection | None = None
    try:
        conn = psycopg2.connect(**kwargs)
        conn.autocommit = True
    except Exception as exc:  # pragma: no cover - handled by status endpoint
        logger.warning("Postgres connection failed: %s", exc)
        yield None
        return
    try:
        yield conn
    finally:
        if conn is not None:
            conn.close()
</file>

<file path="postgres_enabled.py">
"""Check whether Postgres is enabled by configuration."""

from tactix._connection_kwargs import _connection_kwargs
from tactix.config import Settings


def postgres_enabled(settings: Settings) -> bool:
    """Return True when Postgres settings are configured."""
    return _connection_kwargs(settings) is not None
</file>

<file path="postgres_pgns_enabled.py">
"""Helpers to check Postgres PGN configuration."""

from tactix.config import Settings
from tactix.postgres_enabled import postgres_enabled


def postgres_pgns_enabled(settings: Settings) -> bool:
    """Return True when Postgres PGN storage is enabled."""
    return settings.postgres_pgns_enabled and postgres_enabled(settings)
</file>

<file path="postgres_status.py">
"""Postgres health status models."""

from __future__ import annotations

from dataclasses import dataclass


@dataclass(slots=True)
class PostgresStatus:
    """Status snapshot for Postgres connectivity checks."""

    enabled: bool
    status: str
    latency_ms: float | None = None
    error: str | None = None
    schema: str | None = None
    tables: list[str] | None = None
</file>

<file path="PostgresSettings.py">
"""Postgres connection settings definition."""

# pylint: disable=invalid-name

import os
from dataclasses import dataclass


@dataclass(slots=True)
class PostgresSettings:  # pylint: disable=invalid-name,too-many-instance-attributes
    """PostgreSQL connection settings."""

    dsn: str | None = os.getenv("TACTIX_POSTGRES_DSN")
    host: str | None = os.getenv("TACTIX_POSTGRES_HOST")
    port: int = int(os.getenv("TACTIX_POSTGRES_PORT", "5432"))
    db: str | None = os.getenv("TACTIX_POSTGRES_DB")
    user: str | None = os.getenv("TACTIX_POSTGRES_USER")
    password: str | None = os.getenv("TACTIX_POSTGRES_PASSWORD")
    sslmode: str = os.getenv("TACTIX_POSTGRES_SSLMODE", "disable")
    connect_timeout_s: int = int(os.getenv("TACTIX_POSTGRES_CONNECT_TIMEOUT", "5"))
    analysis_enabled: bool = os.getenv("TACTIX_POSTGRES_ANALYSIS_ENABLED", "1") == "1"
    pgns_enabled: bool = os.getenv("TACTIX_POSTGRES_PGNS_ENABLED", "1") == "1"

    @property
    def is_configured(self) -> bool:
        """Check if sufficient settings are provided to connect.

        Returns:
            True if either DSN is provided or host, db, user, and password are set.
        """

        return bool(self.dsn) or all([self.host, self.db, self.user, self.password])
</file>

<file path="prepare_analysis_inputs__pipeline.py">
"""Prepare analysis inputs for the pipeline."""

from __future__ import annotations

from tactix.AnalysisPrepResult import AnalysisPrepResult
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import (
    ProgressCallback,
)
from tactix.GameRow import GameRow
from tactix.load_resume_positions__pipeline import _load_resume_positions
from tactix.persist_and_extract_positions__pipeline import (
    PersistAndExtractPositionsContext,
    _persist_and_extract_positions,
)
from tactix.refresh_raw_pgns_for_existing_positions__pipeline import (
    _refresh_raw_pgns_for_existing_positions,
)


def _prepare_analysis_inputs(
    conn,
    settings: Settings,
    games_to_process: list[GameRow],
    progress: ProgressCallback | None,
    profile: str | None,
) -> AnalysisPrepResult:
    """Return prepared analysis inputs and metrics."""
    game_ids = [game["game_id"] for game in games_to_process]
    analysis_checkpoint_path = settings.analysis_checkpoint_path
    positions, resume_index, analysis_signature = _load_resume_positions(
        conn, analysis_checkpoint_path, game_ids, settings.source
    )
    if positions:
        (raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched), postgres_raw_pgns_inserted = (
            _refresh_raw_pgns_for_existing_positions(
                conn,
                settings,
                games_to_process,
                progress,
                profile,
            )
        )
        return AnalysisPrepResult(
            positions=positions,
            resume_index=resume_index,
            analysis_signature=analysis_signature,
            raw_pgns_inserted=raw_pgns_inserted,
            raw_pgns_hashed=raw_pgns_hashed,
            raw_pgns_matched=raw_pgns_matched,
            postgres_raw_pgns_inserted=postgres_raw_pgns_inserted,
        )

    positions, analysis_signature, raw_metrics, postgres_raw_pgns_inserted = (
        _persist_and_extract_positions(
            PersistAndExtractPositionsContext(
                conn=conn,
                settings=settings,
                games_to_process=games_to_process,
                progress=progress,
                profile=profile,
                game_ids=game_ids,
            )
        )
    )
    raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched = raw_metrics
    return AnalysisPrepResult(
        positions=positions,
        resume_index=resume_index,
        analysis_signature=analysis_signature,
        raw_pgns_inserted=raw_pgns_inserted,
        raw_pgns_hashed=raw_pgns_hashed,
        raw_pgns_matched=raw_pgns_matched,
        postgres_raw_pgns_inserted=postgres_raw_pgns_inserted,
    )
</file>

<file path="prepare_dag_helpers__airflow.py">
from __future__ import annotations

from datetime import datetime, timedelta
from typing import SupportsInt, cast

from airflow.decorators import task
from airflow.utils import timezone

from tactix.dashboard_query import DashboardQuery
from tactix.pipeline import get_dashboard_payload
from tactix.utils.logger import Logger

logger = Logger(__name__)


def default_args(*, retries: int = 2) -> dict[str, object]:
    return {
        "owner": "tactix",
        "depends_on_past": False,
        "retries": retries,
        "retry_delay": timedelta(minutes=5),
        "retry_exponential_backoff": True,
        "max_retry_delay": timedelta(minutes=20),
    }


def to_epoch_ms(value: datetime | None) -> int | None:
    if value is None:
        return None
    if value.tzinfo is None:
        value = value.replace(tzinfo=timezone.utc)
    return int(value.timestamp() * 1000)


def _dag_run_conf(dag_run) -> dict[str, object] | None:
    if not dag_run:
        return None
    conf = getattr(dag_run, "conf", None)
    if isinstance(conf, dict):
        return conf
    return None


def _first_conf_value(conf: dict[str, object], keys: tuple[str, ...]) -> str | None:
    for key in keys:
        value = conf.get(key)
        if value:
            return str(value)
    return None


def _coerce_optional_int(value: object | None) -> int | None:
    if value is None:
        return None
    try:
        return int(cast(SupportsInt | str | bytes | bytearray, value))
    except (TypeError, ValueError):
        return None


def resolve_profile(dag_run, source: str) -> str | None:
    conf = _dag_run_conf(dag_run)
    if not conf:
        return None
    if source == "chesscom":
        return _first_conf_value(conf, ("chesscom_profile", "profile"))
    return _first_conf_value(conf, ("profile", "lichess_profile"))


def _read_backfill_conf(conf: dict[str, object]) -> tuple[int | None, int | None, int | None]:
    start_ms = _coerce_optional_int(conf.get("backfill_start_ms"))
    end_ms = _coerce_optional_int(conf.get("backfill_end_ms"))
    triggered_at_ms = _coerce_optional_int(conf.get("triggered_at_ms"))
    return start_ms, end_ms, triggered_at_ms


def _apply_triggered_end(end_ms: int | None, triggered_at_ms: int | None) -> int | None:
    if triggered_at_ms is None:
        return end_ms
    if end_ms is None or end_ms > triggered_at_ms:
        return triggered_at_ms
    return end_ms


def _backfill_from_interval(
    run_type: str,
    data_interval_start: datetime | None,
    data_interval_end: datetime | None,
) -> tuple[int | None, int | None]:
    if run_type != "backfill":
        return None, None
    return to_epoch_ms(data_interval_start), to_epoch_ms(data_interval_end)


def _resolve_backfill_conf(conf: dict[str, object]) -> tuple[int | None, int | None, int | None]:
    start_ms, end_ms, triggered_at_ms = _read_backfill_conf(conf)
    end_ms = _apply_triggered_end(end_ms, triggered_at_ms)
    return start_ms, end_ms, triggered_at_ms


def _resolve_backfill_from_interval(
    run_type: str,
    data_interval_start: datetime | None,
    data_interval_end: datetime | None,
    start_ms: int | None,
    end_ms: int | None,
) -> tuple[int | None, int | None, bool]:
    is_backfill = start_ms is not None or end_ms is not None
    if is_backfill:
        return start_ms, end_ms, True
    interval_start, interval_end = _backfill_from_interval(
        run_type,
        data_interval_start,
        data_interval_end,
    )
    start_ms = interval_start
    end_ms = interval_end
    return start_ms, end_ms, start_ms is not None or end_ms is not None


def resolve_backfill_window(
    dag_run,
    run_type: str,
    data_interval_start: datetime | None,
    data_interval_end: datetime | None,
) -> tuple[int | None, int | None, int | None, bool]:
    conf = _dag_run_conf(dag_run) or {}
    start_ms, end_ms, triggered_at_ms = _resolve_backfill_conf(conf)
    start_ms, end_ms, is_backfill = _resolve_backfill_from_interval(
        run_type,
        data_interval_start,
        data_interval_end,
        start_ms,
        end_ms,
    )
    return start_ms, end_ms, triggered_at_ms, is_backfill


def make_notify_dashboard_task(
    settings,
    *,
    source: str | None = None,
    task_id: str = "notify_dashboard",
):
    @task(task_id=task_id)
    def notify_dashboard(_: dict[str, object]) -> dict[str, object]:
        payload = get_dashboard_payload(
            DashboardQuery(source=source),
            settings,
        )
        logger.info(
            "Dashboard payload refreshed; metrics_version=%s",
            payload.get("metrics_version"),
        )
        return payload

    return notify_dashboard


__all__ = [
    "default_args",
    "make_notify_dashboard_task",
    "resolve_backfill_window",
    "resolve_profile",
    "to_epoch_ms",
]

# References to avoid vulture reporting these helpers as unused when only
# Airflow DAG modules import them.
_USED_BY_AIRFLOW = (
    default_args,
    make_notify_dashboard_task,
    resolve_backfill_window,
    resolve_profile,
    to_epoch_ms,
)
</file>

<file path="prepare_error__http_status.py">
"""HTTP error preparation helpers."""

from __future__ import annotations

import requests


def prepare_error__http_status(response: requests.Response, context: str) -> None:
    """Raise a RuntimeError when an HTTP response is not OK.

    Args:
        response: HTTP response to inspect.
        context: Error context to include in the message.
    """
    if response.ok:
        return
    message = f"{context}: {response.status_code} {response.text.strip()}"
    raise RuntimeError(message.strip())
</file>

<file path="prepare_games_for_sync__pipeline.py">
"""Prepare games for sync, including filters and expansion."""

from __future__ import annotations

from tactix.app.use_cases.pipeline_support import (
    _emit_fetch_progress,
    _maybe_emit_window_filtered,
    _resolve_last_timestamp_value,
)
from tactix.fetch_incremental_games__pipeline import _fetch_incremental_games
from tactix.FetchContext import FetchContext
from tactix.filter_games_for_window__pipeline import _filter_games_for_window
from tactix.GameRow import GameRow
from tactix.normalize_and_expand_games__pipeline import _normalize_and_expand_games
from tactix.sync_contexts import (
    FetchProgressContext,
    PrepareGamesForSyncContext,
    WindowFilterContext,
)


def _prepare_games_for_sync(
    context: PrepareGamesForSyncContext,
) -> tuple[list[GameRow], FetchContext, int, int]:
    fetch_context = _fetch_incremental_games(
        context.settings,
        context.client,
        context.backfill_mode,
        context.window_start_ms,
        context.window_end_ms,
    )
    games = _normalize_and_expand_games(fetch_context.raw_games, context.settings)
    games, window_filtered = _filter_games_for_window(
        games,
        context.window_start_ms,
        context.window_end_ms,
    )
    _maybe_emit_window_filtered(
        WindowFilterContext(
            settings=context.settings,
            progress=context.progress,
            backfill_mode=context.backfill_mode,
            window_filtered=window_filtered,
            window_start_ms=context.window_start_ms,
            window_end_ms=context.window_end_ms,
        )
    )
    last_timestamp_value = _resolve_last_timestamp_value(games, fetch_context.last_timestamp_ms)
    _emit_fetch_progress(
        FetchProgressContext(
            settings=context.settings,
            progress=context.progress,
            fetch_context=fetch_context,
            backfill_mode=context.backfill_mode,
            window_start_ms=context.window_start_ms,
            window_end_ms=context.window_end_ms,
            fetched_games=len(games),
        )
    )
    return games, fetch_context, window_filtered, last_timestamp_value
</file>

<file path="prepare_raw_pgn_context__pipeline.py">
"""Prepare context for raw PGN operations."""

from __future__ import annotations

import duckdb

from tactix.build_pipeline_settings__pipeline import _build_pipeline_settings
from tactix.config import Settings
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.db.raw_pgn_repository_provider import fetch_latest_raw_pgns


def _prepare_raw_pgn_context(
    settings: Settings | None = None,
    source: str | None = None,
    profile: str | None = None,
    limit: int | None = None,
) -> tuple[Settings, duckdb.DuckDBPyConnection, list[dict[str, object]]]:
    """Build settings, connection, and raw PGN rows for a pipeline."""
    settings = _build_pipeline_settings(settings, source=source, profile=profile)
    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    raw_pgns = fetch_latest_raw_pgns(conn, settings.source, limit)
    return settings, conn, raw_pgns
</file>

<file path="prime_dashboard_cache__api_cache.py">
from __future__ import annotations

from tactix.build_dashboard_cache_key__api_cache import _dashboard_cache_key
from tactix.config import get_settings
from tactix.dashboard_query import DashboardQuery
from tactix.pipeline import get_dashboard_payload
from tactix.set_dashboard_cache__api_cache import _set_dashboard_cache


def _prime_dashboard_cache(
    query: DashboardQuery,
) -> None:
    settings = get_settings(source=query.source)
    payload = get_dashboard_payload(query, settings)
    key = _dashboard_cache_key(settings, query)
    _set_dashboard_cache(key, payload)
</file>

<file path="process_analysis_position__pipeline.py">
"""Process a single analysis position within the pipeline."""

from __future__ import annotations

from importlib import import_module

from tactix.analysis_context import AnalysisPositionContext
from tactix.app.use_cases.pipeline_support import (
    _maybe_emit_analysis_progress,
    _maybe_write_analysis_checkpoint,
)
from tactix.db.tactic_repository_provider import upsert_tactic_with_outcome
from tactix.maybe_upsert_postgres_analysis__pipeline import _maybe_upsert_postgres_analysis


def _process_analysis_position(
    context: AnalysisPositionContext,
) -> tuple[int, int]:
    """Analyze a position and persist results, returning row counts."""
    if context.idx <= context.resume_index:
        return 0, 0
    pipeline_module = import_module("tactix.pipeline")
    result = pipeline_module.analyse_with_retries(
        context.engine,
        context.pos,
        context.settings,
    )
    if result is None:
        _maybe_write_analysis_checkpoint(
            context.analysis_checkpoint_path,
            context.analysis_signature,
            context.idx,
        )
        return 0, 0
    tactic_row, outcome_row = result
    upsert_tactic_with_outcome(context.conn, tactic_row, outcome_row)
    postgres_delta = (
        1
        if _maybe_upsert_postgres_analysis(
            context.pg_conn,
            context.analysis_pg_enabled,
            tactic_row,
            outcome_row,
        )
        else 0
    )
    _maybe_write_analysis_checkpoint(
        context.analysis_checkpoint_path,
        context.analysis_signature,
        context.idx,
    )
    _maybe_emit_analysis_progress(
        context.progress,
        context.settings,
        context.idx,
        context.total_positions,
        context.progress_every,
    )
    return 1, postgres_delta
</file>

<file path="ProgressCallback.py">
"""Compatibility alias for progress callbacks."""

# pylint: disable=invalid-name

from __future__ import annotations

from collections.abc import Callable

ProgressCallback = Callable[[dict[str, object]], None]

__all__ = ["ProgressCallback"]
</file>

<file path="raise_for_hash_mismatch__pipeline.py">
"""Validate raw PGN hash matches and raise on mismatch."""

from __future__ import annotations

from collections.abc import Mapping


def _raise_for_hash_mismatch(
    source: str,
    computed: Mapping[str, str],
    stored: Mapping[str, str],
    matched: int,
) -> None:
    """Raise a ValueError when computed and stored hashes diverge."""
    if matched == len(computed):
        return
    missing = [game_id for game_id, pgn_hash in computed.items() if stored.get(game_id) != pgn_hash]
    missing_sorted = ", ".join(sorted(missing))
    raise ValueError(
        f"Raw PGN hash mismatch for source={source} expected={len(computed)} "
        f"matched={matched} missing={missing_sorted}"
    )
</file>

<file path="raise_on_unexpected_kwargs__config.py">
"""Raise on unexpected keyword arguments for settings."""

from __future__ import annotations


def _raise_on_unexpected_kwargs(kwargs: dict[str, object]) -> None:
    """Raise a TypeError when unexpected kwargs are provided."""
    if kwargs:
        unexpected = next(iter(kwargs))
        raise TypeError(f"Settings.__init__() got an unexpected keyword argument '{unexpected}'")
</file>

<file path="raise_unsupported_job__api_jobs.py">
from __future__ import annotations

from typing import NoReturn


def _raise_unsupported_job(job: str) -> NoReturn:
    raise ValueError(f"Unsupported job: {job}")
</file>

<file path="read_fork_severity_floor__config.py">
from __future__ import annotations

import os


def _read_fork_severity_floor() -> float | None:
    value = os.getenv("TACTIX_FORK_SEVERITY_FLOOR")
    if not value:
        return None
    return float(value)
</file>

<file path="read_optional_text__filesystem.py">
"""Read optional filesystem text content."""

from __future__ import annotations

from pathlib import Path


def _read_optional_text(path: Path) -> str | None:
    try:
        raw = path.read_text().strip()
    except FileNotFoundError:
        return None
    return raw or None
</file>

<file path="record_daily_sync_complete__pipeline.py">
from __future__ import annotations

from tactix.ops_event import OpsEvent
from tactix.record_ops_event import record_ops_event
from tactix.sync_contexts import DailySyncCompleteContext


def _record_daily_sync_complete(
    context: DailySyncCompleteContext,
) -> None:
    record_ops_event(
        OpsEvent(
            settings=context.settings,
            component=context.settings.run_context,
            event_type="daily_game_sync_complete",
            source=context.settings.source,
            profile=context.profile,
            metadata={
                "fetched_games": len(context.games),
                "raw_pgns_inserted": context.raw_pgns_inserted,
                "postgres_raw_pgns_inserted": context.postgres_raw_pgns_inserted,
                "positions": context.positions_count,
                "tactics": context.tactics_count,
                "postgres_tactics_written": context.postgres_written,
                "postgres_tactics_synced": context.postgres_synced,
                "metrics_version": context.metrics_version,
                "backfill": context.backfill_mode,
            },
        )
    )
</file>

<file path="record_ops_event.py">
"""Persist operational events to Postgres."""

from typing import cast

from psycopg2.extras import Json

from tactix.config import Settings
from tactix.init_postgres_schema import init_postgres_schema
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.ops_event import OpsEvent
from tactix.postgres_connection import postgres_connection


def _collect_ops_event_values(
    args: tuple[object, ...],
    legacy: dict[str, object],
) -> dict[str, object]:
    ordered_keys = ("component", "event_type", "source", "profile", "metadata")
    values = init_legacy_values(ordered_keys)
    apply_legacy_kwargs(values, ordered_keys, legacy)
    apply_legacy_args(values, ordered_keys, args)
    return values


def _build_ops_event(settings: Settings, values: dict[str, object]) -> OpsEvent:
    if values["component"] is None or values["event_type"] is None:
        raise TypeError("component and event_type are required")
    return OpsEvent(
        settings=settings,
        component=cast(str, values["component"]),
        event_type=cast(str, values["event_type"]),
        source=cast(str | None, values["source"]),
        profile=cast(str | None, values["profile"]),
        metadata=cast(dict[str, object] | None, values["metadata"]),
    )


def record_ops_event(
    event: OpsEvent | Settings,
    *args: object,
    **legacy: object,
) -> bool:
    """Insert an ops event into Postgres if available."""
    if isinstance(event, OpsEvent):
        resolved = event
    else:
        values = _collect_ops_event_values(args, legacy)
        resolved = _build_ops_event(event, values)
    with postgres_connection(resolved.settings) as conn:
        if conn is None:
            return False
        init_postgres_schema(conn)
        with conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO tactix_ops.ops_events
                    (component, event_type, source, profile, metadata)
                VALUES (%s, %s, %s, %s, %s)
                """,
                (
                    resolved.component,
                    resolved.event_type,
                    resolved.source,
                    resolved.profile,
                    Json(resolved.metadata or {}),
                ),
            )
        return True
</file>

<file path="refresh_dashboard_cache_async__api_cache.py">
"""Refresh dashboard cache in a background thread."""

from __future__ import annotations

from threading import Thread

from tactix.dashboard_query import DashboardQuery
from tactix.prime_dashboard_cache__api_cache import _prime_dashboard_cache
from tactix.utils.logger import Logger

logger = Logger("tactix.api")


def _refresh_dashboard_cache_async(sources: list[str | None]) -> None:
    def worker() -> None:
        for source in sources:
            try:
                _prime_dashboard_cache(DashboardQuery(source=source))
            except (RuntimeError, ValueError, TypeError):  # pragma: no cover - defensive
                logger.exception("Failed to prime dashboard cache", extra={"source": source})

    Thread(target=worker, daemon=True).start()
</file>

<file path="refresh_metrics_result.py">
"""Pydantic model for refresh metrics responses."""

from pydantic import BaseModel


class RefreshMetricsResult(BaseModel):
    """Response payload for metrics refresh calls."""

    source: str
    user: str
    metrics_version: int
    metrics_rows: int
</file>

<file path="refresh_raw_pgns_for_existing_positions__pipeline.py">
"""Refresh raw PGNs for existing positions."""

from __future__ import annotations

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.GameRow import GameRow
from tactix.persist_raw_pgns__pipeline import PersistRawPgnsContext, _persist_raw_pgns
from tactix.upsert_postgres_raw_pgns_if_enabled__pipeline import (
    _upsert_postgres_raw_pgns_if_enabled,
)


def _refresh_raw_pgns_for_existing_positions(
    conn,
    settings: Settings,
    games_to_process: list[GameRow],
    progress: ProgressCallback | None,
    profile: str | None,
) -> tuple[tuple[int, int, int], int]:
    raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched = _persist_raw_pgns(
        PersistRawPgnsContext(
            conn=conn,
            games_to_process=games_to_process,
            settings=settings,
            progress=progress,
            profile=profile,
            delete_existing=False,
            emit_start=False,
        )
    )
    postgres_raw_pgns_inserted = _upsert_postgres_raw_pgns_if_enabled(
        settings,
        games_to_process,
        progress,
        profile,
    )
    return (raw_pgns_inserted, raw_pgns_hashed, raw_pgns_matched), postgres_raw_pgns_inserted
</file>

<file path="request_chesscom_games__pipeline.py">
from __future__ import annotations

from tactix.infra.clients.chesscom_client import ChesscomFetchRequest, ChesscomFetchResult
from tactix.ports.game_source_client import GameSourceClient


def _request_chesscom_games(
    client: GameSourceClient, cursor_value: str | None, backfill_mode: bool
) -> ChesscomFetchResult:
    return client.fetch_incremental_games(
        ChesscomFetchRequest(cursor=cursor_value, full_history=backfill_mode)
    )
</file>

<file path="require_api_token__request_auth.py">
"""Request authentication helpers for API token enforcement."""

from __future__ import annotations

from fastapi import HTTPException, Request, status

from tactix.config import get_settings
from tactix.extract_api_token__request_auth import _extract_api_token


def require_api_token(request: Request) -> None:
    """Raise HTTP 401 when the request token is missing or invalid."""
    if request.url.path == "/api/health":
        return
    settings = get_settings()
    expected = settings.api_token
    supplied = _extract_api_token(request)
    if not supplied or supplied != expected:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized")
</file>

<file path="resolve_backfill_end_ms__airflow_jobs.py">
"""Resolve the backfill end timestamp for Airflow jobs."""

from __future__ import annotations

from tactix.coerce_backfill_end_ms__airflow_jobs import _coerce_backfill_end_ms
from tactix.validate_backfill_window__airflow_jobs import _validate_backfill_window


def _resolve_backfill_end_ms(
    backfill_start_ms: int | None,
    backfill_end_ms: int | None,
    triggered_at_ms: int,
) -> int | None:
    if backfill_start_ms is None and backfill_end_ms is None:
        return backfill_end_ms
    effective_end_ms = _coerce_backfill_end_ms(backfill_end_ms, triggered_at_ms)
    _validate_backfill_window(backfill_start_ms, effective_end_ms)
    return effective_end_ms
</file>

<file path="resolve_field_value__config.py">
from __future__ import annotations

from dataclasses import MISSING

from tactix.define_config_defaults__config import _MISSING


def _field_value(name: str, field_info: object, kwargs: dict[str, object]) -> object:
    value = kwargs.pop(name, _MISSING)
    if value is not _MISSING:
        return value
    default_factory = getattr(field_info, "default_factory", MISSING)
    if default_factory is not MISSING:
        return default_factory()
    default = getattr(field_info, "default", MISSING)
    if default is not MISSING:
        return default
    raise TypeError(f"Missing required argument: {name}")
</file>

<file path="resolve_pgn_hash__db_store.py">
"""Resolve PGN hash values for storage."""

from __future__ import annotations

from collections.abc import Callable

from tactix.normalize_pgn_text__db_store import _normalize_pgn_text
from tactix.utils import hash as hash_value


def _resolve_pgn_hash(
    pgn_text: str,
    normalize_pgn: Callable[[str], str] | None,
    hash_pgn: Callable[[str], str] | None,
) -> tuple[str | None, str]:
    """Return normalized PGN text and its hash."""
    normalized = _normalize_pgn_text(pgn_text, normalize_pgn)
    hasher = hash_pgn if hash_pgn is not None else hash_value
    source = normalized if normalized is not None else pgn_text
    return normalized, hasher(source)
</file>

<file path="resolve_timestamp__db_store.py">
from __future__ import annotations

from datetime import datetime

from tactix.utils import Now


def _resolve_timestamp(value: datetime | None) -> datetime:
    return value if value is not None else Now.as_datetime()
</file>

<file path="resolve_unclear_outcome_context.py">
from typing import Any

from tactix.outcome_context import BaseOutcomeContext
from tactix.unclear_outcome_params import UnclearOutcomeParams


def _init_unclear_values(
    params: UnclearOutcomeParams | None,
) -> dict[str, Any]:
    values: dict[str, Any] = {
        "motif": None,
        "best_move": None,
        "user_move_uci": None,
        "swing": None,
        "threshold": None,
    }
    if params is None:
        return values
    values.update(
        {
            "motif": params.motif,
            "best_move": params.best_move,
            "user_move_uci": params.user_move_uci,
            "swing": params.swing,
            "threshold": params.threshold,
        }
    )
    return values


def _apply_unclear_legacy(values: dict[str, Any], legacy: dict[str, Any]) -> None:
    for key in tuple(values.keys()):
        if key in legacy:
            values[key] = legacy.pop(key)
    if legacy:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(legacy))}")


def _apply_unclear_args(values: dict[str, Any], args: tuple[Any, ...]) -> None:
    if not args:
        return
    ordered_keys = ("motif", "best_move", "user_move_uci", "swing", "threshold")
    if len(args) > len(ordered_keys):
        raise TypeError("Too many positional arguments")
    for key, value in zip(ordered_keys, args, strict=False):
        if values[key] is not None:
            raise TypeError(f"{key} provided multiple times")
        values[key] = value


def _apply_int_threshold(values: dict[str, Any], threshold: int) -> None:
    current = values["threshold"]
    if current is None:
        values["threshold"] = threshold
        return
    if current != threshold:
        raise TypeError("threshold provided multiple times")


def _apply_str_threshold(values: dict[str, Any], threshold: str) -> None:
    if values["motif"] is None:
        values["motif"] = threshold
        return
    raise TypeError("threshold provided multiple times")


def _apply_unclear_threshold(values: dict[str, Any], threshold: int | str | None) -> None:
    if threshold is None:
        return
    if isinstance(threshold, int):
        _apply_int_threshold(values, threshold)
        return
    if isinstance(threshold, str):
        _apply_str_threshold(values, threshold)
        return
    raise TypeError("threshold provided multiple times")


def _should_coerce_unclear_threshold(
    context: BaseOutcomeContext | str,
    values: dict[str, Any],
) -> bool:
    return all(
        (
            isinstance(context, BaseOutcomeContext),
            values["threshold"] is None,
            values["user_move_uci"] is None,
            values["best_move"] is None,
            values["swing"] is None,
            isinstance(values["motif"], int),
        )
    )


def _coerce_unclear_threshold(
    context: BaseOutcomeContext | str,
    values: dict[str, Any],
) -> None:
    if not _should_coerce_unclear_threshold(context, values):
        return
    values["threshold"] = values["motif"]
    values["motif"] = None


def _build_unclear_context(
    context: BaseOutcomeContext | str,
    values: dict[str, Any],
) -> BaseOutcomeContext:
    if isinstance(context, BaseOutcomeContext):
        return context
    if values["motif"] is None or values["user_move_uci"] is None:
        raise TypeError("result, motif, and user_move_uci are required")
    return BaseOutcomeContext(
        result=context,
        motif=values["motif"],
        best_move=values["best_move"],
        user_move_uci=values["user_move_uci"],
        swing=values["swing"],
    )


def resolve_unclear_outcome_context(
    context: BaseOutcomeContext | str,
    threshold: int | str | None,
    args: tuple[Any, ...],
    params: UnclearOutcomeParams | None,
    legacy: dict[str, Any],
) -> tuple[BaseOutcomeContext, int | None]:
    values = _init_unclear_values(params)
    _apply_unclear_legacy(values, legacy)
    _apply_unclear_args(values, args)
    _apply_unclear_threshold(values, threshold)
    _coerce_unclear_threshold(context, values)
    resolved = _build_unclear_context(context, values)
    return resolved, values["threshold"]
</file>

<file path="run_analysis_and_metrics__pipeline.py">
"""Run analysis and refresh metrics."""

from __future__ import annotations

from tactix.analysis_context import (
    AnalysisAndMetricsContext,
    AnalysisRunContext,
    AnalysisRunInputs,
)
from tactix.analyze_positions_with_progress__pipeline import _analyze_positions_with_progress
from tactix.app.use_cases.pipeline_support import _emit_progress
from tactix.DailyAnalysisResult import DailyAnalysisResult
from tactix.ops_event import OpsEvent
from tactix.record_ops_event import record_ops_event
from tactix.update_metrics_and_version__pipeline import _update_metrics_and_version


def _run_analysis_and_metrics(
    ctx: AnalysisAndMetricsContext,
) -> DailyAnalysisResult:
    """Run analysis and metrics refresh for a batch."""
    total_positions = len(ctx.positions)
    tactics_count, postgres_written, postgres_synced = _analyze_positions_with_progress(
        AnalysisRunContext(
            conn=ctx.conn,
            settings=ctx.settings,
            run=AnalysisRunInputs(
                positions=ctx.positions,
                resume_index=ctx.resume_index,
                analysis_signature=ctx.analysis_signature,
                progress=ctx.progress,
            ),
        )
    )
    record_ops_event(
        OpsEvent(
            settings=ctx.settings,
            component="analysis",
            event_type="analysis_complete",
            source=ctx.settings.source,
            profile=ctx.profile,
            metadata={
                "positions_analyzed": total_positions,
                "tactics_detected": tactics_count,
                "resume_index": ctx.resume_index,
                "postgres_tactics_written": postgres_written,
                "postgres_tactics_synced": postgres_synced,
            },
        )
    )
    metrics_version = _update_metrics_and_version(ctx.settings, ctx.conn)
    _emit_progress(
        ctx.progress,
        "metrics_refreshed",
        source=ctx.settings.source,
        metrics_version=metrics_version,
        message="Metrics refreshed",
    )
    return DailyAnalysisResult(
        total_positions=total_positions,
        tactics_count=tactics_count,
        postgres_written=postgres_written,
        postgres_synced=postgres_synced,
        metrics_version=metrics_version,
    )
</file>

<file path="run_analysis_loop__pipeline.py">
"""Run the analysis loop over extracted positions."""

from __future__ import annotations

from tactix.analysis_context import (
    AnalysisPositionContext,
    AnalysisPositionMeta,
    AnalysisPositionPersistence,
)
from tactix.analysis_progress_interval__pipeline import _analysis_progress_interval
from tactix.AnalysisLoopContext import AnalysisLoopContext
from tactix.process_analysis_position__pipeline import _process_analysis_position
from tactix.StockfishEngine import StockfishEngine


def _run_analysis_loop(
    context: AnalysisLoopContext,
) -> tuple[int, int]:
    """Execute analysis over positions and return counters."""
    total_positions = len(context.positions)
    progress_every = _analysis_progress_interval(total_positions)
    tactics_count = 0
    postgres_written = 0
    with StockfishEngine(context.settings) as engine:
        for idx, pos in enumerate(context.positions):
            tactics_delta, postgres_delta = _process_analysis_position(
                AnalysisPositionContext(
                    conn=context.conn,
                    settings=context.settings,
                    engine=engine,
                    meta=AnalysisPositionMeta(
                        pos=pos,
                        idx=idx,
                        resume_index=context.resume_index,
                        total_positions=total_positions,
                        progress_every=progress_every,
                    ),
                    persistence=AnalysisPositionPersistence(
                        analysis_checkpoint_path=context.analysis_checkpoint_path,
                        analysis_signature=context.analysis_signature,
                        pg_conn=context.pg_conn,
                        analysis_pg_enabled=context.analysis_pg_enabled,
                    ),
                    progress=context.progress,
                )
            )
            tactics_count += tactics_delta
            postgres_written += postgres_delta
    return tactics_count, postgres_written
</file>

<file path="run_daily_game_sync__pipeline.py">
"""Run the daily game sync pipeline with resolved settings."""

from __future__ import annotations

from typing import cast

from tactix.build_chess_client__pipeline import _build_chess_client
from tactix.build_pipeline_settings__pipeline import _build_pipeline_settings
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.ports.game_source_client import GameSourceClient
from tactix.run_daily_game_sync_internal__pipeline import _run_daily_game_sync
from tactix.sync_contexts import DailyGameSyncContext, DailyGameSyncRequest


def run_daily_game_sync(
    request: DailyGameSyncRequest | Settings,
    *args: object,
    **legacy: object,
) -> dict[str, object]:
    """Execute the daily game sync pipeline for a request."""
    if isinstance(request, DailyGameSyncRequest):
        resolved = request
    else:
        ordered_keys = (
            "source",
            "progress",
            "profile",
            "window_start_ms",
            "window_end_ms",
            "client",
        )
        values = init_legacy_values(ordered_keys)
        apply_legacy_kwargs(values, ordered_keys, legacy)
        apply_legacy_args(values, ordered_keys, args)
        resolved = DailyGameSyncRequest(
            settings=request,
            source=cast(str | None, values["source"]),
            progress=cast(ProgressCallback | None, values["progress"]),
            profile=cast(str | None, values["profile"]),
            window_start_ms=cast(int | None, values["window_start_ms"]),
            window_end_ms=cast(int | None, values["window_end_ms"]),
            client=cast(GameSourceClient | None, values["client"]),
        )
    settings = _build_pipeline_settings(
        resolved.settings,
        source=resolved.source,
        profile=resolved.profile,
    )
    client = _build_chess_client(settings, resolved.client)
    return _run_daily_game_sync(
        DailyGameSyncContext(
            settings=settings,
            client=client,
            progress=resolved.progress,
            window_start_ms=resolved.window_start_ms,
            window_end_ms=resolved.window_end_ms,
            profile=resolved.profile,
        )
    )
</file>

<file path="run_daily_game_sync_internal__pipeline.py">
"""Run the daily game sync pipeline."""

from __future__ import annotations

from tactix.analysis_context import AnalysisAndMetricsContext, AnalysisRunInputs
from tactix.app.use_cases.pipeline_support import (
    _emit_daily_sync_start,
    _emit_positions_ready,
    _handle_no_games,
    _handle_no_games_after_dedupe,
    _log_skipped_backfill,
    _update_daily_checkpoint,
)
from tactix.apply_backfill_filter__pipeline import _apply_backfill_filter
from tactix.build_daily_sync_payload__pipeline import _build_daily_sync_payload
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.is_backfill_mode__pipeline import _is_backfill_mode
from tactix.log_raw_pgns_persisted__pipeline import _log_raw_pgns_persisted
from tactix.prepare_analysis_inputs__pipeline import _prepare_analysis_inputs
from tactix.prepare_games_for_sync__pipeline import _prepare_games_for_sync
from tactix.record_daily_sync_complete__pipeline import _record_daily_sync_complete
from tactix.run_analysis_and_metrics__pipeline import _run_analysis_and_metrics
from tactix.sync_contexts import (
    AnalysisMetrics,
    DailyGameSyncContext,
    DailySyncCheckpoint,
    DailySyncCompleteContext,
    DailySyncPayloadContext,
    DailySyncPayloadMetrics,
    DailySyncStartContext,
    DailySyncTotals,
    NoGamesAfterDedupeContext,
    NoGamesContext,
    NoGamesWindowContext,
    PrepareGamesForSyncContext,
    RawPgnMetrics,
)


def _run_daily_game_sync(
    context: DailyGameSyncContext,
) -> dict[str, object]:
    backfill_mode = _is_backfill_mode(context.window_start_ms, context.window_end_ms)
    _emit_daily_sync_start(
        DailySyncStartContext(
            settings=context.settings,
            progress=context.progress,
            profile=context.profile,
            backfill_mode=backfill_mode,
            window_start_ms=context.window_start_ms,
            window_end_ms=context.window_end_ms,
        )
    )
    games, fetch_context, window_filtered, last_timestamp_value = _prepare_games_for_sync(
        PrepareGamesForSyncContext(
            settings=context.settings,
            client=context.client,
            backfill_mode=backfill_mode,
            window_start_ms=context.window_start_ms,
            window_end_ms=context.window_end_ms,
            progress=context.progress,
        )
    )
    conn = get_connection(context.settings.duckdb_path)
    init_schema(conn)
    window_context = NoGamesWindowContext(
        fetch_context=fetch_context,
        last_timestamp_value=last_timestamp_value,
        window_filtered=window_filtered,
    )
    if not games:
        return _handle_no_games(
            NoGamesContext(
                settings=context.settings,
                conn=conn,
                progress=context.progress,
                backfill_mode=backfill_mode,
                window=window_context,
            )
        )
    games_to_process, skipped_games = _apply_backfill_filter(
        conn,
        games,
        backfill_mode,
        context.settings.source,
    )
    _log_skipped_backfill(context.settings, skipped_games)
    if not games_to_process:
        return _handle_no_games_after_dedupe(
            NoGamesAfterDedupeContext(
                settings=context.settings,
                conn=conn,
                progress=context.progress,
                backfill_mode=backfill_mode,
                games=games,
                window=window_context,
            )
        )
    analysis_prep = _prepare_analysis_inputs(
        conn,
        context.settings,
        games_to_process,
        context.progress,
        context.profile,
    )
    _log_raw_pgns_persisted(
        context.settings,
        analysis_prep.raw_pgns_inserted,
        analysis_prep.raw_pgns_hashed,
        analysis_prep.raw_pgns_matched,
        games_to_process,
    )
    _emit_positions_ready(context.settings, context.progress, analysis_prep.positions)
    analysis_result = _run_analysis_and_metrics(
        AnalysisAndMetricsContext(
            conn=conn,
            settings=context.settings,
            run=AnalysisRunInputs(
                positions=analysis_prep.positions,
                resume_index=analysis_prep.resume_index,
                analysis_signature=analysis_prep.analysis_signature,
                progress=context.progress,
            ),
            profile=context.profile,
        )
    )
    checkpoint_value, last_timestamp_value = _update_daily_checkpoint(
        context.settings,
        backfill_mode,
        fetch_context,
        games,
        last_timestamp_value,
    )
    _record_daily_sync_complete(
        DailySyncCompleteContext(
            settings=context.settings,
            profile=context.profile,
            games=games,
            totals=DailySyncTotals(
                raw_pgns_inserted=analysis_prep.raw_pgns_inserted,
                postgres_raw_pgns_inserted=analysis_prep.postgres_raw_pgns_inserted,
                positions_count=analysis_result.total_positions,
                tactics_count=analysis_result.tactics_count,
                postgres_written=analysis_result.postgres_written,
                postgres_synced=analysis_result.postgres_synced,
                metrics_version=analysis_result.metrics_version,
            ),
            backfill_mode=backfill_mode,
        )
    )
    return _build_daily_sync_payload(
        DailySyncPayloadContext(
            settings=context.settings,
            fetch_context=fetch_context,
            games=games,
            metrics=DailySyncPayloadMetrics(
                raw_pgns=RawPgnMetrics(
                    raw_pgns_inserted=analysis_prep.raw_pgns_inserted,
                    raw_pgns_hashed=analysis_prep.raw_pgns_hashed,
                    raw_pgns_matched=analysis_prep.raw_pgns_matched,
                    postgres_raw_pgns_inserted=analysis_prep.postgres_raw_pgns_inserted,
                ),
                analysis=AnalysisMetrics(
                    positions_count=analysis_result.total_positions,
                    tactics_count=analysis_result.tactics_count,
                    metrics_version=analysis_result.metrics_version,
                ),
                checkpoint=DailySyncCheckpoint(
                    checkpoint_value=checkpoint_value,
                    last_timestamp_value=last_timestamp_value,
                ),
            ),
            backfill_mode=backfill_mode,
        )
    )
</file>

<file path="run_migrations__pipeline.py">
"""Run DuckDB schema migrations."""

from __future__ import annotations

from tactix.app.use_cases.pipeline_support import _emit_progress
from tactix.config import Settings, get_settings
from tactix.db.duckdb_store import get_connection, get_schema_version, migrate_schema
from tactix.define_pipeline_state__pipeline import ProgressCallback


def run_migrations(
    settings: Settings | None = None,
    source: str | None = None,
    progress: ProgressCallback | None = None,
) -> dict[str, object]:
    """Run migrations and return the resulting schema version."""
    settings = settings or get_settings(source=source)
    if source:
        settings.source = source
    settings.ensure_dirs()

    _emit_progress(
        progress,
        "migrations_start",
        source=settings.source,
        message="Starting DuckDB schema migrations",
    )

    conn = get_connection(settings.duckdb_path)
    migrate_schema(conn)
    schema_version = get_schema_version(conn)

    _emit_progress(
        progress,
        "migrations_complete",
        source=settings.source,
        schema_version=schema_version,
    )

    return {
        "source": settings.source,
        "schema_version": schema_version,
    }
</file>

<file path="run_monitor_new_positions__pipeline.py">
from __future__ import annotations

from tactix.analyze_positions__pipeline import _analyze_positions
from tactix.collect_positions_for_monitor__pipeline import _collect_positions_for_monitor
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import logger
from tactix.ops_event import OpsEvent
from tactix.prepare_raw_pgn_context__pipeline import _prepare_raw_pgn_context
from tactix.record_ops_event import record_ops_event
from tactix.update_metrics_and_version__pipeline import _update_metrics_and_version


def run_monitor_new_positions(
    settings: Settings | None = None,
    source: str | None = None,
    profile: str | None = None,
    limit: int | None = None,
) -> dict[str, object]:
    settings, conn, raw_pgns = _prepare_raw_pgn_context(
        settings=settings,
        source=source,
        profile=profile,
        limit=limit,
    )
    positions_extracted, new_game_ids, positions_to_analyze = _collect_positions_for_monitor(
        conn, settings, raw_pgns
    )

    positions_analyzed, tactics_detected = _analyze_positions(conn, settings, positions_to_analyze)

    metrics_version = _update_metrics_and_version(settings, conn)

    logger.info(
        "Monitor run complete: source=%s new_games=%s positions_extracted=%s "
        "positions_analyzed=%s tactics_detected=%s metrics_version=%s",
        settings.source,
        len(new_game_ids),
        positions_extracted,
        positions_analyzed,
        tactics_detected,
        metrics_version,
    )

    record_ops_event(
        OpsEvent(
            settings=settings,
            component=settings.run_context,
            event_type="monitor_new_positions_complete",
            source=settings.source,
            profile=profile,
            metadata={
                "new_games": len(new_game_ids),
                "positions_extracted": positions_extracted,
                "positions_analyzed": positions_analyzed,
                "tactics_detected": tactics_detected,
                "metrics_version": metrics_version,
            },
        )
    )

    return {
        "source": settings.source,
        "user": settings.user,
        "raw_pgns_checked": len(raw_pgns),
        "new_games": len(new_game_ids),
        "positions_extracted": positions_extracted,
        "positions_analyzed": positions_analyzed,
        "tactics_detected": tactics_detected,
        "metrics_version": metrics_version,
    }
</file>

<file path="run_pipeline__api.py">
"""API endpoint to run the pipeline with explicit date ranges."""

from __future__ import annotations

from datetime import UTC, datetime
from pathlib import Path
from typing import Annotated

from fastapi import Depends, HTTPException

from tactix.coerce_date_to_datetime__datetime import _coerce_date_to_datetime
from tactix.config import get_settings
from tactix.dashboard_query import DashboardQuery
from tactix.db.dashboard_repository_provider import (
    fetch_opportunity_motif_counts,
    fetch_pipeline_table_counts,
)
from tactix.db.duckdb_store import get_connection, init_schema
from tactix.list_sources_for_cache_refresh__api_cache import _sources_for_cache_refresh
from tactix.normalize_source__source import _normalize_source
from tactix.pipeline import run_daily_game_sync
from tactix.pipeline_run_filters import PipelineRunFilters
from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async


def run_pipeline(  # pragma: no cover
    filters: Annotated[PipelineRunFilters, Depends()],
) -> dict[str, object]:
    """Run the pipeline for the provided date range and return counts."""
    normalized_source = _normalize_source(filters.source)
    settings = _resolve_pipeline_settings(
        normalized_source,
        filters,
    )
    start_datetime, end_datetime, window_start_ms, window_end_ms = _resolve_window_range(filters)
    result = run_daily_game_sync(
        settings,
        source=normalized_source,
        window_start_ms=window_start_ms,
        window_end_ms=window_end_ms,
        profile=None,
    )
    _refresh_dashboard_cache_async(_sources_for_cache_refresh(normalized_source))
    counts, motif_counts = _fetch_pipeline_counts(
        settings.duckdb_path,
        DashboardQuery(
            source=normalized_source,
            start_date=start_datetime,
            end_date=end_datetime,
        ),
    )
    return {
        "status": "ok",
        "result": result,
        "counts": counts,
        "motif_counts": motif_counts,
        "window_start_ms": window_start_ms,
        "window_end_ms": window_end_ms,
    }


def _fetch_pipeline_counts(  # pragma: no cover
    db_path: Path,
    query: DashboardQuery,
) -> tuple[dict[str, int], dict[str, int]]:
    conn = get_connection(db_path)
    try:
        init_schema(conn)
        return (
            fetch_pipeline_table_counts(conn, query),
            fetch_opportunity_motif_counts(conn, query),
        )
    finally:
        conn.close()


def _resolve_pipeline_settings(  # pragma: no cover
    normalized_source: str | None,
    filters: PipelineRunFilters,
):
    settings = get_settings(source=normalized_source, profile=filters.profile)
    _apply_user_settings(settings, filters.user_id)
    _apply_fixture_settings(settings, filters.use_fixture, filters.fixture_name)
    _apply_db_settings(settings, filters.db_name, filters.reset_db)
    return settings


def _apply_user_settings(settings, user_id: str | None) -> None:  # pragma: no cover
    if not user_id:
        return
    settings.user = user_id
    settings.lichess.user = user_id
    settings.chesscom.user = user_id


def _apply_fixture_settings(  # pragma: no cover
    settings,
    use_fixture: bool,
    fixture_name: str | None,
) -> None:
    if not use_fixture:
        return
    settings.chesscom.token = None
    settings.chesscom_use_fixture_when_no_token = True
    settings.stockfish_movetime_ms = 60
    settings.stockfish_depth = 8
    settings.stockfish_multipv = 2
    if not fixture_name:
        return
    safe_name = Path(fixture_name).name
    repo_root = Path(__file__).resolve().parents[2]
    settings.chesscom_fixture_pgn_path = repo_root / "tests" / "fixtures" / safe_name


def _apply_db_settings(settings, db_name: str | None, reset_db: bool) -> None:  # pragma: no cover
    if not db_name:
        return
    safe_name = Path(db_name).name
    filename = safe_name if safe_name.endswith(".duckdb") else f"{safe_name}.duckdb"
    settings.duckdb_path = settings.data_dir / filename
    if reset_db and settings.duckdb_path.exists():
        settings.duckdb_path.unlink()


def _resolve_window_range(  # pragma: no cover
    filters: PipelineRunFilters,
) -> tuple[datetime | None, datetime | None, int | None, int | None]:
    start_datetime = _coerce_date_to_datetime(filters.start_date)
    end_datetime = _coerce_date_to_datetime(filters.end_date, end_of_day=True)
    window_start_ms = _datetime_to_ms(start_datetime)
    window_end_ms = _datetime_to_ms(end_datetime)
    if window_end_ms is not None:
        window_end_ms += 1
    if (
        window_start_ms is not None
        and window_end_ms is not None
        and window_start_ms >= window_end_ms
    ):
        raise HTTPException(status_code=400, detail="start_date must be before end_date")
    return start_datetime, end_datetime, window_start_ms, window_end_ms


def _datetime_to_ms(value: datetime | None) -> int | None:  # pragma: no cover
    if value is None:
        return None
    if value.tzinfo is None:
        value = value.replace(tzinfo=UTC)
    return int(value.timestamp() * 1000)


__all__ = ["run_pipeline"]
</file>

<file path="run_refresh_metrics__pipeline.py">
"""Refresh metrics summaries and version metadata."""

from __future__ import annotations

from tactix.app.use_cases.pipeline_support import _emit_progress
from tactix.config import Settings, get_settings
from tactix.dashboard_query import DashboardQuery
from tactix.db.dashboard_repository_provider import fetch_metrics
from tactix.db.duckdb_store import (
    get_connection,
    init_schema,
    write_metrics_version,
)
from tactix.db.metrics_repository_provider import update_metrics_summary
from tactix.define_pipeline_state__pipeline import ProgressCallback


def run_refresh_metrics(
    settings: Settings | None = None,
    source: str | None = None,
    progress: ProgressCallback | None = None,
) -> dict[str, object]:
    """Refresh metrics summary rows and return metadata."""
    settings = settings or get_settings(source=source)
    if source:
        settings.source = source
    settings.ensure_dirs()

    _emit_progress(
        progress,
        "start",
        source=settings.source,
        message="Refreshing metrics",
    )

    conn = get_connection(settings.duckdb_path)
    init_schema(conn)
    update_metrics_summary(conn)
    metrics_version = write_metrics_version(conn)
    settings.metrics_version_file.write_text(str(metrics_version))

    _emit_progress(
        progress,
        "metrics_refreshed",
        source=settings.source,
        metrics_version=metrics_version,
        message="Metrics refreshed",
    )

    return {
        "source": settings.source,
        "user": settings.user,
        "metrics_version": metrics_version,
        "metrics_rows": len(fetch_metrics(conn, DashboardQuery(source=settings.source))),
    }
</file>

<file path="serialize_status.py">
"""Serialize status models for API responses."""

from typing import Any

from tactix.postgres_status import PostgresStatus


def serialize_status(status: PostgresStatus) -> dict[str, Any]:
    """Return a JSON-serializable payload for the status object."""
    payload: dict[str, Any] = {
        "enabled": status.enabled,
        "status": status.status,
    }
    if status.latency_ms is not None:
        payload["latency_ms"] = status.latency_ms
    if status.error:
        payload["error"] = status.error
    if status.schema:
        payload["schema"] = status.schema
    if status.tables is not None:
        payload["tables"] = status.tables
    return payload
</file>

<file path="set_dashboard_cache__api_cache.py">
"""Set entries in the dashboard cache."""

from __future__ import annotations

import time as time_module

from tactix.dashboard_cache_state__api_cache import (
    _DASHBOARD_CACHE,
    _DASHBOARD_CACHE_LOCK,
    _DASHBOARD_CACHE_MAX_ENTRIES,
)


def _set_dashboard_cache(key: tuple[object, ...], payload: dict[str, object]) -> None:
    with _DASHBOARD_CACHE_LOCK:
        _DASHBOARD_CACHE[key] = (time_module.time(), payload)
        _DASHBOARD_CACHE.move_to_end(key)
        while len(_DASHBOARD_CACHE) > _DASHBOARD_CACHE_MAX_ENTRIES:
            _DASHBOARD_CACHE.popitem(last=False)
</file>

<file path="should_override_failed_attempt__tactics.py">
"""Helpers for failed attempt overrides."""

from __future__ import annotations


def _should_override_failed_attempt(
    result: str,
    swing: int | None,
    threshold: int | None,
    target_motif: str,
) -> bool:
    """Return True when a failed attempt override should apply."""
    return bool(
        result == "unclear"
        and swing is not None
        and threshold is not None
        and swing < threshold
        and target_motif
    )
</file>

<file path="should_skip_backfill__pipeline.py">
from __future__ import annotations

from collections.abc import Mapping

from tactix.db.raw_pgn_repository_provider import hash_pgn
from tactix.define_pipeline_state__pipeline import ZERO_COUNT
from tactix.GameRow import GameRow


def _should_skip_backfill(
    game: GameRow,
    latest_hashes: Mapping[str, str],
    position_counts: Mapping[str, int],
) -> bool:
    game_id = game["game_id"]
    current_hash = hash_pgn(game["pgn"])
    existing_hash = latest_hashes.get(game_id)
    return bool(
        existing_hash == current_hash and position_counts.get(game_id, ZERO_COUNT) > ZERO_COUNT
    )
</file>

<file path="SkewerDetector.py">
"""Detector for skewer motifs."""

from __future__ import annotations

from tactix._has_skewer_in_steps import _has_skewer_in_steps
from tactix._skewer_sources import _skewer_sources
from tactix.BaseTacticDetector import BaseTacticDetector
from tactix.TacticContext import TacticContext


class SkewerDetector(BaseTacticDetector):
    """Detect skewer motifs."""

    motif = "skewer"

    def detect(self, context: TacticContext) -> bool:
        """Return True when a skewer is created."""
        for square, steps in _skewer_sources(context.board_after, context.mover_color):
            if _has_skewer_in_steps(
                self,
                context.board_after,
                square,
                steps,
                opponent=not context.mover_color,
            ):
                return True
        return False
</file>

<file path="split_pgn_chunks.py">
from tactix.FIXTURE_SPLIT_RE import FIXTURE_SPLIT_RE


def split_pgn_chunks(text: str) -> list[str]:
    """Takes a string containing one or more PGN games and splits it into individual PGN chunks."""
    if not text:
        return []
    return [chunk.strip() for chunk in FIXTURE_SPLIT_RE.split(text) if chunk.strip()]
</file>

<file path="sql_tactics.py">
"""Shared SQL fragments for tactics queries and inserts."""

TACTIC_COLUMNS = """
    t.tactic_id,
    t.game_id,
    t.position_id,
    t.motif,
    t.severity,
    t.best_uci,
    t.best_san,
    t.explanation,
    t.eval_cp
"""

TACTIC_ANALYSIS_COLUMNS = """
    t.tactic_id,
    t.position_id,
    t.game_id,
    t.motif,
    t.severity,
    t.best_uci,
    t.best_san,
    t.explanation,
    t.eval_cp,
    t.created_at
"""

OUTCOME_COLUMNS = """
    o.result,
    o.user_uci,
    o.eval_delta
"""

TACTIC_QUEUE_COLUMNS = """
    t.tactic_id,
    t.game_id,
    t.position_id,
    t.motif,
    t.severity,
    t.best_uci,
    t.eval_cp,
    t.created_at
"""

TACTIC_INSERT_COLUMNS: tuple[str, ...] = (
    "game_id",
    "position_id",
    "motif",
    "severity",
    "best_uci",
    "best_san",
    "explanation",
    "eval_cp",
)


_VULTURE_USED = (TACTIC_INSERT_COLUMNS,)
</file>

<file path="stockfish_runner.py">
"""Convenience exports for Stockfish engine usage."""

from __future__ import annotations

import shutil
from pathlib import Path

from tactix.engine_result import EngineResult
from tactix.StockfishEngine import StockfishEngine
from tactix.utils.logger import Logger

logger = Logger(__name__)

__all__ = ["EngineResult", "Path", "StockfishEngine", "logger", "shutil"]
</file>

<file path="StockfishEngine.py">
"""Stockfish engine wrapper used by analysis pipelines."""

# pylint: disable=invalid-name

from __future__ import annotations

import shutil
from contextlib import suppress
from pathlib import Path
from typing import Any

import chess
import chess.engine

from tactix._apply_engine_options import (
    _apply_engine_options,
)
from tactix._engine_command_available import _engine_command_available
from tactix._initialize_engine import _initialize_engine
from tactix.config import Settings
from tactix.engine_result import EngineResult


class StockfishEngine:  # pylint: disable=protected-access
    """Thin wrapper around python-chess engine handling config and fallbacks."""

    def __init__(self, settings: Settings) -> None:
        """Initialize the engine wrapper with settings."""
        self.settings = settings
        self.engine: chess.engine.SimpleEngine | None = None
        self.applied_options: dict[str, Any] = {}

    def __enter__(self) -> StockfishEngine:
        """Enter context by starting the engine if needed."""
        if self.engine is None and self._should_start_engine():
            self._start_engine()
        return self

    def __exit__(self, _exc_type, _exc, _tb) -> None:
        """Exit context by closing the engine."""
        self.close()

    def close(self) -> None:
        """Shut down the engine if running."""
        if self.engine is None:
            return
        with suppress(chess.engine.EngineError):
            self.engine.quit()
        self.engine = None

    def restart(self) -> None:
        """Restart the engine process."""
        if self.engine is not None:
            with suppress(chess.engine.EngineError):
                self.engine.quit()
        self.engine = None
        self._start_engine()

    def _resolve_command(self) -> str:
        """Resolve the stockfish binary path."""
        if self.settings.stockfish_path.exists():
            return str(self.settings.stockfish_path)
        resolved = shutil.which(str(self.settings.stockfish_path)) or shutil.which("stockfish")
        if resolved:
            self.settings.stockfish_path = Path(resolved)
            return resolved
        return str(self.settings.stockfish_path)

    def _build_limit(self) -> chess.engine.Limit:
        """Build the analysis limit for the engine."""
        if self.settings.stockfish_depth:
            return chess.engine.Limit(depth=self.settings.stockfish_depth)
        return chess.engine.Limit(time=self.settings.stockfish_movetime_ms / 1000)

    def _material_score(self, board: chess.Board) -> int:
        """Compute a basic material score for the board."""
        piece_values = {
            chess.PAWN: 100,
            chess.KNIGHT: 320,
            chess.BISHOP: 330,
            chess.ROOK: 500,
            chess.QUEEN: 900,
            chess.KING: 0,
        }
        score = 0
        for piece_type, value in piece_values.items():
            score += len(board.pieces(piece_type, chess.WHITE)) * value
            score -= len(board.pieces(piece_type, chess.BLACK)) * value
        return score

    def _configure_options(self) -> dict[str, Any]:
        """Return the configured engine options."""
        options: dict[str, Any] = {
            "Threads": self.settings.stockfish_threads,
            "Hash": self.settings.stockfish_hash_mb,
            "Skill Level": self.settings.stockfish_skill_level,
            "UCI_AnalyseMode": self.settings.stockfish_uci_analyse_mode,
            "UCI_LimitStrength": self.settings.stockfish_limit_strength,
            "Use NNUE": self.settings.stockfish_use_nnue,
            "Random Seed": self.settings.stockfish_random_seed,
            "Seed": self.settings.stockfish_random_seed,
            "UCI_Elo": self.settings.stockfish_uci_elo,
        }
        return {name: value for name, value in options.items() if value is not None}

    def _should_start_engine(self) -> bool:
        """Return True when the engine should start."""
        return bool(self.settings._stockfish_path_overridden)

    def _start_engine(self) -> None:
        """Start the engine process if available."""
        if not self._should_start_engine():
            self._reset_engine_state()
            return
        command = self._resolve_command()
        if not _engine_command_available(command):
            self._reset_engine_state()
            return
        engine = _initialize_engine(command, self.settings)
        if engine is None:
            self._reset_engine_state()
            return
        self.engine = engine
        self.applied_options = _apply_engine_options(engine, self._configure_options())

    def _reset_engine_state(self) -> None:
        """Clear engine state to defaults."""
        self.engine = None
        self.applied_options = {}

    def analyse(self, board: chess.Board) -> EngineResult:
        """Analyze a board and return an engine result."""
        if self.engine is None:
            if self._should_start_engine():
                self._start_engine()
            if self.engine is None:
                return self._fallback_engine_result(board)
        engine = self.engine
        if engine is None:
            return self._fallback_engine_result(board)
        info = engine.analyse(
            board,
            limit=self._build_limit(),
            multipv=self.settings.stockfish_multipv,
            options={"Clear Hash": True},
        )
        return EngineResult.from_engine_result(info, board=board)

    def _fallback_engine_result(self, board: chess.Board) -> EngineResult:
        """Return a lightweight analysis result when no engine is available."""
        mate_move = self._find_mate_in_one(board)
        if mate_move is not None:
            return EngineResult(best_move=mate_move, score_cp=100000, depth=0, mate_in=1)
        return EngineResult(best_move=None, score_cp=self._material_score(board), depth=0)

    def _find_mate_in_one(self, board: chess.Board) -> chess.Move | None:
        """Return a move that delivers immediate checkmate when available."""
        for move in board.legal_moves:
            board.push(move)
            is_mate = board.is_checkmate()
            board.pop()
            if is_mate:
                return move
        return None
</file>

<file path="sync_contexts.py">
"""Shared dataclass containers for daily sync contexts."""

from __future__ import annotations

from dataclasses import dataclass
from typing import cast

from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback
from tactix.FetchContext import FetchContext
from tactix.GameRow import GameRow
from tactix.ports.game_source_client import GameSourceClient


@dataclass(frozen=True)
class DailySyncStartContext:
    """Inputs for emitting daily sync start events."""

    settings: Settings
    progress: ProgressCallback | None
    profile: str | None
    backfill_mode: bool
    window_start_ms: int | None
    window_end_ms: int | None


@dataclass(frozen=True)
class DailyGameSyncContext:
    """Inputs for running a daily game sync."""

    settings: Settings
    client: GameSourceClient
    progress: ProgressCallback | None
    window_start_ms: int | None
    window_end_ms: int | None
    profile: str | None


@dataclass(frozen=True)
class DailyGameSyncRequest:
    """Optional inputs for daily game sync requests."""

    settings: Settings | None = None
    source: str | None = None
    progress: ProgressCallback | None = None
    window_start_ms: int | None = None
    window_end_ms: int | None = None
    profile: str | None = None
    client: GameSourceClient | None = None


@dataclass(frozen=True)
class PrepareGamesForSyncContext:
    """Inputs for preparing games for sync."""

    settings: Settings
    client: GameSourceClient
    backfill_mode: bool
    window_start_ms: int | None
    window_end_ms: int | None
    progress: ProgressCallback | None


@dataclass(frozen=True)
class WindowFilterContext:
    """Inputs for reporting filtered window counts."""

    settings: Settings
    progress: ProgressCallback | None
    backfill_mode: bool
    window_filtered: int
    window_start_ms: int | None
    window_end_ms: int | None


@dataclass(frozen=True)
class FetchProgressContext:
    """Inputs for fetch progress updates."""

    settings: Settings
    progress: ProgressCallback | None
    fetch_context: FetchContext
    backfill_mode: bool
    window_start_ms: int | None
    window_end_ms: int | None
    fetched_games: int


@dataclass(frozen=True)
class DailySyncTotals:
    """Aggregated totals produced by a sync run."""

    raw_pgns_inserted: int
    postgres_raw_pgns_inserted: int
    positions_count: int
    tactics_count: int
    postgres_written: int
    postgres_synced: int
    metrics_version: int


def _validate_allowed_kwargs(kwargs: dict[str, object], allowed_keys: set[str]) -> None:
    unknown = sorted(set(kwargs) - allowed_keys)
    if unknown:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(unknown)}")


def _resolve_required_kwargs(
    kwargs: dict[str, object],
    required_keys: set[str],
) -> dict[str, object]:
    required_fields = {key: kwargs.get(key) for key in required_keys}
    missing = [name for name, value in required_fields.items() if value is None]
    if missing:
        raise TypeError(f"Missing required arguments: {', '.join(sorted(missing))}")
    return required_fields


def _collect_unexpected_kwargs(kwargs: dict[str, object], keys: set[str]) -> list[str]:
    return [name for name in keys if kwargs.get(name) is not None]


_NO_GAMES_WINDOW_KEYS = {"fetch_context", "last_timestamp_value", "window_filtered"}
_NO_GAMES_WINDOW_GAMES_KEYS = _NO_GAMES_WINDOW_KEYS | {"games"}


def _resolve_daily_sync_totals(
    totals: DailySyncTotals | None,
    totals_kwargs: dict[str, object],
) -> DailySyncTotals:
    allowed_keys = {
        "raw_pgns_inserted",
        "postgres_raw_pgns_inserted",
        "positions_count",
        "tactics_count",
        "postgres_written",
        "postgres_synced",
        "metrics_version",
    }
    _validate_allowed_kwargs(totals_kwargs, allowed_keys)
    if totals is None:
        required_fields = _resolve_required_kwargs(totals_kwargs, allowed_keys)
        return DailySyncTotals(
            raw_pgns_inserted=required_fields["raw_pgns_inserted"],
            postgres_raw_pgns_inserted=required_fields["postgres_raw_pgns_inserted"],
            positions_count=required_fields["positions_count"],
            tactics_count=required_fields["tactics_count"],
            postgres_written=required_fields["postgres_written"],
            postgres_synced=required_fields["postgres_synced"],
            metrics_version=required_fields["metrics_version"],
        )
    unexpected = _collect_unexpected_kwargs(totals_kwargs, allowed_keys)
    if unexpected:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(unexpected))}")
    return totals


@dataclass(frozen=True, init=False)
class DailySyncCompleteContext:
    """Context describing a completed daily sync run."""

    settings: Settings
    profile: str | None
    games: list[GameRow]
    totals: DailySyncTotals
    backfill_mode: bool

    def __init__(
        self,
        settings: Settings,
        profile: str | None,
        games: list[GameRow],
        totals: DailySyncTotals | None = None,
        **totals_kwargs: object,
    ) -> None:
        backfill_mode = bool(totals_kwargs.pop("backfill_mode", False))
        totals = _resolve_daily_sync_totals(
            totals,
            totals_kwargs,
        )
        object.__setattr__(self, "settings", settings)
        object.__setattr__(self, "profile", profile)
        object.__setattr__(self, "games", games)
        object.__setattr__(self, "totals", totals)
        object.__setattr__(self, "backfill_mode", backfill_mode)

    @property
    def raw_pgns_inserted(self) -> int:
        """Return inserted raw PGN count."""
        return self.totals.raw_pgns_inserted

    @property
    def postgres_raw_pgns_inserted(self) -> int:
        """Return inserted Postgres raw PGN count."""
        return self.totals.postgres_raw_pgns_inserted

    @property
    def positions_count(self) -> int:
        """Return analyzed position count."""
        return self.totals.positions_count

    @property
    def tactics_count(self) -> int:
        """Return detected tactic count."""
        return self.totals.tactics_count

    @property
    def postgres_written(self) -> int:
        """Return Postgres tactics written count."""
        return self.totals.postgres_written

    @property
    def postgres_synced(self) -> int:
        """Return Postgres tactics synced count."""
        return self.totals.postgres_synced

    @property
    def metrics_version(self) -> int:
        """Return the metrics version."""
        return self.totals.metrics_version


@dataclass(frozen=True)
class RawPgnMetrics:
    """Metrics for raw PGN ingestion."""

    raw_pgns_inserted: int
    raw_pgns_hashed: int
    raw_pgns_matched: int
    postgres_raw_pgns_inserted: int


@dataclass(frozen=True)
class AnalysisMetrics:
    """Metrics for analysis output."""

    positions_count: int
    tactics_count: int
    metrics_version: int


@dataclass(frozen=True)
class DailySyncCheckpoint:
    """Checkpoint metadata for a sync run."""

    checkpoint_value: int | None
    last_timestamp_value: int


@dataclass(frozen=True)
class DailySyncPayloadMetrics:
    """Grouped metrics for daily sync payloads."""

    raw_pgns: RawPgnMetrics
    analysis: AnalysisMetrics
    checkpoint: DailySyncCheckpoint

    @property
    def raw_pgns_inserted(self) -> int:
        """Return inserted raw PGN count."""
        return self.raw_pgns.raw_pgns_inserted

    @property
    def raw_pgns_hashed(self) -> int:
        """Return hashed raw PGN count."""
        return self.raw_pgns.raw_pgns_hashed

    @property
    def raw_pgns_matched(self) -> int:
        """Return matched raw PGN count."""
        return self.raw_pgns.raw_pgns_matched

    @property
    def postgres_raw_pgns_inserted(self) -> int:
        """Return inserted Postgres raw PGN count."""
        return self.raw_pgns.postgres_raw_pgns_inserted

    @property
    def positions_count(self) -> int:
        """Return analyzed position count."""
        return self.analysis.positions_count

    @property
    def tactics_count(self) -> int:
        """Return detected tactic count."""
        return self.analysis.tactics_count

    @property
    def metrics_version(self) -> int:
        """Return the metrics version."""
        return self.analysis.metrics_version

    @property
    def checkpoint_value(self) -> int | None:
        """Return the checkpoint value."""
        return self.checkpoint.checkpoint_value

    @property
    def last_timestamp_value(self) -> int:
        """Return the last timestamp value."""
        return self.checkpoint.last_timestamp_value


def _resolve_daily_sync_payload_metrics(
    metrics: DailySyncPayloadMetrics | None,
    metrics_kwargs: dict[str, object],
) -> DailySyncPayloadMetrics:
    allowed_keys = {
        "raw_pgns_inserted",
        "raw_pgns_hashed",
        "raw_pgns_matched",
        "postgres_raw_pgns_inserted",
        "positions_count",
        "tactics_count",
        "metrics_version",
        "checkpoint_value",
        "last_timestamp_value",
    }
    _validate_allowed_kwargs(metrics_kwargs, allowed_keys)
    if metrics is None:
        required_fields = _resolve_required_kwargs(
            metrics_kwargs,
            {
                "raw_pgns_inserted",
                "raw_pgns_hashed",
                "raw_pgns_matched",
                "postgres_raw_pgns_inserted",
                "positions_count",
                "tactics_count",
                "metrics_version",
                "last_timestamp_value",
            },
        )
        return DailySyncPayloadMetrics(
            raw_pgns=RawPgnMetrics(
                raw_pgns_inserted=required_fields["raw_pgns_inserted"],
                raw_pgns_hashed=required_fields["raw_pgns_hashed"],
                raw_pgns_matched=required_fields["raw_pgns_matched"],
                postgres_raw_pgns_inserted=required_fields["postgres_raw_pgns_inserted"],
            ),
            analysis=AnalysisMetrics(
                positions_count=required_fields["positions_count"],
                tactics_count=required_fields["tactics_count"],
                metrics_version=required_fields["metrics_version"],
            ),
            checkpoint=DailySyncCheckpoint(
                checkpoint_value=metrics_kwargs.get("checkpoint_value"),
                last_timestamp_value=required_fields["last_timestamp_value"],
            ),
        )
    unexpected = _collect_unexpected_kwargs(metrics_kwargs, allowed_keys)
    if unexpected:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(unexpected))}")
    return metrics


@dataclass(frozen=True, init=False)
class DailySyncPayloadContext:
    """Context describing the daily sync payload."""

    settings: Settings
    fetch_context: FetchContext
    games: list[GameRow]
    metrics: DailySyncPayloadMetrics
    backfill_mode: bool

    def __init__(
        self,
        settings: Settings,
        fetch_context: FetchContext,
        games: list[GameRow],
        metrics: DailySyncPayloadMetrics | None = None,
        **metrics_kwargs: object,
    ) -> None:
        backfill_mode = bool(metrics_kwargs.pop("backfill_mode", False))
        metrics = _resolve_daily_sync_payload_metrics(
            metrics,
            metrics_kwargs,
        )
        object.__setattr__(self, "settings", settings)
        object.__setattr__(self, "fetch_context", fetch_context)
        object.__setattr__(self, "games", games)
        object.__setattr__(self, "metrics", metrics)
        object.__setattr__(self, "backfill_mode", backfill_mode)

    @property
    def raw_pgns_inserted(self) -> int:
        """Return inserted raw PGN count."""
        return self.metrics.raw_pgns_inserted

    @property
    def raw_pgns_hashed(self) -> int:
        """Return hashed raw PGN count."""
        return self.metrics.raw_pgns_hashed

    @property
    def raw_pgns_matched(self) -> int:
        """Return matched raw PGN count."""
        return self.metrics.raw_pgns_matched

    @property
    def postgres_raw_pgns_inserted(self) -> int:
        """Return inserted Postgres raw PGN count."""
        return self.metrics.postgres_raw_pgns_inserted

    @property
    def positions_count(self) -> int:
        """Return analyzed position count."""
        return self.metrics.positions_count

    @property
    def tactics_count(self) -> int:
        """Return detected tactic count."""
        return self.metrics.tactics_count

    @property
    def metrics_version(self) -> int:
        """Return the metrics version."""
        return self.metrics.metrics_version

    @property
    def checkpoint_value(self) -> int | None:
        """Return the checkpoint value."""
        return self.metrics.checkpoint_value

    @property
    def last_timestamp_value(self) -> int:
        """Return the last timestamp value."""
        return self.metrics.last_timestamp_value


@dataclass(frozen=True)
class NoGamesWindowContext:
    """Grouped window metadata for no-games contexts."""

    fetch_context: FetchContext
    last_timestamp_value: int
    window_filtered: int


def _resolve_no_games_window_context(
    window: NoGamesWindowContext | None,
    *,
    fetch_context: FetchContext | None,
    last_timestamp_value: int | None,
    window_filtered: int | None,
) -> NoGamesWindowContext:
    if window is None:
        _resolve_required_kwargs(
            {
                "fetch_context": fetch_context,
                "last_timestamp_value": last_timestamp_value,
                "window_filtered": window_filtered,
            },
            _NO_GAMES_WINDOW_KEYS,
        )
        return NoGamesWindowContext(
            fetch_context=fetch_context,
            last_timestamp_value=last_timestamp_value,
            window_filtered=window_filtered,
        )
    unexpected = _collect_unexpected_kwargs(
        {
            "fetch_context": fetch_context,
            "last_timestamp_value": last_timestamp_value,
            "window_filtered": window_filtered,
        },
        _NO_GAMES_WINDOW_KEYS,
    )
    if unexpected:
        raise TypeError(f"Unexpected keyword arguments: {', '.join(sorted(unexpected))}")
    return window


def _resolve_no_games_window_from_kwargs(
    window: NoGamesWindowContext | None,
    window_kwargs: dict[str, object],
    *,
    allowed_keys: set[str],
) -> NoGamesWindowContext:
    _validate_allowed_kwargs(window_kwargs, allowed_keys)
    return _resolve_no_games_window_context(
        window,
        fetch_context=window_kwargs.get("fetch_context"),
        last_timestamp_value=window_kwargs.get("last_timestamp_value"),
        window_filtered=window_kwargs.get("window_filtered"),
    )


def _require_no_games_payload_games(window_kwargs: dict[str, object]) -> list[GameRow]:
    games = window_kwargs.get("games")
    if games is None:
        raise TypeError("Missing required arguments: games")
    return cast(list[GameRow], games)


class _NoGamesWindowAccessors:
    window: NoGamesWindowContext

    @property
    def fetch_context(self) -> FetchContext:
        """Return the fetch context."""
        return self.window.fetch_context

    @property
    def last_timestamp_value(self) -> int:
        """Return the last timestamp value."""
        return self.window.last_timestamp_value

    @property
    def window_filtered(self) -> int:
        """Return the filtered window count."""
        return self.window.window_filtered


@dataclass(frozen=True, init=False)
class NoGamesPayloadContext(_NoGamesWindowAccessors):
    """Payload context when no games are returned."""

    settings: Settings
    conn: object
    backfill_mode: bool
    window: NoGamesWindowContext

    def __init__(
        self,
        settings: Settings,
        conn: object,
        backfill_mode: bool,
        window: NoGamesWindowContext | None = None,
        **window_kwargs: object,
    ) -> None:
        window = _resolve_no_games_window_from_kwargs(
            window,
            window_kwargs,
            allowed_keys=_NO_GAMES_WINDOW_KEYS,
        )
        object.__setattr__(self, "settings", settings)
        object.__setattr__(self, "conn", conn)
        object.__setattr__(self, "backfill_mode", backfill_mode)
        object.__setattr__(self, "window", window)


@dataclass(frozen=True, init=False)
class NoGamesAfterDedupePayloadContext(_NoGamesWindowAccessors):
    """Payload context when games vanish after de-dupe."""

    settings: Settings
    conn: object
    backfill_mode: bool
    games: list[GameRow]
    window: NoGamesWindowContext

    def __init__(
        self,
        settings: Settings,
        conn: object,
        backfill_mode: bool,
        window: NoGamesWindowContext | None = None,
        **window_kwargs: object,
    ) -> None:
        games = _require_no_games_payload_games(window_kwargs)
        window = _resolve_no_games_window_from_kwargs(
            window,
            window_kwargs,
            allowed_keys=_NO_GAMES_WINDOW_GAMES_KEYS,
        )
        object.__setattr__(self, "settings", settings)
        object.__setattr__(self, "conn", conn)
        object.__setattr__(self, "backfill_mode", backfill_mode)
        object.__setattr__(self, "games", games)
        object.__setattr__(self, "window", window)


@dataclass(frozen=True, init=False)
class NoGamesContext(_NoGamesWindowAccessors):
    """Context for no-games handling."""

    settings: Settings
    conn: object
    progress: ProgressCallback | None
    backfill_mode: bool
    window: NoGamesWindowContext

    def __init__(
        self,
        settings: Settings,
        conn: object,
        progress: ProgressCallback | None,
        window: NoGamesWindowContext | None = None,
        **window_kwargs: object,
    ) -> None:
        backfill_mode = bool(window_kwargs.pop("backfill_mode", False))
        window = _resolve_no_games_window_from_kwargs(
            window,
            window_kwargs,
            allowed_keys=_NO_GAMES_WINDOW_KEYS,
        )
        object.__setattr__(self, "settings", settings)
        object.__setattr__(self, "conn", conn)
        object.__setattr__(self, "progress", progress)
        object.__setattr__(self, "backfill_mode", backfill_mode)
        object.__setattr__(self, "window", window)


@dataclass(frozen=True, init=False)
class NoGamesAfterDedupeContext(_NoGamesWindowAccessors):
    """Context for handling no-games after de-dupe."""

    settings: Settings
    conn: object
    progress: ProgressCallback | None
    backfill_mode: bool
    games: list[GameRow]
    window: NoGamesWindowContext

    def __init__(
        self,
        settings: Settings,
        conn: object,
        progress: ProgressCallback | None,
        window: NoGamesWindowContext | None = None,
        **window_kwargs: object,
    ) -> None:
        backfill_mode = bool(window_kwargs.pop("backfill_mode", False))
        games = _require_no_games_payload_games(window_kwargs)
        window = _resolve_no_games_window_from_kwargs(
            window,
            window_kwargs,
            allowed_keys=_NO_GAMES_WINDOW_GAMES_KEYS,
        )
        object.__setattr__(self, "settings", settings)
        object.__setattr__(self, "conn", conn)
        object.__setattr__(self, "progress", progress)
        object.__setattr__(self, "backfill_mode", backfill_mode)
        object.__setattr__(self, "games", games)
        object.__setattr__(self, "window", window)
</file>

<file path="sync_postgres_analysis_results__pipeline.py">
"""Sync recent analysis results into Postgres."""

from __future__ import annotations

from importlib import import_module

# pylint: disable=broad-exception-caught
from tactix.config import Settings
from tactix.dashboard_query import DashboardQuery
from tactix.db.dashboard_repository_provider import fetch_recent_tactics
from tactix.define_pipeline_state__pipeline import DEFAULT_SYNC_LIMIT, logger


def _sync_postgres_analysis_results(
    conn,
    pg_conn,
    settings: Settings,
    limit: int = DEFAULT_SYNC_LIMIT,
) -> int:
    if pg_conn is None:
        return 0
    synced = 0
    recent = fetch_recent_tactics(
        conn,
        DashboardQuery(source=settings.source),
        limit=limit,
    )
    pipeline_module = import_module("tactix.pipeline")
    for row in recent:
        tactic_row = {
            "game_id": row.get("game_id"),
            "position_id": row.get("position_id"),
            "motif": row.get("motif", "unknown"),
            "severity": row.get("severity", 0.0),
            "best_uci": row.get("best_uci", ""),
            "best_san": row.get("best_san"),
            "explanation": row.get("explanation"),
            "eval_cp": row.get("eval_cp", 0),
        }
        outcome_row = {
            "result": row.get("result", "unclear"),
            "user_uci": row.get("user_uci", ""),
            "eval_delta": row.get("eval_delta", 0),
        }
        try:
            pipeline_module.upsert_analysis_tactic_with_outcome(
                pg_conn,
                tactic_row,
                outcome_row,
            )
            synced += 1
        except Exception as exc:
            logger.warning("Postgres analysis sync failed: %s", exc)
    return synced
</file>

<file path="TacticContext.py">
"""Context for tactic detector evaluation."""

# pylint: disable=invalid-name

from __future__ import annotations

from dataclasses import dataclass

import chess


@dataclass(frozen=True)
class TacticContext:
    """Inputs used by tactic detectors."""

    board_before: chess.Board
    board_after: chess.Board
    best_move: chess.Move
    mover_color: bool
</file>

<file path="TacticDetails.py">
from dataclasses import dataclass


@dataclass(frozen=True)
class TacticDetails:
    motif: str
    severity: float
    best_move: str | None
    base_cp: int
    mate_in: int | None
    best_san: str | None
    explanation: str | None
</file>

<file path="TacticRow.py">
from __future__ import annotations

from dataclasses import dataclass

from tactix.TacticRowInput import TacticRowInput
from tactix.utils.logger import funclogger


@dataclass(frozen=True)
class TacticRowDetails:
    best_uci: str | None
    best_san: str | None
    explanation: str | None
    mate_in: int | None


@dataclass(frozen=True)
class TacticRow:
    game_id: str
    position_id: str | None
    motif: str
    severity: float
    eval_cp: int
    details: TacticRowDetails

    @classmethod
    @funclogger
    def from_inputs(cls, inputs: TacticRowInput) -> TacticRow:
        return cls(
            game_id=inputs.position["game_id"],
            position_id=inputs.position.get("position_id"),
            motif=inputs.details.motif,
            severity=inputs.details.severity,
            eval_cp=inputs.details.base_cp,
            details=TacticRowDetails(
                best_uci=inputs.details.best_move,
                best_san=inputs.details.best_san,
                explanation=inputs.details.explanation,
                mate_in=inputs.details.mate_in,
            ),
        )

    def to_row(self) -> dict[str, object]:
        return {
            "game_id": self.game_id,
            "position_id": self.position_id,
            "motif": self.motif,
            "severity": self.severity,
            "best_uci": self.details.best_uci,
            "eval_cp": self.eval_cp,
            "best_san": self.details.best_san,
            "explanation": self.details.explanation,
            "mate_in": self.details.mate_in,
        }
</file>

<file path="TacticRowInput.py">
from dataclasses import dataclass

from tactix.OutcomeDetails import OutcomeDetails
from tactix.TacticDetails import TacticDetails


@dataclass(frozen=True)
class TacticRowInput:
    position: dict[str, object]
    details: TacticDetails
    outcome: OutcomeDetails
</file>

<file path="tactics_search_filters.py">
"""Pydantic models for tactics search filters."""

from __future__ import annotations

from datetime import date

from pydantic import BaseModel


class TacticsSearchFilters(BaseModel):
    """Filters accepted by the tactics search endpoint."""

    source: str | None = None
    motif: str | None = None
    rating_bucket: str | None = None
    time_control: str | None = None
    start_date: date | None = None
    end_date: date | None = None
</file>

<file path="trend_stats.py">
"""API handler for trend stats."""

from typing import Annotated

from fastapi import Depends

from tactix.build_dashboard_stats_payload__api import _build_dashboard_stats_payload
from tactix.dashboard_query_filters import DashboardQueryFilters
from tactix.db.dashboard_repository_provider import fetch_trend_stats


def trend_stats(
    filters: Annotated[DashboardQueryFilters, Depends()],
) -> dict[str, object]:
    """Return trend stats payload for the provided filters."""
    return _build_dashboard_stats_payload(filters, fetch_trend_stats, "trends")
</file>

<file path="trigger_airflow_daily_sync__airflow_jobs.py">
from __future__ import annotations

from typing import cast

from tactix.airflow_daily_sync_context import AirflowDailySyncTriggerContext
from tactix.build_airflow_conf__airflow_jobs import _airflow_conf
from tactix.config import Settings
from tactix.get_airflow_run_id__airflow_response import _airflow_run_id
from tactix.legacy_args import apply_legacy_args, apply_legacy_kwargs, init_legacy_values
from tactix.orchestrate_dag_run__airflow_trigger import (
    orchestrate_dag_run__airflow_trigger,
)


def _trigger_airflow_daily_sync(
    context: AirflowDailySyncTriggerContext | Settings,
    *args: object,
    **legacy: object,
) -> str:
    if isinstance(context, AirflowDailySyncTriggerContext):
        resolved = context
    else:
        ordered_keys = (
            "source",
            "profile",
            "backfill_start_ms",
            "backfill_end_ms",
            "triggered_at_ms",
        )
        values = init_legacy_values(ordered_keys)
        apply_legacy_kwargs(values, ordered_keys, legacy)
        apply_legacy_args(values, ordered_keys, args)
        resolved = AirflowDailySyncTriggerContext(
            settings=context,
            source=cast(str | None, values["source"]),
            profile=cast(str | None, values["profile"]),
            backfill_start_ms=cast(int | None, values["backfill_start_ms"]),
            backfill_end_ms=cast(int | None, values["backfill_end_ms"]),
            triggered_at_ms=cast(int | None, values["triggered_at_ms"]),
        )
    payload = orchestrate_dag_run__airflow_trigger(
        resolved.settings,
        "daily_game_sync",
        _airflow_conf(
            resolved.source,
            resolved.profile,
            backfill_start_ms=resolved.backfill_start_ms,
            backfill_end_ms=resolved.backfill_end_ms,
            triggered_at_ms=resolved.triggered_at_ms,
        ),
    )
    return _airflow_run_id(payload)
</file>

<file path="trigger_daily_sync__api_jobs.py">
"""API endpoint to trigger a daily sync job."""

from __future__ import annotations

from typing import Annotated

from fastapi import Query

from tactix.config import get_settings
from tactix.list_sources_for_cache_refresh__api_cache import _sources_for_cache_refresh
from tactix.pipeline import run_daily_game_sync
from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async


def trigger_daily_sync(
    source: Annotated[str | None, Query()] = None,
    backfill_start_ms: Annotated[int | None, Query(ge=0)] = None,
    backfill_end_ms: Annotated[int | None, Query(ge=0)] = None,
    profile: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    """Trigger the daily sync pipeline and return status."""
    settings = get_settings(source=source, profile=profile)
    result = run_daily_game_sync(
        settings,
        source=source,
        window_start_ms=backfill_start_ms,
        window_end_ms=backfill_end_ms,
        profile=profile,
    )
    _refresh_dashboard_cache_async(_sources_for_cache_refresh(source))
    return {"status": "ok", "result": result}
</file>

<file path="trigger_job__api_jobs.py">
"""API endpoint to trigger a background job."""

from __future__ import annotations

import time as time_module
from queue import Queue
from typing import Annotated

from fastapi import Query

from tactix._ignore_progress import _ignore_progress
from tactix.config import get_settings
from tactix.job_stream import BackfillWindow, StreamJobRunContext, _run_stream_job
from tactix.list_sources_for_cache_refresh__api_cache import _sources_for_cache_refresh
from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async
from tactix.resolve_backfill_end_ms__airflow_jobs import _resolve_backfill_end_ms


def trigger_job(
    job: Annotated[str, Query()] = "daily_game_sync",
    source: Annotated[str | None, Query()] = None,
    profile: Annotated[str | None, Query()] = None,
    backfill_start_ms: Annotated[int | None, Query(ge=0)] = None,
    backfill_end_ms: Annotated[int | None, Query(ge=0)] = None,
) -> dict[str, object]:
    """Trigger a background job and return status."""
    settings = get_settings(source=source, profile=profile)
    queue: Queue[object] = Queue()
    triggered_at_ms = int(time_module.time() * 1000)
    effective_end_ms = _resolve_backfill_end_ms(
        backfill_start_ms,
        backfill_end_ms,
        triggered_at_ms,
    )
    result = _run_stream_job(
        StreamJobRunContext(
            settings=settings,
            queue=queue,
            job=job,
            window=BackfillWindow(
                source=source,
                profile=profile,
                backfill_start_ms=backfill_start_ms,
                backfill_end_ms=effective_end_ms,
                triggered_at_ms=triggered_at_ms,
            ),
            progress=_ignore_progress,
        )
    )
    if job in {"daily_game_sync", "refresh_metrics"}:
        _refresh_dashboard_cache_async(_sources_for_cache_refresh(source))
    return {"status": "ok", "job": job, "result": result}
</file>

<file path="trigger_migrations__api_jobs.py">
"""API handler for triggering migrations."""

from __future__ import annotations

from typing import Annotated

from fastapi import Query

from tactix.config import get_settings
from tactix.pipeline import run_migrations


def trigger_migrations(
    source: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    """Trigger database migrations and return status."""
    result = run_migrations(get_settings(source=source), source=source)
    return {"status": "ok", "result": result}
</file>

<file path="trigger_refresh_metrics__api_jobs.py">
from __future__ import annotations

from typing import Annotated

from fastapi import Query

from tactix.config import get_settings
from tactix.list_sources_for_cache_refresh__api_cache import _sources_for_cache_refresh
from tactix.pipeline import run_refresh_metrics
from tactix.refresh_dashboard_cache_async__api_cache import _refresh_dashboard_cache_async


def trigger_refresh_metrics(
    source: Annotated[str | None, Query()] = None,
) -> dict[str, object]:
    result = run_refresh_metrics(get_settings(source=source), source=source)
    _refresh_dashboard_cache_async(_sources_for_cache_refresh(source))
    return {"status": "ok", "result": result}
</file>

<file path="unclear_outcome_params.py">
"""Helpers for resolving unclear outcome inputs."""

from __future__ import annotations

from dataclasses import dataclass


@dataclass(frozen=True)
class UnclearOutcomeParams:
    motif: str | None = None
    best_move: str | None = None
    user_move_uci: str | None = None
    swing: int | None = None
    threshold: int | None = None
</file>

<file path="update_metrics_and_version__pipeline.py">
"""Update metrics summary and version counters."""

from __future__ import annotations

from tactix.config import Settings
from tactix.db.duckdb_store import write_metrics_version
from tactix.db.metrics_repository_provider import update_metrics_summary


def _update_metrics_and_version(settings: Settings, conn) -> int:
    update_metrics_summary(conn)
    metrics_version = write_metrics_version(conn)
    settings.metrics_version_file.write_text(str(metrics_version))
    return metrics_version
</file>

<file path="upsert_analysis_tactic_with_outcome.py">
"""Upsert tactics and outcome rows into Postgres."""

from collections.abc import Mapping
from typing import cast

from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix._delete_existing_analysis import _delete_existing_analysis
from tactix._insert_analysis_outcome import _insert_analysis_outcome
from tactix._insert_analysis_tactic import _insert_analysis_tactic
from tactix.define_base_db_store__db_store import BaseDbStore


def upsert_analysis_tactic_with_outcome(
    conn: PgConnection,
    tactic_row: Mapping[str, object],
    outcome_row: Mapping[str, object],
) -> int:
    """Upsert a tactic and its outcome and return the tactic id."""
    position_id = cast(
        int,
        BaseDbStore.require_position_id(
            tactic_row,
            "position_id is required for Postgres analysis upsert",
        ),
    )
    tactic_plan = BaseDbStore.build_tactic_insert_plan(
        game_id=tactic_row.get("game_id"),
        position_id=position_id,
        tactic_row=tactic_row,
    )
    outcome_plan = BaseDbStore.build_outcome_insert_plan(outcome_row)
    autocommit_state = conn.autocommit
    conn.autocommit = False
    try:
        with conn.cursor() as cur:
            _delete_existing_analysis(cur, position_id)
            tactic_id = _insert_analysis_tactic(cur, tactic_plan)
            _insert_analysis_outcome(cur, tactic_id, outcome_plan)
    except Exception:
        conn.rollback()
        raise
    else:
        conn.commit()
        return tactic_id
    finally:
        conn.autocommit = autocommit_state
</file>

<file path="upsert_postgres_raw_pgns_if_enabled__pipeline.py">
"""Upsert raw PGNs into Postgres when enabled."""

from __future__ import annotations

from collections.abc import Mapping
from typing import cast

import psycopg2

from tactix.app.use_cases.pipeline_support import _emit_progress
from tactix.config import Settings
from tactix.define_pipeline_state__pipeline import ProgressCallback, logger
from tactix.GameRow import GameRow
from tactix.init_pgn_schema import init_pgn_schema
from tactix.ops_event import OpsEvent
from tactix.postgres_connection import postgres_connection
from tactix.postgres_pgns_enabled import postgres_pgns_enabled
from tactix.record_ops_event import (
    record_ops_event,
)
from tactix.upsert_postgres_raw_pgns import upsert_postgres_raw_pgns


def _upsert_postgres_raw_pgns_if_enabled(
    settings: Settings,
    games_to_process: list[GameRow],
    progress: ProgressCallback | None,
    profile: str | None,
) -> int:
    if not postgres_pgns_enabled(settings):
        return 0
    inserted = 0
    with postgres_connection(settings) as pg_conn:
        if pg_conn is None:
            logger.warning("Postgres raw PGN mirror enabled but connection unavailable")
        else:
            init_pgn_schema(pg_conn)
            try:
                inserted = upsert_postgres_raw_pgns(
                    pg_conn,
                    cast(list[Mapping[str, object]], games_to_process),
                )
            except psycopg2.Error as exc:
                logger.warning("Postgres raw PGN upsert failed: %s", exc)
    _emit_progress(
        progress,
        "postgres_raw_pgns_persisted",
        source=settings.source,
        inserted=inserted,
        total=len(games_to_process),
    )
    record_ops_event(
        OpsEvent(
            settings=settings,
            component="ingestion",
            event_type="postgres_raw_pgns_persisted",
            source=settings.source,
            profile=profile,
            metadata={
                "inserted": inserted,
                "total": len(games_to_process),
            },
        )
    )
    return inserted
</file>

<file path="upsert_postgres_raw_pgns.py">
"""Persist raw PGN rows into Postgres."""

from collections.abc import Mapping

import psycopg2
from psycopg2.extensions import connection as PgConnection  # noqa: N812

from tactix._upsert_postgres_raw_pgn_rows import _upsert_postgres_raw_pgn_rows


def upsert_postgres_raw_pgns(
    conn: PgConnection,
    rows: list[Mapping[str, object]],
) -> int:
    """Insert raw PGN rows inside a transaction."""
    if not rows:
        return 0
    autocommit_state = conn.autocommit
    conn.autocommit = False
    try:
        with conn.cursor() as cur:
            inserted = _upsert_postgres_raw_pgn_rows(cur, rows)
    except psycopg2.Error:
        conn.rollback()
        raise
    else:
        conn.commit()
        return inserted
    finally:
        conn.autocommit = autocommit_state
</file>

<file path="validate_backfill_window__airflow_jobs.py">
from __future__ import annotations

from fastapi import HTTPException


def _validate_backfill_window(
    backfill_start_ms: int | None,
    backfill_end_ms: int | None,
) -> None:
    if backfill_start_ms is None or backfill_end_ms is None:
        return
    if backfill_start_ms >= backfill_end_ms:
        raise HTTPException(
            status_code=400,
            detail="Backfill window must end after start",
        )
</file>

<file path="validate_raw_pgn_hashes__pipeline.py">
from __future__ import annotations

from collections.abc import Callable

from tactix.compute_pgn_hashes__pipeline import _compute_pgn_hashes
from tactix.count_hash_matches__pipeline import _count_hash_matches
from tactix.GameRow import GameRow
from tactix.raise_for_hash_mismatch__pipeline import _raise_for_hash_mismatch


def _validate_raw_pgn_hashes(
    rows: list[GameRow],
    source: str,
    fetch_latest_pgn_hashes: Callable[[list[str], str], dict[str, str]],
) -> dict[str, int]:
    """
    Validates raw PGN hashes for a set of chess games by comparing computed hashes
    with those stored in the database.

    Parameters
    ----------
    rows : list of GameRow
        List of game rows containing raw PGN data to be validated.
    source : str
        Identifier for the data source (e.g., 'chesscom', 'lichess').
    fetch_latest_pgn_hashes : Callable[[list[str], str], dict[str, str]]
        Fetcher returning latest hashes keyed by game id for the provided source.

    Returns
    -------
    dict of str to int
        Dictionary with the following keys:
        - 'computed': Number of PGN hashes computed from the input rows.
        - 'matched': Number of computed hashes that matched the stored hashes.

    Raises
    ------
    Exception
        Raises an exception if there is a mismatch between computed and stored
        hashes. The specific exception type and message depend on the implementation
        of `_raise_for_hash_mismatch`.

    Examples
    --------
    >>> rows = [GameRow(...), GameRow(...)]
    >>> result = _validate_raw_pgn_hashes(rows, "chesscom", fetch_latest_pgn_hashes)
    >>> print(result)
    {'computed': 2, 'matched': 2}

    Commentary
    ----------
    This function encapsulates a clear validation step for PGN hashes, which is a
    distinct and testable unit of work. However, as a private function (by naming
    convention), it may be better suited as part of a larger validation or pipeline
    module rather than a standalone module. If this is the only function in its
    module, consider combining it with related pipeline or validation logic to
    maintain a cohesive module structure.
    """
    if not rows:
        return {"computed": 0, "matched": 0}
    computed = _compute_pgn_hashes(rows, source)
    stored = fetch_latest_pgn_hashes(list(computed.keys()), source)
    matched = _count_hash_matches(computed, stored)
    _raise_for_hash_mismatch(source, computed, stored, matched)
    return {"computed": len(computed), "matched": matched}
</file>

<file path="verify_stockfish_checksum.py">
"""Verify Stockfish binary checksums."""

import hashlib
from pathlib import Path

from tactix.utils.logger import Logger

logger = Logger(__name__)


def verify_stockfish_checksum(path: Path, expected: str | None, mode: str = "warn") -> bool:
    """Verify a Stockfish binary checksum.

    Args:
        path: Path to the Stockfish binary.
        expected: Expected hex digest.
        mode: "enforce" to raise on mismatch, otherwise warn.

    Returns:
        True if checksum matches, False otherwise.
    """

    if not expected:
        return True
    payload = path.read_bytes()
    digest = hashlib.sha256(payload).hexdigest()
    if digest == expected:
        return True
    message = f"Stockfish checksum mismatch for {path}"
    if mode == "enforce":
        raise RuntimeError(message)
    logger.warning(message)
    return False
</file>

<file path="within_window__pipeline.py">
"""Check whether a game falls within a time window."""

from __future__ import annotations

from tactix.GameRow import GameRow


def _within_window(game: GameRow, start_ms: int | None, end_ms: int | None) -> bool:
    last_ts = game["last_timestamp_ms"]
    return (start_ms is None or last_ts >= start_ms) and (end_ms is None or last_ts < end_ms)
</file>

</files>
